#!/usr/bin/env python3
import psycopg2
from psycopg2 import sql # SQL ì‹ë³„ì ì•ˆì „ ì²˜ë¦¬ìš©
import yaml # YAML ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import datetime # íƒ€ì„ìŠ¤íƒ¬í”„ìš©
import os # ë””ë ‰í† ë¦¬ ìƒì„±ìš©
import argparse # ì»¤ë§¨ë“œë¼ì¸ ì¸ìˆ˜ ì²˜ë¦¬ìš©
import re # SQL ì •ê·œí™”ìš©
from collections import defaultdict
try:
    from .dataMig import run_data_migration_parallel, compare_row_counts
except ImportError:
    from dataMig import run_data_migration_parallel, compare_row_counts
# --- ì œì™¸í•  ê°ì²´ ëª©ë¡ ---
# Liquibase ë“± ë§ˆì´ê·¸ë ˆì´ì…˜ ë„êµ¬ ê´€ë ¨ í…Œì´ë¸” ë˜ëŠ” ê¸°íƒ€ ì œì™¸ ëŒ€ìƒ
EXCLUDE_TABLES = ['databasechangelog', 'databasechangeloglock']
# ê´€ë ¨ ì¸ë±ìŠ¤ ë˜ëŠ” ê¸°íƒ€ ì œì™¸ ëŒ€ìƒ
EXCLUDE_INDEXES = ['databasechangeloglock_pkey'] # í•„ìš”ì‹œ íƒ€ê²Ÿ ì „ìš© ì¸ë±ìŠ¤ ì¶”ê°€

DEFAULT_CONFIG_PATH = "config.yaml"
AUTO_INSTALL_EXTENSIONS = {"pg_trgm"}

# --- DB ì—°ê²° í•¨ìˆ˜ ---
def get_connection(config):
    conn = psycopg2.connect(**config)
    return conn

def load_config(config_path):
    """Load config YAML from a given path."""
    try:
        with open(config_path, 'r', encoding='utf-8') as stream:
            config = yaml.safe_load(stream)
            if not config:
                print(f"Error: {config_path} is empty or invalid.")
                return None
            return config
    except FileNotFoundError:
        print(f"Error: {config_path} not found.")
        return None
    except yaml.YAMLError as exc:
        print(f"Error parsing {config_path}: {exc}")
        return None
    except Exception as e:
        print(f"An unexpected error occurred while reading {config_path}: {e}")
        return None

def fetch_extensions(conn):
    """Fetch installed extensions from the connected database."""
    cur = conn.cursor()
    cur.execute("SELECT extname FROM pg_extension;")
    extensions = {row[0] for row in cur.fetchall()}
    cur.close()
    return extensions

def get_extension_diffs(src_conn, tgt_conn, allowlist=None):
    """Return missing extensions and those skipped by allowlist."""
    src_exts = fetch_extensions(src_conn)
    tgt_exts = fetch_extensions(tgt_conn)
    missing = src_exts - tgt_exts
    if allowlist is None:
        return sorted(missing), []
    allowed_missing = sorted(missing & allowlist)
    skipped_missing = sorted(missing - allowlist)
    return allowed_missing, skipped_missing

def build_extension_sql(extensions):
    """Build CREATE EXTENSION statements."""
    return [
        f"-- EXTENSION {ext}\nCREATE EXTENSION IF NOT EXISTS {ext};\n"
        for ext in extensions
    ]

# --- Enum DDL ì¡°íšŒ ---
def fetch_enums(conn):
    cur = conn.cursor()
    query = """
    SELECT t.typname,
           'CREATE TYPE public.' || t.typname || ' AS ENUM (' ||
           string_agg(quote_literal(e.enumlabel), ', ' ORDER BY e.enumsortorder) ||
           ');' as ddl
    FROM pg_type t
    JOIN pg_enum e ON t.oid = e.enumtypid
    JOIN pg_namespace n ON t.typnamespace = n.oid
    WHERE n.nspname = 'public'
    GROUP BY t.typname;
    """
    cur.execute(query)
    enums = {typname: ddl for typname, ddl in cur.fetchall()}
    cur.close()
    return enums

# --- Enum Values ì¡°íšŒ ---
def fetch_enums_values(conn):
    """Enum íƒ€ì…ë³„ ê°’ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤."""
    cur = conn.cursor()
    # ë¨¼ì € public ìŠ¤í‚¤ë§ˆì˜ ëª¨ë“  enum íƒ€ì… ì´ë¦„ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    enum_types_query = """
    SELECT t.typname
    FROM pg_type t
    JOIN pg_namespace n ON t.typnamespace = n.oid
    WHERE n.nspname = 'public' AND t.typtype = 'e';
    """
    cur.execute(enum_types_query)
    enum_names = [row[0] for row in cur.fetchall()]
    
    enums_values = {}
    for enum_name in enum_names:
        # ê° enum íƒ€ì…ì˜ ê°’ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤.
        # psycopg2.sql ì‚¬ìš©í•˜ì—¬ ì•ˆì „í•˜ê²Œ ì‹ë³„ì ì²˜ë¦¬
        query_template = sql.SQL("SELECT enum_range(NULL::{})").format(
            sql.Identifier('public', enum_name)
        )
        try:
            cur.execute(query_template)
            # ê²°ê³¼ëŠ” íŠœí”Œ í˜•íƒœì˜ ë¦¬ìŠ¤íŠ¸ [(value1, value2, ...)] ì´ë¯€ë¡œ ì²«ë²ˆì§¸ ìš”ì†Œ ì‚¬ìš©
            values = cur.fetchone()[0] if cur.rowcount > 0 else []
            enums_values[enum_name] = sorted(values) # ì¼ê´€ëœ ë¹„êµë¥¼ ìœ„í•´ ì •ë ¬
        except psycopg2.Error as e:
            print(f"Warning: Could not fetch values for enum {enum_name}. Error: {e}")
            enums_values[enum_name] = [] # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬
            conn.rollback() # ì˜¤ë¥˜ ë°œìƒ ì‹œ íŠ¸ëœì­ì…˜ ë¡¤ë°±

    cur.close()
    return enums_values

# --- Table Metadata (ì»¬ëŸ¼ ì •ë³´) ì¡°íšŒ ---
# --- Table Metadata (ì»¬ëŸ¼ ì •ë³´) ì¡°íšŒ ---
def fetch_tables_metadata(conn):
    cur = conn.cursor()

    # 1. í…Œì´ë¸” ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    cur.execute("""
    SELECT table_name
    FROM information_schema.tables
    WHERE table_schema = 'public' AND table_type = 'BASE TABLE'
    """)
    table_names = [row[0] for row in cur.fetchall()]

    # 2. ì œì•½ì¡°ê±´ ì •ë³´: FKëŠ” ë³„ë„ ì¿¼ë¦¬ë¡œ, UNIQUE / PRIMARYëŠ” ê¸°ì¡´ ë°©ì‹
    # FK ì •ë³´ë¥¼ pg_constraintì—ì„œ ì§ì ‘ ê°€ì ¸ì™€ì„œ ì¤‘ë³µ ë°©ì§€ ë° CASCADE ì˜µì…˜ í¬í•¨
    cur.execute("""
    SELECT
        con.conname AS constraint_name,
        tbl.relname AS table_name,
        ARRAY_AGG(att.attname ORDER BY u.pos) AS columns,
        ref_tbl.relname AS ref_table_name,
        ARRAY_AGG(ref_att.attname ORDER BY u.pos) AS ref_columns,
        con.confdeltype AS on_delete,
        con.confupdtype AS on_update
    FROM pg_constraint con
    JOIN pg_class tbl ON con.conrelid = tbl.oid
    JOIN pg_namespace ns ON tbl.relnamespace = ns.oid
    JOIN pg_class ref_tbl ON con.confrelid = ref_tbl.oid
    JOIN LATERAL UNNEST(con.conkey) WITH ORDINALITY AS u(attnum, pos) ON TRUE
    JOIN pg_attribute att ON att.attrelid = tbl.oid AND att.attnum = u.attnum
    JOIN LATERAL UNNEST(con.confkey) WITH ORDINALITY AS u2(ref_attnum, pos2) ON u.pos = u2.pos2
    JOIN pg_attribute ref_att ON ref_att.attrelid = ref_tbl.oid AND ref_att.attnum = u2.ref_attnum
    WHERE con.contype = 'f'
      AND ns.nspname = 'public'
    GROUP BY con.conname, tbl.relname, ref_tbl.relname, con.confdeltype, con.confupdtype
    ORDER BY tbl.relname, con.conname;
    """)
    
    composite_fks_temp = defaultdict(list)
    for constraint_name, table, columns, ref_table, ref_columns, on_delete, on_update in cur.fetchall():
        composite_fks_temp[table].append({
            'constraint_name': constraint_name,
            'columns': columns,
            'ref_table': ref_table,
            'ref_columns': ref_columns,
            'on_delete': on_delete,
            'on_update': on_update
        })
    
    # UNIQUEì™€ PRIMARY KEYëŠ” ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì¡°íšŒ
    cur.execute("""
    SELECT
      tc.constraint_name,
      tc.constraint_type,
      tc.table_name,
      kcu.column_name,
      kcu.ordinal_position
    FROM information_schema.table_constraints AS tc
    LEFT JOIN information_schema.key_column_usage AS kcu
      ON tc.constraint_name = kcu.constraint_name AND tc.table_schema = kcu.table_schema
    WHERE tc.table_schema = 'public'
      AND tc.constraint_type IN ('UNIQUE', 'PRIMARY KEY')
    ORDER BY tc.constraint_name, kcu.ordinal_position;
    """)

    fk_lookup = {}
    unique_col_flags = {}
    primary_col_flags = {}
    composite_uniques_temp = defaultdict(list)
    composite_primaries_temp = defaultdict(list)

    for constraint_name, constraint_type, table, column, ordinal_pos in cur.fetchall():
        if constraint_type == 'UNIQUE':
            if column:
                composite_uniques_temp[(table, constraint_name)].append(column)
        elif constraint_type == 'PRIMARY KEY':
            if column:
                composite_primaries_temp[(table, constraint_name)].append(column)

    # ëª¨ë“  FKë¥¼ composite_fks_finalì— ì €ì¥ (ë‹¨ì¼ ì»¬ëŸ¼ê³¼ ë³µí•© FK ëª¨ë‘)
    composite_fks_final = defaultdict(list)
    for table, fk_list in composite_fks_temp.items():
        for fk_info in fk_list:
            cols = fk_info['columns']
            ref_table = fk_info['ref_table']
            ref_cols = fk_info['ref_columns']
            constraint_name = fk_info['constraint_name']
            on_delete = fk_info['on_delete']
            on_update = fk_info['on_update']
            
            # ë‹¨ì¼ ë° ë³µí•© FK ëª¨ë‘ composite_fks_finalì— ì €ì¥
            composite_fks_final[table].append({
                'constraint_name': constraint_name,
                'columns': cols,
                'ref_table': ref_table,
                'ref_columns': ref_cols,
                'on_delete': on_delete,
                'on_update': on_update
            })
            
            # ë‹¨ì¼ ì»¬ëŸ¼ FKëŠ” ì»¬ëŸ¼ ë©”íƒ€ë°ì´í„°ì—ë„ ê¸°ë¡ (í•˜ìœ„ í˜¸í™˜ì„±)
            if len(cols) == 1:
                fk_lookup[(table, cols[0])] = {
                    'table': ref_table, 
                    'column': ref_cols[0],
                    'on_delete': on_delete,
                    'on_update': on_update
                }

    for (table, constraint), cols in composite_uniques_temp.items():
        if len(cols) == 1:
            unique_col_flags[(table, cols[0])] = True
        elif len(cols) > 1:
            pass  # ë³µí•© í‚¤ëŠ” ë‚˜ì¤‘ì— ì²˜ë¦¬

    for (table, constraint), cols in composite_primaries_temp.items():
        if len(cols) == 1:
            primary_col_flags[(table, cols[0])] = True
        elif len(cols) > 1:
            pass  # ë³µí•© í‚¤ëŠ” ë‚˜ì¤‘ì— ì²˜ë¦¬
    
    # ìµœì¢… composite êµ¬ì¡° ìƒì„±
    final_composite_uniques = defaultdict(list)
    for (table, constraint_name), cols in composite_uniques_temp.items():
        if len(cols) > 1:
            # ì¤‘ë³µ ì œê±°í•˜ë©´ì„œ ìˆœì„œ ìœ ì§€
            seen = set()
            deduped = []
            for c in cols:
                if c not in seen:
                    seen.add(c)
                    deduped.append(c)
            final_composite_uniques[table].append((constraint_name, deduped))

    final_composite_primaries = {}
    for (table, constraint_name), cols in composite_primaries_temp.items():
        if len(cols) > 1:
            # ì¤‘ë³µ ì œê±°
            seen = set()
            deduped = []
            for c in cols:
                if c not in seen:
                    seen.add(c)
                    deduped.append(c)
            final_composite_primaries[table] = deduped

    # 3. ì»¬ëŸ¼ ì •ë³´ ìˆ˜ì§‘
    tables_metadata = {}
    for table_name in table_names:
        cur.execute("""
        SELECT column_name, data_type, is_nullable, udt_name, column_default, is_identity
        FROM information_schema.columns
        WHERE table_schema = 'public' AND table_name = %s
        ORDER BY ordinal_position;
        """, (table_name,))

        columns = []
        for col_name, data_type, is_nullable, udt_name, col_default, is_identity in cur.fetchall():
            col_type = data_type
            if data_type == 'ARRAY':
                base_type = udt_name.lstrip('_')
                col_type = base_type + '[]'

            # DEFAULT nextval('sequence_name') í˜•íƒœë¥¼ IDENTITYë¡œ ì¸ì‹
            identity_flag = is_identity == 'YES'
            if col_default and 'nextval(' in col_default:
                identity_flag = True

            col_data = {
                'name': col_name,
                'type': col_type,
                'nullable': is_nullable == 'YES',
                'default': col_default,
                'identity': identity_flag  # ìˆ˜ì •ëœ identity_flag ì‚¬ìš©
            }
            if (table_name, col_name) in fk_lookup:
                col_data['foreign_key'] = fk_lookup[(table_name, col_name)]
            if (table_name, col_name) in unique_col_flags:
                col_data['unique'] = True
            if (table_name, col_name) in primary_col_flags:
                col_data['primary_key'] = True

            columns.append(col_data)

        tables_metadata[table_name] = columns

    cur.close()
    return tables_metadata, final_composite_uniques, final_composite_primaries, composite_fks_final

# def fetch_tables_metadata(conn):
#     """í…Œì´ë¸”ë³„ ì»¬ëŸ¼ ë©”íƒ€ë°ì´í„°(ì´ë¦„, íƒ€ì…, Nullì—¬ë¶€, ê¸°ë³¸ê°’, identity, FK, UNIQUE)ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
#     cur = conn.cursor()
#     params = []
#     query_str = """
#     SELECT table_name
#     FROM information_schema.tables
#     WHERE table_schema = 'public' AND table_type='BASE TABLE'
#     """
#     if EXCLUDE_TABLES:
#         query_str += " AND table_name NOT IN %s"
#         params.append(tuple(EXCLUDE_TABLES))

#     cur.execute(query_str, params if params else None)
#     tables_metadata = {}
#     table_names = [row[0] for row in cur.fetchall()]

#     fk_lookup = {}
#     unique_lookup = set()
#     pk_lookup = set()

#     # ì „ì²´ FK ì •ë³´ ë¯¸ë¦¬ ì¡°íšŒ
#     cur.execute("""
#     SELECT
#     tc.constraint_type,
#         tc.table_name,
#         kcu.column_name,
#         ccu.table_name AS foreign_table_name,
#         ccu.column_name AS foreign_column_name
#     FROM information_schema.table_constraints AS tc
#     LEFT JOIN information_schema.key_column_usage AS kcu
#         ON tc.constraint_name = kcu.constraint_name AND tc.table_schema = kcu.table_schema
#     LEFT JOIN information_schema.constraint_column_usage AS ccu
#         ON tc.constraint_name = ccu.constraint_name AND tc.table_schema = ccu.table_schema
#     WHERE tc.table_schema = 'public';
#     """)


#     for constraint_type, table_name, column_name, ref_table, ref_column in cur.fetchall():
#         if not column_name:
#             continue  # ë³µí•© í‚¤ì˜ ì¼ë¶€ê°€ ì•„ë‹ ìˆ˜ ìˆìŒ
#         if constraint_type == 'FOREIGN KEY' and ref_table and ref_column:
#             fk_lookup[(table_name, column_name)] = {"table": ref_table, "column": ref_column}
#         elif constraint_type == 'UNIQUE':
#             unique_lookup.add((table_name, column_name))
#         elif constraint_type == 'PRIMARY KEY':
#             pk_lookup.add((table_name, column_name))


#     # í…Œì´ë¸”ë³„ ì»¬ëŸ¼ ì¡°íšŒ
#     for table_name in table_names:
#         col_query = f"""
#         SELECT column_name,
#                data_type,
#                is_nullable,
#                udt_name,
#                column_default,
#                is_identity
#         FROM information_schema.columns
#         WHERE table_schema = 'public' AND table_name = %s
#         ORDER BY ordinal_position;
#         """
#         cur.execute(col_query, (table_name,))
#         columns = []
#         for col_name, data_type, is_nullable, udt_name, col_default, is_identity in cur.fetchall():
#             col_type = data_type
#             if data_type == 'ARRAY':
#                 base_type = udt_name.lstrip('_')
#                 col_type = base_type + '[]' if base_type else 'text[]'  # fallback

#             col_data = {
#                 'name': col_name,
#                 'type': col_type,
#                 'nullable': is_nullable == 'YES',
#                 'default': col_default,
#                 'identity': is_identity == 'YES'
#             }
#             if (table_name, col_name) in fk_lookup:
#                 col_data["foreign_key"] = fk_lookup[(table_name, col_name)]
#             if (table_name, col_name) in unique_lookup:
#                 col_data["unique"] = True
#             if (table_name, col_name) in pk_lookup:
#                 col_data["primary_key"] = True  # âœ… ì—¬ê¸°ì— ì¶”ê°€
#             columns.append(col_data)
#         tables_metadata[table_name] = columns

#     cur.close()
#     return tables_metadata



# --- Table DDL ìƒì„± í•¨ìˆ˜ (ë©”íƒ€ë°ì´í„° ê¸°ë°˜ - í•„ìš” ì‹œ ì‚¬ìš©) ---
def generate_create_table_ddl(table_name, columns, 
                              composite_uniques=None, 
                              composite_primaries=None):
    """ì»¬ëŸ¼ ë©”íƒ€ë°ì´í„°ì™€ ë³µí•© ì œì•½ ì¡°ê±´ìœ¼ë¡œ CREATE TABLE DDL ìƒì„±"""
    composite_uniques = composite_uniques or {}
    composite_primaries = composite_primaries or {}

    col_defs = []
    table_constraints = []
    enum_ddls = []

    for col in columns:
        col_type = col['type']
        quoted_col_name = f'"{col["name"]}"'

        # ì‚¬ìš©ì ì •ì˜ enum íƒ€ì… ì²˜ë¦¬
        if isinstance(col_type, str) and col_type.upper() == 'USER-DEFINED':
            if table_name in ("menu_item_opts_set_schema", "menu_item_opts_schema", "cur_option_set_schema") and col['name'] == "type":
                col_type = "public.option_type"
                enum_ddls.append(
                    """DO $$
BEGIN
  IF NOT EXISTS (SELECT 1 FROM pg_type WHERE typname = 'option_type') THEN
    CREATE TYPE public.option_type AS ENUM ('additional', 'substitution');
  END IF;
END$$;"""
                )
            elif table_name == "menu" and col['name'] == "onboarding_status":
                col_type = "public.p2_onboarding_status"
                enum_ddls.append(
                    """DO $$
BEGIN
  IF NOT EXISTS (SELECT 1 FROM pg_type WHERE typname = 'p2_onboarding_status') THEN
    CREATE TYPE public.p2_onboarding_status AS ENUM ('NOT_STARTED', 'STEP1', 'STEP2', 'STEP3', 'COMPLETED');
  END IF;
END$$;"""
                )
            elif table_name in {"order_menu_items", "order_payments", "orders"} and col['name'] == "status":
                col_type = "public.order_status"
                enum_ddls.append(
                    """DO $$
BEGIN
  IF NOT EXISTS (SELECT 1 FROM pg_type WHERE typname = 'order_status') THEN
    CREATE TYPE public.order_status AS ENUM ('new', 'accepted', 'canceled', 'banned', 'cooking', 'pickup', 'prepayment', 'done');
  END IF;
END$$;"""
                )
            else:
                # ì•Œ ìˆ˜ ì—†ëŠ” USER-DEFINED íƒ€ì…ì˜ ê²½ìš° textë¡œ ëŒ€ì²´
                print(f"    âš ï¸  Unknown USER-DEFINED type for {table_name}.{col['name']}, using text instead")
                col_type = "text"

        # âœ… inline ì»¬ëŸ¼ ì •ì˜ ì²˜ë¦¬
        is_identity = col.get("identity", False)
        is_primary = col.get("primary_key", False)
        is_unique = col.get("unique", False)
        is_nullable = col.get("nullable", True)
        default_val = col.get("default")

        if is_identity and is_primary:
            col_def = f'{quoted_col_name} BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY'
        else:
            col_def = f"{quoted_col_name} {col_type}"
            if is_identity:
                col_def += " GENERATED BY DEFAULT AS IDENTITY"
            if default_val is not None and 'nextval(' not in str(default_val):
                # nextvalì´ ì•„ë‹Œ ê¸°ë³¸ê°’ë§Œ ì¶”ê°€ (nextvalì€ IDENTITYë¡œ ì²˜ë¦¬ë¨)
                col_def += f" DEFAULT {default_val}"
            if not is_nullable:
                col_def += " NOT NULL"
            if is_unique:
                col_def += " UNIQUE"
            if is_primary:
                col_def += " PRIMARY KEY"

        col_defs.append(col_def)
    print("composite_uniques",composite_uniques)
    # âœ… ë³µí•© UNIQUE ì œì•½ì¡°ê±´
    if table_name in composite_uniques:
        for constraint_name, cols in composite_uniques[table_name]:
            quoted_cols = ", ".join(f'"{c}"' for c in cols)
            table_constraints.append(
                f'CONSTRAINT "{constraint_name}" UNIQUE ({quoted_cols})'
            )

    # âœ… ë³µí•© PRIMARY KEY ì œì•½ì¡°ê±´
    if table_name in composite_primaries:
        cols = composite_primaries[table_name]
        quoted_cols = ", ".join(f'"{col}"' for col in cols)
        constraint_name = f"{table_name}_pkey"
        table_constraints.append(f'CONSTRAINT {constraint_name} PRIMARY KEY ({quoted_cols})')
    print("table_constraints",table_constraints)
    # ì „ì²´ CREATE TABLE DDL
    all_defs = col_defs + table_constraints
    table_ddl = f'CREATE TABLE public."{table_name}" (\n    ' + ",\n    ".join(all_defs) + "\n);"

    return "\n\n".join(enum_ddls + [table_ddl])


# def generate_foreign_key_ddls(tables_metadata):
#     """ëª¨ë“  foreign keyë¥¼ ALTER TABLE DDLë¡œ ìƒì„±"""
#     fk_ddls = []
#     for table_name, columns in tables_metadata.items():
#         for col in columns:
#             if "foreign_key" in col:
#                 fk = col["foreign_key"]
#                 constraint_name = f"{table_name}_{col['name']}_fkey"
#                 ddl = (
#                     f'ALTER TABLE public."{table_name}" '
#                     f'ADD CONSTRAINT "{constraint_name}" '
#                     f'FOREIGN KEY ("{col["name"]}") '
#                     f'REFERENCES public."{fk["table"]}" ("{fk["column"]}");'
#                 )
#                 fk_ddls.append(ddl)
#     return fk_ddls

# --- View DDL ì¡°íšŒ ---
def fetch_views(conn):
    """ë·° DDLì„ information_schema.views.view_definitionì„ ì‚¬ìš©í•˜ì—¬ ì¡°íšŒí•©ë‹ˆë‹¤."""
    cur = conn.cursor()
    query = """
    SELECT table_name,
           view_definition
    FROM information_schema.views
    WHERE table_schema = 'public';
    """
    cur.execute(query)
    views = {}
    for view_name, view_def in cur.fetchall():
        # view_definitionì€ SELECT ë¬¸ë§Œ í¬í•¨í•˜ë¯€ë¡œ CREATE OR REPLACE VIEW ì¶”ê°€
        # view_definition ëì— ì„¸ë¯¸ì½œë¡ ì´ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì œê±° í›„ ì¶”ê°€
        ddl = f"CREATE OR REPLACE VIEW public.{view_name} AS\n{view_def.rstrip(';')};"
        views[view_name] = ddl
    # ì¤‘ë³µ ì½”ë“œ ì œê±°: ìœ„ì—ì„œ ì´ë¯¸ views ë”•ì…”ë„ˆë¦¬ì— í• ë‹¹í•¨
    cur.close()
    return views

# --- Function DDL ì¡°íšŒ ---
def fetch_functions(conn):
    cur = conn.cursor()
    query = """
    SELECT p.proname,
           pg_get_functiondef(p.oid) as ddl
    FROM pg_proc p
    JOIN pg_namespace n ON p.pronamespace = n.oid
    JOIN pg_language l ON p.prolang = l.oid  -- Join with pg_language
    WHERE n.nspname = 'public'
      AND p.prokind = 'f'  -- Filter for regular functions
      AND l.lanname != 'c'; -- Filter out C language functions
    """

    cur.execute(query)
    functions = {proname: ddl for proname, ddl in cur.fetchall()}
    cur.close()
    return functions

# --- Index DDL ì¡°íšŒ (ê¸°ë³¸ í‚¤ ì¸ë±ìŠ¤ ë¶„ë¦¬) ---
def fetch_indexes(conn):
    """ì¸ë±ìŠ¤ DDLì„ ì¡°íšŒí•˜ë˜, UNIQUE/PRIMARY KEY ì œì•½ì¡°ê±´ìœ¼ë¡œ ìƒì„±ëœ ì¸ë±ìŠ¤ëŠ” ì œì™¸í•©ë‹ˆë‹¤."""
    cur = conn.cursor()

    # 1. constraintì—ì„œ ìƒì„±ëœ ì¸ë±ìŠ¤ ì´ë¦„ë“¤ ìˆ˜ì§‘
    cur.execute("""
    SELECT conname
    FROM pg_constraint
    WHERE contype IN ('u', 'p')  -- UNIQUE or PRIMARY KEY
      AND connamespace = 'public'::regnamespace;
    """)
    constraint_index_names = {row[0] for row in cur.fetchall()}

    # 2. pg_indexesì—ì„œ ì¼ë°˜ ì¸ë±ìŠ¤ ì¡°íšŒ
    cur.execute("""
    SELECT indexname,
           indexdef
    FROM pg_indexes
    WHERE schemaname = 'public';
    """)

    indexes = {}
    pkey_indexes = {}

    for indexname, ddl in cur.fetchall():
        if indexname in constraint_index_names:
            # âœ… UNIQUE/PK constraintì—ì„œ ìœ ë˜í•œ ì¸ë±ìŠ¤ëŠ” ë¬´ì‹œ
            continue
        if indexname.endswith('_pkey'):
            pkey_indexes[indexname] = ddl
        else:
            indexes[indexname] = ddl

    cur.close()
    return indexes, pkey_indexes


# --- Sequence DDL ì¡°íšŒ ---
def fetch_sequences(conn):
    """ì‹œí€€ìŠ¤ DDLì„ ì¡°íšŒí•©ë‹ˆë‹¤. IDENTITY ì»¬ëŸ¼ì˜ ì‹œí€€ìŠ¤ëŠ” ì œì™¸í•©ë‹ˆë‹¤."""
    cur = conn.cursor()
    
    # IDENTITY ì»¬ëŸ¼ì˜ ì‹œí€€ìŠ¤ëŠ” ìë™ìœ¼ë¡œ ìƒì„±ë˜ë¯€ë¡œ ì œì™¸
    # pg_dependë¥¼ ì‚¬ìš©í•˜ì—¬ IDENTITY ì‹œí€€ìŠ¤ í™•ì¸
    cur.execute("""
    SELECT 
        c.relname as sequence_name
    FROM pg_class c 
    JOIN pg_namespace n ON c.relnamespace = n.oid 
    WHERE n.nspname = 'public' 
      AND c.relkind = 'S'
      AND NOT EXISTS (
        -- IDENTITY ì»¬ëŸ¼ì˜ ì‹œí€€ìŠ¤ ì œì™¸ (pg_dependë¥¼ í†µí•´ IDENTITY ê´€ê³„ í™•ì¸)
        SELECT 1
        FROM pg_depend d
        JOIN pg_attribute a ON d.refobjid = a.attrelid AND d.refobjsubid = a.attnum
        WHERE d.objid = c.oid
          AND d.deptype = 'i'  -- internal dependency (IDENTITY)
          AND a.attidentity IN ('a', 'd')  -- ALWAYS or BY DEFAULT
      )
    ORDER BY c.relname;
    """)
    rows = cur.fetchall()
    
    print(f"    Raw sequence query returned {len(rows)} rows")
    
    sequences = {}
    
    for row in rows:
        seq_name = row[0]
        print(f"    Processing sequence: {seq_name}")
        
        # ì‹œí€€ìŠ¤ì˜ í˜„ì¬ ê°’ í™•ì¸
        try:
            cur.execute(f"SELECT last_value, is_called FROM public.{seq_name}")
            current_last_value, current_is_called = cur.fetchone()
            print(f"      last_value={current_last_value}, is_called={current_is_called}")
        except Exception as e:
            print(f"      Warning: Could not fetch current value for sequence {seq_name}: {e}")
            current_last_value, current_is_called = None, False
        
        # ê¸°ë³¸ CREATE SEQUENCE DDL ìƒì„±
        ddl_parts = [f"CREATE SEQUENCE public.{seq_name}"]
        
        # í˜„ì¬ ê°’ ì„¤ì • (ì‹œí€€ìŠ¤ê°€ ì´ë¯¸ ì‚¬ìš©ëœ ê²½ìš°)
        if current_is_called and current_last_value is not None:
            ddl_parts.append(f"RESTART WITH {current_last_value}")
        
        ddl = " ".join(ddl_parts) + ";"
        sequences[seq_name] = ddl
    
    cur.close()
    return sequences

def verify_sequence_values(conn, tables_metadata):
    """ì‹œí€€ìŠ¤ì˜ last_valueì™€ í…Œì´ë¸”ì˜ ìµœëŒ€ ID ê°’ì„ ë¹„êµí•˜ì—¬ ê²€ì¦í•˜ê³  í•„ìš”ì‹œ ìˆ˜ì •í•©ë‹ˆë‹¤."""
    print("\n--- Verifying and Fixing Sequence Values ---")
    
    with conn.cursor() as cur:
        for table_name, columns in tables_metadata.items():
            # IDENTITY ì»¬ëŸ¼ ì°¾ê¸°
            identity_columns = [col for col in columns if col.get('identity', False)]
            
            for col in identity_columns:
                col_name = col['name']
                seq_name = f"{table_name}_{col_name}_seq"
                
                try:
                    # ì‹œí€€ìŠ¤ì˜ last_value ì¡°íšŒ
                    cur.execute(f"SELECT last_value FROM public.{seq_name}")
                    seq_last_value = cur.fetchone()[0]
                    
                    # í…Œì´ë¸”ì˜ ìµœëŒ€ ID ê°’ ì¡°íšŒ
                    cur.execute(f"SELECT COALESCE(MAX({col_name}), 0) FROM public.{table_name}")
                    table_max_id = cur.fetchone()[0]
                    
                    print(f"  ğŸ“Š {table_name}.{col_name}:")
                    print(f"    Sequence last_value: {seq_last_value}")
                    print(f"    Table max ID: {table_max_id}")
                    
                    # ê°’ ë¹„êµ ë° ì²˜ë¦¬
                    if seq_last_value == table_max_id:
                        print(f"  âœ… {seq_name}: sequence and table values match")
                    elif seq_last_value > table_max_id:
                        print(f"  âš ï¸  {seq_name}: sequence value ({seq_last_value}) > table max ID ({table_max_id})")
                        print(f"      Keeping sequence value (data may have been deleted)")
                    else:
                        print(f"  ğŸ”§ {seq_name}: sequence value ({seq_last_value}) < table max ID ({table_max_id})")
                        print(f"      Updating sequence value to match table max ID")
                        
                        # ì‹œí€€ìŠ¤ ê°’ì„ í…Œì´ë¸” ìµœëŒ€ IDë¡œ ì—…ë°ì´íŠ¸
                        setval_sql = f"SELECT setval('public.{seq_name}', {table_max_id}, true)"
                        print(f"      Executing: {setval_sql}")
                        cur.execute(setval_sql)
                        
                        # ì—…ë°ì´íŠ¸ í›„ ê°’ í™•ì¸
                        cur.execute(f"SELECT last_value FROM public.{seq_name}")
                        new_last_value = cur.fetchone()[0]
                        print(f"      After setval: last_value={new_last_value}")
                        print(f"  âœ… {seq_name}: updated from {seq_last_value} to {table_max_id}")
                        
                except Exception as e:
                    print(f"  âŒ {seq_name}: failed to verify/fix - {e}")
                    import traceback
                    traceback.print_exc()

def initialize_sequences_after_migration(src_conn, tgt_conn, src_sequences, src_tables_meta):
    """í…Œì´ë¸” ë§ˆì´ê·¸ë ˆì´ì…˜ ì´í›„ì— ì†ŒìŠ¤ ì‹œí€€ìŠ¤ì˜ last_valueë¡œ íƒ€ê²Ÿ ì‹œí€€ìŠ¤ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    print("\n--- Initializing Sequences After Table Migration ---")
    
    # IDENTITY ì»¬ëŸ¼ì˜ ì‹œí€€ìŠ¤ ì´ˆê¸°í™”
    print(f"\n--- Initializing IDENTITY Sequence Values ---")
    with src_conn.cursor() as src_cur, tgt_conn.cursor() as tgt_cur:
        for table_name, columns in src_tables_meta.items():
            # IDENTITY ì»¬ëŸ¼ ì°¾ê¸°
            identity_columns = [col for col in columns if col.get('identity', False)]
            
            for col in identity_columns:
                col_name = col['name']
                seq_name = f"{table_name}_{col_name}_seq"
                
                try:
                    # ì†ŒìŠ¤ ì‹œí€€ìŠ¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                    src_cur.execute("""
                    SELECT 1 FROM pg_class c 
                    JOIN pg_namespace n ON c.relnamespace = n.oid 
                    WHERE n.nspname = 'public' AND c.relkind = 'S' AND c.relname = %s
                    """, (seq_name,))
                    
                    if not src_cur.fetchone():
                        print(f"  âš ï¸  {seq_name}: source sequence does not exist, skipping")
                        continue
                    
                    # íƒ€ê²Ÿ ì‹œí€€ìŠ¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                    tgt_cur.execute("""
                    SELECT 1 FROM pg_class c 
                    JOIN pg_namespace n ON c.relnamespace = n.oid 
                    WHERE n.nspname = 'public' AND c.relkind = 'S' AND c.relname = %s
                    """, (seq_name,))
                    
                    if not tgt_cur.fetchone():
                        print(f"  ğŸ”§ {seq_name}: target sequence does not exist, creating...")
                        # ì‹œí€€ìŠ¤ ìƒì„±
                        create_seq_sql = f"CREATE SEQUENCE public.{seq_name}"
                        tgt_cur.execute(create_seq_sql)
                        print(f"    Created sequence: {seq_name}")
                    
                    # ì†ŒìŠ¤ ì‹œí€€ìŠ¤ì˜ last_value ì¡°íšŒ
                    src_cur.execute(f"SELECT last_value FROM public.{seq_name}")
                    src_last_value = src_cur.fetchone()[0]
                    
                    print(f"  ğŸ“Š {table_name}.{col_name}:")
                    print(f"    Source sequence last_value: {src_last_value}")
                    
                    # íƒ€ê²Ÿ ì‹œí€€ìŠ¤ ê°’ì„ ì†ŒìŠ¤ì™€ ë™ì¼í•˜ê²Œ ì´ˆê¸°í™”
                    setval_sql = f"SELECT setval('public.{seq_name}', {src_last_value}, true)"
                    print(f"    Executing: {setval_sql}")
                    tgt_cur.execute(setval_sql)
                    
                    # ì—…ë°ì´íŠ¸ í›„ ì‹œí€€ìŠ¤ ê°’ í™•ì¸
                    tgt_cur.execute(f"SELECT last_value FROM public.{seq_name}")
                    new_last_value = tgt_cur.fetchone()[0]
                    print(f"    After setval: last_value={new_last_value}")
                    
                    print(f"  âœ… {seq_name}: initialized to {src_last_value}")
                    
                except Exception as e:
                    print(f"  âŒ {seq_name}: failed to initialize - {e}")
                    import traceback
                    traceback.print_exc()
    
    # ëª…ì‹œì  ì‹œí€€ìŠ¤ ì´ˆê¸°í™”
    if src_sequences:
        print(f"\n--- Initializing Explicit Sequence Values ---")
        print(f"Source sequences: {list(src_sequences.keys())}")
        
        with src_conn.cursor() as src_cur, tgt_conn.cursor() as tgt_cur:
            for seq_name in src_sequences.keys():
                try:
                    # ì†ŒìŠ¤ ì‹œí€€ìŠ¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                    src_cur.execute("""
                    SELECT 1 FROM pg_class c 
                    JOIN pg_namespace n ON c.relnamespace = n.oid 
                    WHERE n.nspname = 'public' AND c.relkind = 'S' AND c.relname = %s
                    """, (seq_name,))
                    
                    if not src_cur.fetchone():
                        print(f"  âš ï¸  {seq_name}: source sequence does not exist, skipping")
                        continue
                    
                    # íƒ€ê²Ÿ ì‹œí€€ìŠ¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                    tgt_cur.execute("""
                    SELECT 1 FROM pg_class c 
                    JOIN pg_namespace n ON c.relnamespace = n.oid 
                    WHERE n.nspname = 'public' AND c.relkind = 'S' AND c.relname = %s
                    """, (seq_name,))
                    
                    if not tgt_cur.fetchone():
                        print(f"  ğŸ”§ {seq_name}: target sequence does not exist, creating...")
                        # ì‹œí€€ìŠ¤ ìƒì„±
                        create_seq_sql = f"CREATE SEQUENCE public.{seq_name}"
                        tgt_cur.execute(create_seq_sql)
                        print(f"    Created sequence: {seq_name}")
                    
                    # ì†ŒìŠ¤ ì‹œí€€ìŠ¤ì˜ í˜„ì¬ ê°’ ì¡°íšŒ
                    src_cur.execute(f"SELECT last_value, is_called FROM public.{seq_name}")
                    src_last_value, src_is_called = src_cur.fetchone()
                    
                    print(f"  ğŸ“Š {seq_name}:")
                    print(f"    Source: last_value={src_last_value}, is_called={src_is_called}")
                    
                    # íƒ€ê²Ÿ ì‹œí€€ìŠ¤ ê°’ì„ ì†ŒìŠ¤ì™€ ë™ì¼í•˜ê²Œ ì´ˆê¸°í™”
                    setval_sql = f"SELECT setval('public.{seq_name}', {src_last_value}, {src_is_called})"
                    print(f"    Executing: {setval_sql}")
                    tgt_cur.execute(setval_sql)
                    
                    # ì—…ë°ì´íŠ¸ í›„ ê°’ í™•ì¸
                    tgt_cur.execute(f"SELECT last_value, is_called FROM public.{seq_name}")
                    new_tgt_last_value, new_tgt_is_called = tgt_cur.fetchone()
                    print(f"    After setval: last_value={new_tgt_last_value}, is_called={new_tgt_is_called}")
                    
                    print(f"  âœ… {seq_name}: initialized to {src_last_value}")
                    
                except Exception as e:
                    print(f"  âŒ {seq_name}: failed to initialize - {e}")
                    import traceback
                    traceback.print_exc()

def sync_identity_sequence_values(src_conn, tgt_conn, tables_metadata):
    """IDENTITY ì»¬ëŸ¼ì˜ ì‹œí€€ìŠ¤ ê°’ì„ ì†ŒìŠ¤ì—ì„œ íƒ€ê²Ÿìœ¼ë¡œ ë™ê¸°í™”í•©ë‹ˆë‹¤."""
    print("\n--- Syncing IDENTITY Sequence Values ---")
    
    with src_conn.cursor() as src_cur, tgt_conn.cursor() as tgt_cur:
        for table_name, columns in tables_metadata.items():
            # IDENTITY ì»¬ëŸ¼ ì°¾ê¸°
            identity_columns = [col for col in columns if col.get('identity', False)]
            
            for col in identity_columns:
                col_name = col['name']
                seq_name = f"{table_name}_{col_name}_seq"
                
                try:
                    # ì†ŒìŠ¤ ì‹œí€€ìŠ¤ì˜ last_value ì¡°íšŒ
                    src_cur.execute(f"SELECT last_value FROM public.{seq_name}")
                    src_last_value = src_cur.fetchone()[0]
                    
                    # ì†ŒìŠ¤ í…Œì´ë¸”ì˜ ìµœëŒ€ ID ê°’ ì¡°íšŒ
                    src_cur.execute(f"SELECT COALESCE(MAX({col_name}), 0) FROM public.{table_name}")
                    src_max_id = src_cur.fetchone()[0]
                    
                    print(f"  ğŸ“Š {table_name}.{col_name}:")
                    print(f"    Source sequence last_value: {src_last_value}")
                    print(f"    Source table max ID: {src_max_id}")
                    
                    # ì†ŒìŠ¤ ì‹œí€€ìŠ¤ì˜ last_valueì™€ ì†ŒìŠ¤ í…Œì´ë¸”ì˜ MAX(id) ë¹„êµ
                    if src_last_value > src_max_id:
                        # last_valueê°€ ë” í¬ë©´ last_valueë¥¼ ì‚¬ìš©
                        target_value = src_last_value
                        print(f"    Using sequence last_value ({src_last_value}) > table max ID ({src_max_id})")
                    else:
                        # MAX(id)ê°€ ë” í¬ë©´ MAX(id)ë¥¼ ì‚¬ìš©
                        target_value = src_max_id
                        print(f"    Using table max ID ({src_max_id}) >= sequence last_value ({src_last_value})")
                    
                    # íƒ€ê²Ÿ ì‹œí€€ìŠ¤ ê°’ì„ ì„¤ì •
                    setval_sql = f"SELECT setval('public.{seq_name}', {target_value}, true)"
                    print(f"    Executing: {setval_sql}")
                    tgt_cur.execute(setval_sql)
                    
                    # ì—…ë°ì´íŠ¸ í›„ ì‹œí€€ìŠ¤ ê°’ í™•ì¸
                    tgt_cur.execute(f"SELECT last_value FROM public.{seq_name}")
                    new_last_value = tgt_cur.fetchone()[0]
                    print(f"    After setval: last_value={new_last_value}")
                    
                    print(f"  âœ… {seq_name}: set to {target_value}")
                        
                except Exception as e:
                    print(f"  âŒ {seq_name}: failed to sync - {e}")
                    import traceback
                    traceback.print_exc()

def sync_sequence_values(src_conn, tgt_conn, sequence_names):
    """ì‹œí€€ìŠ¤ì˜ í˜„ì¬ ê°’ì„ ì†ŒìŠ¤ì—ì„œ íƒ€ê²Ÿìœ¼ë¡œ ë™ê¸°í™”í•©ë‹ˆë‹¤."""
    print("\n--- Syncing Sequence Values ---")
    
    with src_conn.cursor() as src_cur, tgt_conn.cursor() as tgt_cur:
        for seq_name in sequence_names:
            try:
                # ì†ŒìŠ¤ ì‹œí€€ìŠ¤ì˜ í˜„ì¬ ê°’ ì¡°íšŒ
                src_cur.execute(f"SELECT last_value, is_called FROM public.{seq_name}")
                src_last_value, src_is_called = src_cur.fetchone()
                
                # íƒ€ê²Ÿ ì‹œí€€ìŠ¤ì˜ í˜„ì¬ ê°’ ì¡°íšŒ
                tgt_cur.execute(f"SELECT last_value, is_called FROM public.{seq_name}")
                tgt_last_value, tgt_is_called = tgt_cur.fetchone()
                
                print(f"  ğŸ“Š {seq_name}:")
                print(f"    Source: last_value={src_last_value}, is_called={src_is_called}")
                print(f"    Target: last_value={tgt_last_value}, is_called={tgt_is_called}")
                
                # ê°’ì´ ë‹¤ë¥¸ ê²½ìš°ì—ë§Œ ì—…ë°ì´íŠ¸
                if src_last_value != tgt_last_value:
                    # ì‹œí€€ìŠ¤ ê°’ì„ ì†ŒìŠ¤ì™€ ë™ì¼í•˜ê²Œ ì„¤ì •
                    setval_sql = f"SELECT setval('public.{seq_name}', {src_last_value}, {src_is_called})"
                    print(f"    Executing: {setval_sql}")
                    tgt_cur.execute(setval_sql)
                    
                    # ì—…ë°ì´íŠ¸ í›„ ê°’ í™•ì¸
                    tgt_cur.execute(f"SELECT last_value, is_called FROM public.{seq_name}")
                    new_tgt_last_value, new_tgt_is_called = tgt_cur.fetchone()
                    print(f"    After setval: last_value={new_tgt_last_value}, is_called={new_tgt_is_called}")
                    
                    print(f"  âœ… {seq_name}: {tgt_last_value} â†’ {src_last_value}")
                else:
                    print(f"  â­ï¸  {seq_name}: already synced ({src_last_value})")
                    
            except Exception as e:
                print(f"  âŒ {seq_name}: failed to sync - {e}")
                import traceback
                traceback.print_exc()

def cleanup_duplicate_sequences(conn):
    """íƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¤‘ë³µëœ ì‹œí€€ìŠ¤ë“¤ì„ ì •ë¦¬í•©ë‹ˆë‹¤."""
    print("\n--- Cleaning up duplicate sequences ---")
    
    with conn.cursor() as cur:
        # ì¤‘ë³µëœ ì‹œí€€ìŠ¤ ì°¾ê¸° (ì´ë¦„ì´ _seq1ë¡œ ëë‚˜ëŠ” ê²ƒë“¤)
        cur.execute("""
        SELECT c.relname
        FROM pg_class c 
        JOIN pg_namespace n ON c.relnamespace = n.oid 
        WHERE n.nspname = 'public' 
          AND c.relkind = 'S'
          AND c.relname LIKE '%_seq1'
        ORDER BY c.relname
        """)
        duplicate_sequences = [row[0] for row in cur.fetchall()]
        
        if duplicate_sequences:
            print(f"Found {len(duplicate_sequences)} duplicate sequences:")
            for seq_name in duplicate_sequences:
                print(f"  - {seq_name}")
            
            # ì¤‘ë³µëœ ì‹œí€€ìŠ¤ë“¤ ì‚­ì œ
            for seq_name in duplicate_sequences:
                try:
                    cur.execute(f"DROP SEQUENCE IF EXISTS public.{seq_name} CASCADE")
                    print(f"  âœ… Dropped: {seq_name}")
                except Exception as e:
                    print(f"  âŒ Failed to drop {seq_name}: {e}")
            
            conn.commit()
            print("Duplicate sequences cleanup completed.")
        else:
            print("No duplicate sequences found.")

# --- ì•ˆì „í•œ íƒ€ì… ë³€ê²½ íŒë‹¨ í•¨ìˆ˜ ---
def is_safe_type_change(old_type, new_type):
    """ì•”ì‹œì  ë³€í™˜ì´ ê°€ëŠ¥í•˜ê³  ì•ˆì „í•œ íƒ€ì… ë³€ê²½ì¸ì§€ íŒë‹¨í•©ë‹ˆë‹¤."""
    old_type_norm = normalize_sql(old_type)
    new_type_norm = normalize_sql(new_type)

    # varchar ê¸¸ì´ ì¦ê°€ ë˜ëŠ” textë¡œ ë³€ê²½
    if old_type_norm.startswith('character varying') and (new_type_norm.startswith('character varying') or new_type_norm == 'text'):
        try:
            old_len_match = re.search(r'\((\d+)\)', old_type_norm)
            new_len_match = re.search(r'\((\d+)\)', new_type_norm)
            old_len = int(old_len_match.group(1)) if old_len_match else float('inf')
            new_len = int(new_len_match.group(1)) if new_len_match else float('inf')
            # ê¸¸ì´ê°€ ê°™ê±°ë‚˜ ì¦ê°€í•˜ëŠ” ê²½ìš° ë˜ëŠ” textë¡œ ë³€ê²½í•˜ëŠ” ê²½ìš° ì•ˆì „
            return new_len >= old_len or new_type_norm == 'text'
        except:
            return False # ê¸¸ì´ íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì•ˆì „í•˜ì§€ ì•ŠìŒìœ¼ë¡œ ê°„ì£¼
    # ìˆ«ì íƒ€ì… í™•ì¥ (smallint -> int -> bigint)
    elif old_type_norm == 'smallint' and new_type_norm in ['integer', 'bigint']:
        return True
    elif old_type_norm == 'integer' and new_type_norm == 'bigint':
        return True
    # ìˆ«ì -> ë¬¸ìì—´ (ì¼ë°˜ì ìœ¼ë¡œ ì•ˆì „)
    elif old_type_norm in ['smallint', 'integer', 'bigint', 'numeric', 'real', 'double precision'] and \
         (new_type_norm.startswith('character varying') or new_type_norm == 'text'):
         return True
    # TODO: ë‹¤ë¥¸ ì•ˆì „í•œ ë³€í™˜ ì¶”ê°€ ê°€ëŠ¥ (ì˜ˆ: timestamp -> timestamptz)

    return False # ê·¸ ì™¸ëŠ” ì•ˆì „í•˜ì§€ ì•ŠìŒìœ¼ë¡œ ê°„ì£¼

# --- ë¹„êµ í›„ migration SQL ìƒì„± (íƒ€ì…ë³„ ë¡œì§ ë¶„ê¸°, Enum DDL ì°¸ì¡° ì¶”ê°€, ALTER TABLE ì§€ì› ì¶”ê°€) ---
def compare_and_generate_migration(src_data, tgt_data, obj_type, src_enum_ddls=None, use_alter=False,
                                 src_composite_uniques=None, tgt_composite_uniques=None,
                                 src_composite_primaries=None, tgt_composite_primaries=None,
                                 fk_not_valid=False):
    """
    ì†ŒìŠ¤ì™€ íƒ€ê²Ÿ ë°ì´í„°ë¥¼ ë¹„êµí•˜ì—¬ ë§ˆì´ê·¸ë ˆì´ì…˜ SQLê³¼ ê±´ë„ˆë›´ SQLì„ ìƒì„±í•©ë‹ˆë‹¤.
    obj_typeì— ë”°ë¼ ë¹„êµ ë°©ì‹ì„ ë‹¤ë¥´ê²Œ ì ìš©í•©ë‹ˆë‹¤.
    use_alter=Trueì¼ ê²½ìš°, í…Œì´ë¸” ì»¬ëŸ¼ ì¶”ê°€/ì‚­ì œì— ëŒ€í•´ ALTER TABLE ì‚¬ìš© ì‹œë„.
    Enum íƒ€ì…ì˜ DDL ìƒì„±ì„ ìœ„í•´ src_enum_ddls ë”•ì…”ë„ˆë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.
    fk_not_valid=Trueì´ë©´ FOREIGN KEYë¥¼ NOT VALIDë¡œ ìƒì„±í•©ë‹ˆë‹¤.
    """
    migration_sql = []
    skipped_sql = []
    alter_statements = [] # í•¨ìˆ˜ ì‹œì‘ ì‹œ ì´ˆê¸°í™”
    src_keys = set(src_data.keys())
    tgt_keys = set(tgt_data.keys())
    
    # ì‹œí€€ìŠ¤ ì¤‘ë³µ ì²˜ë¦¬ ë°©ì§€ë¥¼ ìœ„í•œ ì¶”ì 
    processed_sequences = set()

    print(f"    DEBUG: src_keys count={len(src_keys)}, tgt_keys count={len(tgt_keys)}")
    print(f"    DEBUG: source_only={len(src_keys - tgt_keys)}, both_sides={len(src_keys.intersection(tgt_keys))}")

    # ì†ŒìŠ¤ì—ë§Œ ìˆëŠ” ê°ì²´ ì²˜ë¦¬
    for name in src_keys - tgt_keys:
        print(f"  ğŸ” Processing {obj_type}: {name} (source only)")
        if obj_type == "SEQUENCE":
            if name in processed_sequences:
                print(f"    âš ï¸  SEQUENCE {name} already processed, skipping!")
                continue
            processed_sequences.add(name)
            print(f"    ğŸ“ Creating SEQUENCE: {name}")
        if obj_type == "TABLE":
            ddl = generate_create_table_ddl(
                        name,
                        src_data[name],
                        composite_uniques=src_composite_uniques,
                        composite_primaries=src_composite_primaries
                        )

        elif obj_type == "TYPE": # ì†ŒìŠ¤ì—ë§Œ ìˆëŠ” Enum ì²˜ë¦¬
            ddl = src_enum_ddls.get(name, f"-- ERROR: DDL not found for Enum {name}")
        elif obj_type == "SEQUENCE": # ì†ŒìŠ¤ì—ë§Œ ìˆëŠ” Sequence ì²˜ë¦¬
            raw_ddl = src_data.get(name, f"-- ERROR: DDL not found for Sequence {name}")
            # í…Œì´ë¸” ìƒì„± ì‹œ ìë™ìœ¼ë¡œ ìƒì„±ëœ ì‹œí€€ìŠ¤ê°€ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ê°’ë§Œ ì—…ë°ì´íŠ¸
            restart_match = re.search(r'RESTART WITH (\d+)', raw_ddl)
            if restart_match:
                restart_value = restart_match.group(1)
                ddl = f"""
                        DO $$
                        BEGIN
                            -- ì‹œí€€ìŠ¤ê°€ ì¡´ì¬í•˜ë©´ ê°’ë§Œ ì—…ë°ì´íŠ¸
                            IF EXISTS (
                                SELECT 1 FROM pg_class c 
                                JOIN pg_namespace n ON c.relnamespace = n.oid 
                                WHERE n.nspname = 'public' AND c.relkind = 'S' AND c.relname = '{name}'
                            ) THEN
                                ALTER SEQUENCE public.{name} RESTART WITH {restart_value};
                            END IF;
                        END$$;
                        """.strip()
            else:
                # RESTART WITHê°€ ì—†ìœ¼ë©´ ì•„ë¬´ê²ƒë„ í•˜ì§€ ì•ŠìŒ (ì‹œí€€ìŠ¤ê°€ ìë™ìœ¼ë¡œ ìƒì„±ë¨)
                ddl = f"-- Sequence {name} is auto-generated by table, skipping explicit creation"
        elif obj_type == "INDEX":
            raw_ddl = src_data.get(name, f"-- ERROR: DDL not found for Index {name}")
            ddl = f"""
                    DO $$
                    BEGIN
                        IF NOT EXISTS (
                            SELECT 1 FROM pg_indexes WHERE schemaname = 'public' AND indexname = '{name}'
                        ) THEN
                            {raw_ddl};
                        END IF;
                    END$$;
                    """.strip()    
        else: # View, Function, Index ë“±
            ddl = src_data.get(name, f"-- ERROR: DDL not found for {obj_type} {name}")
        if obj_type == "FOREIGN_KEY" and fk_not_valid:
            ddl = add_not_valid_constraint(ddl)
        migration_sql.append(f"-- CREATE {obj_type} {name}\n{ddl}\n")

    # ì–‘ìª½ì— ëª¨ë‘ ìˆëŠ” ê°ì²´ ë¹„êµ ì²˜ë¦¬
    for name in src_keys.intersection(tgt_keys):
        print(f"  ğŸ” Processing {obj_type}: {name} (both sides)")
        if obj_type == "SEQUENCE":
            if name in processed_sequences:
                print(f"    âš ï¸  SEQUENCE {name} already processed, skipping!")
                continue
            processed_sequences.add(name)
            print(f"    ğŸ”„ Comparing SEQUENCE: {name}")
        are_different = False
        ddl = "" # ë³€ê²½ ì‹œ ì‚¬ìš©í•  DDL (ì£¼ë¡œ ì†ŒìŠ¤ ê¸°ì¤€)

        if obj_type == "TABLE":
            src_cols_map = {col['name']: col for col in src_data[name]}
            tgt_cols_map = {col['name']: col for col in tgt_data[name]}
            src_col_names = set(src_cols_map.keys())
            tgt_col_names = set(tgt_cols_map.keys())

            cols_to_add = src_col_names - tgt_col_names
            cols_to_drop = tgt_col_names - src_col_names
            cols_to_compare = src_col_names.intersection(tgt_col_names)

            # alter_statements = [] # ì—¬ê¸°ì„œ ì´ˆê¸°í™” ì œê±°
            needs_recreate = False # ALTERë¡œ ì²˜ë¦¬ ë¶ˆê°€ëŠ¥í•œ ë³€ê²½ì´ ìˆëŠ”ì§€ ì—¬ë¶€

            # ê³µí†µ ì»¬ëŸ¼ì´ í•˜ë‚˜ë„ ì—†ê³ , ì¶”ê°€/ì‚­ì œí•  ì»¬ëŸ¼ì´ ìˆë‹¤ë©´ ì¬ ìƒì„± í•„ìš” (ë²„ê·¸ ìˆ˜ì •)
            if not cols_to_compare and (cols_to_add or cols_to_drop):
                 needs_recreate = True

            # ì»¬ëŸ¼ ì •ì˜ ë¹„êµ (íƒ€ì…, Null ì—¬ë¶€ ë“±) - needs_recreateê°€ ì•„ì§ Falseì¼ ë•Œë§Œ ìˆ˜í–‰
            if not needs_recreate:
                for col_name in cols_to_compare:
                    src_col = src_cols_map[col_name]
                    tgt_col = tgt_cols_map[col_name]
                    src_type_norm = normalize_sql(src_col['type'])
                    tgt_type_norm = normalize_sql(tgt_col['type'])

                    # 1. íƒ€ì… ë³€ê²½ í™•ì¸
                    if src_type_norm != tgt_type_norm:
                        if use_alter and is_safe_type_change(tgt_type_norm, src_type_norm):
                            # ì•ˆì „í•œ íƒ€ì… ë³€ê²½ì´ë©´ ALTER TYPE ì¶”ê°€
                            quoted_col_name = f'"{col_name}"' # ë”°ì˜´í‘œ ì¶”ê°€
                            alter_statements.append(f"ALTER TABLE public.{name} ALTER COLUMN {quoted_col_name} TYPE {src_col['type']};")
                        else:
                            # ì•ˆì „í•˜ì§€ ì•Šì€ íƒ€ì… ë³€ê²½ì´ë©´ ì¬ ìƒì„± í•„ìš”
                            needs_recreate = True
                            break

                    # 2. Null í—ˆìš© ì—¬ë¶€ ë³€ê²½ í™•ì¸ (íƒ€ì…ì´ ë™ì¼í•  ë•Œë§Œ ê³ ë ¤)
                    elif src_col['nullable'] != tgt_col['nullable']:
                        if use_alter:
                            if src_col['nullable'] is False: # NOT NULLë¡œ ë³€ê²½
                                alter_statements.append(f"-- WARNING: Setting NOT NULL on column {col_name} may fail if existing data contains NULLs.")
                                quoted_col_name = f'"{col_name}"' # ë”°ì˜´í‘œ ì¶”ê°€
                                alter_statements.append(f"ALTER TABLE public.{name} ALTER COLUMN {quoted_col_name} SET NOT NULL;")
                            else: # NULL í—ˆìš©ìœ¼ë¡œ ë³€ê²½
                                quoted_col_name = f'"{col_name}"' # ë”°ì˜´í‘œ ì¶”ê°€
                                alter_statements.append(f"ALTER TABLE public.{name} ALTER COLUMN {quoted_col_name} DROP NOT NULL;")
                        else:
                             # use_alter=False ì´ë©´ ì¬ ìƒì„± í•„ìš”
                             needs_recreate = True
                             break

            # ALTER ë¬¸ ìƒì„± (ì»¬ëŸ¼ ì¶”ê°€/ì‚­ì œ) - needs_recreateê°€ Falseì´ê³  use_alter=Trueì¼ ë•Œë§Œ
            if not needs_recreate and use_alter:
                if cols_to_add:
                    for col_name in cols_to_add:
                        col = src_cols_map[col_name]
                        col_def = f"{col['name']} {col['type']}"
                        if col['default'] is not None:
                            col_def += f" DEFAULT {col['default']}"
                        if not col['nullable']:
                            col_def += " NOT NULL"
                        # sql.Identifier ì‚¬ìš© ìœ„í•´ conn ê°ì²´ í•„ìš” -> ì„ì‹œ ì²˜ë¦¬ (f-string ì˜¤ë¥˜ ìˆ˜ì •)
                        default_clause = f" DEFAULT {col['default']}" if col.get('default') is not None else ""
                        not_null_clause = " NOT NULL" if not col.get('nullable', True) else ""
                        # ì»¬ëŸ¼ ì´ë¦„ì— ë”°ì˜´í‘œ ì¶”ê°€ (psycopg2.sql.Identifier ëŒ€ì‹  ì„ì‹œ ì‚¬ìš©)
                        quoted_col_name = f'"{col_name}"'
                        alter_statements.append(f"ALTER TABLE public.{name} ADD COLUMN {quoted_col_name} {col['type']}{default_clause}{not_null_clause};")
                if cols_to_drop:
                    for col_name in cols_to_drop:
                        # ì»¬ëŸ¼ ì‚­ì œëŠ” ìœ„í—˜í•˜ë¯€ë¡œ ì£¼ì„ ì¶”ê°€
                        alter_statements.append(f"-- WARNING: Dropping column {col_name} may cause data loss.")
                        # ì»¬ëŸ¼ ì´ë¦„ì— ë”°ì˜´í‘œ ì¶”ê°€ (psycopg2.sql.Identifier ëŒ€ì‹  ì„ì‹œ ì‚¬ìš©)
                        quoted_col_name = f'"{col_name}"'
                        alter_statements.append(f"ALTER TABLE public.{name} DROP COLUMN {quoted_col_name};")

                if alter_statements: # ALTER ë¬¸ì´ ìƒì„±ëœ ê²½ìš° (ì¶”ê°€/ì‚­ì œ/ë³€ê²½ í¬í•¨)
                    migration_sql.append(f"-- ALTER TABLE {name} for column changes\n" + "\n".join(alter_statements) + "\n")
                    are_different = True # ë§ˆì´ê·¸ë ˆì´ì…˜ SQLì´ ìƒì„±ë˜ì—ˆìœ¼ë¯€ë¡œ differentë¡œ ì²˜ë¦¬
                else:
                    # ALTER ë¬¸ ì—†ê³ , needs_recreateë„ Falseì´ë©´ ë³€ê²½ ì—†ìŒ
                    are_different = False

            # ì¬ ìƒì„± í•„ìš” ì—¬ë¶€ ìµœì¢… ê²°ì •
            # needs_recreateê°€ Trueì´ë©´ ë¬´ì¡°ê±´ ì¬ ìƒì„±
            if needs_recreate:
                are_different = True
                ddl = generate_create_table_ddl(
                    name,
                    src_data[name],
                    composite_uniques=src_composite_uniques,
                        composite_primaries=src_composite_primaries
                )

                alter_statements = [] # ALTER ë¬¸ì€ ë¬´ì‹œ
            # use_alter=False ì´ê³  ì»¬ëŸ¼ êµ¬ì„±ì´ ë‹¤ë¥´ë©´ ì¬ ìƒì„±
            elif not use_alter and (len(src_cols_map) != len(tgt_cols_map) or \
                                    any(sc['name'] != tc['name'] or \
                                        normalize_sql(sc['type']) != normalize_sql(tc['type']) or \
                                        sc['nullable'] != tc['nullable']
                                        for sc, tc in zip(src_data[name], tgt_data[name]))):
                 are_different = True
                 ddl = generate_create_table_ddl(
                        name,
                        src_data[name],
                        composite_uniques=src_composite_uniques,
                        composite_primaries=src_composite_primaries
                        )

                 alter_statements = [] # ALTER ë¬¸ì€ ë¬´ì‹œ
            elif not alter_statements:
                 # ì¬ ìƒì„± í•„ìš” ì—†ê³ , ALTER ë¬¸ë„ ì—†ìœ¼ë©´ ë³€ê²½ ì—†ìŒ
                 are_different = False
        elif obj_type == "INDEX":
            if name not in tgt_data:
                ddl = src_data[name]
                ddl = f"""
                        DO $$
                        BEGIN
                            IF NOT EXISTS (
                                SELECT 1 FROM pg_indexes WHERE schemaname = 'public' AND indexname = '{name}'
                            ) THEN
                                {ddl};
                            END IF;
                        END$$;
                        """.strip()
                migration_sql.append(f"-- INDEX {name} differs or missing. Adding.\n{ddl}\n")
                continue
            else:
                if normalize_sql(src_data[name]) != normalize_sql(tgt_data[name]):
                    ddl = src_data[name]
                    ddl = f"""
                            DO $$
                            BEGIN
                                IF NOT EXISTS (
                                    SELECT 1 FROM pg_indexes WHERE schemaname = 'public' AND indexname = '{name}'
                                ) THEN
                                    {ddl};
                                END IF;
                            END$$;
                            """.strip()
                    migration_sql.append(f"-- INDEX {name} differs. Replacing.\n{ddl}\n")
                else:
                    commented = '\n'.join([f"-- {line}" for line in src_data[name].strip().splitlines()])
                    skipped_sql.append(f"-- INDEX {name} is up-to-date; skipping.\n{commented}\n")
                continue
        elif obj_type == "TYPE": # Enum íƒ€ì… ê°€ì •
            src_values = src_data[name]
            tgt_values = tgt_data[name]
            if src_values != tgt_values:
                are_different = True
                # Enum DDLì€ src_enum_ddls ì—ì„œ ê°€ì ¸ì˜´
                ddl = src_enum_ddls.get(name, f"-- ERROR: DDL not found for Enum {name}")
        elif obj_type == "FUNCTION":
            # í•¨ìˆ˜ëŠ” ì›ë³¸ DDLë¡œ ë¹„êµ (ì •ê·œí™” ì‹œ ë‹¬ëŸ¬ ì¸ìš© ë¬¸ì œ ë°œìƒ ê°€ëŠ¥ì„±)
            if src_data[name] != tgt_data[name]:
                are_different = True
                ddl = src_data[name]
        elif obj_type == "FOREIGN_KEY":
            if name not in tgt_data:
                are_different = True
                ddl = src_data[name]
            else:
                src_ddl = normalize_sql(src_data[name])
                tgt_ddl = normalize_sql(tgt_data[name])
                if src_ddl != tgt_ddl:
                    are_different = True
                    ddl = src_data[name]

            if are_different:
                if fk_not_valid:
                    ddl = add_not_valid_constraint(ddl)
                # âœ… DROP ì—†ì´ ì¶”ê°€ë§Œ ì‹œë„
                migration_sql.append(f"-- FOREIGN_KEY {name} differs or missing. Adding.\n{ddl}\n")
            else:
                # ìŠ¤í‚µ ì²˜ë¦¬
                commented = '\n'.join([f"-- {line}" for line in src_data[name].strip().splitlines()])
                skipped_sql.append(f"-- FOREIGN_KEY {name} is up-to-date; skipping.\n{commented}\n")
            
            continue  # ğŸ‘ˆ ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ ì´í›„ ê³µí†µ ì²˜ë¦¬ ë¸”ë¡ ê±´ë„ˆëœ€
        elif obj_type == "SEQUENCE": # ì–‘ìª½ì— ìˆëŠ” Sequence ì²˜ë¦¬
            print(f"  ğŸ” Processing SEQUENCE: {name}")
            print(f"    Source DDL: {src_data[name]}")
            print(f"    Target DDL: {tgt_data[name]}")
            # ì‹œí€€ìŠ¤ê°€ í…Œì´ë¸”ì—ì„œ ì‚¬ìš© ì¤‘ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ DROP ëŒ€ì‹  ALTER ì‚¬ìš©
            src_ddl_norm = normalize_sql(src_data[name])
            tgt_ddl_norm = normalize_sql(tgt_data[name])
            print(f"    Normalized Source: {src_ddl_norm}")
            print(f"    Normalized Target: {tgt_ddl_norm}")
            if src_ddl_norm != tgt_ddl_norm:
                print(f"    SEQUENCE {name} differs, using ALTER")
                # RESTART WITH ê°’ë§Œ ì¶”ì¶œí•˜ì—¬ ALTER SEQUENCE ì‚¬ìš©
                restart_match = re.search(r'RESTART WITH (\d+)', src_data[name])
                if restart_match:
                    restart_value = restart_match.group(1)
                    ddl = f"ALTER SEQUENCE public.{name} RESTART WITH {restart_value};"
                    migration_sql.append(f"-- ALTER SEQUENCE {name} to sync current value\n{ddl}\n")
                else:
                    # RESTART WITHê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ CREATE SEQUENCE ì‚¬ìš©
                    ddl = src_data[name]
                    migration_sql.append(f"-- SEQUENCE {name} differs. Recreating.\nDROP SEQUENCE IF EXISTS public.{name} CASCADE;\n{ddl}\n")
            else:
                print(f"    SEQUENCE {name} is identical, skipping")
                # ë™ì¼í•œ ê²½ìš° ìŠ¤í‚µ
                commented = '\n'.join([f"-- {line}" for line in src_data[name].strip().splitlines()])
                skipped_sql.append(f"-- SEQUENCE {name} is up-to-date; skipping.\n{commented}\n")
            continue  # ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ ì´í›„ ê³µí†µ ì²˜ë¦¬ ë¸”ë¡ ê±´ë„ˆëœ€
        else:
            # ë‚˜ë¨¸ì§€ íƒ€ì… (View, Function, Index, Sequence ë“±
            if obj_type == "SEQUENCE":
                print(f"  âš ï¸  SEQUENCE {name} being processed again in else block - this should not happen!")
                # ì‹œí€€ìŠ¤ëŠ” ìœ„ì—ì„œ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆìœ¼ë¯€ë¡œ ìŠ¤í‚µ
                continue
                
            src_ddl_norm = normalize_sql(src_data[name])
            tgt_ddl_norm = normalize_sql(tgt_data[name])
            if src_ddl_norm != tgt_ddl_norm:
                are_different = True
                ddl = src_data[name] # ë³€ê²½ ì‹œ ì†ŒìŠ¤ DDL ì‚¬ìš©

        # ë¹„êµ ê²°ê³¼ì— ë”°ë¼ SQL ìƒì„± (TABLE íƒ€ì…ì€ ìœ„ì—ì„œ ì²˜ë¦¬ë¨)
        if obj_type == "FOREIGN_KEY" and are_different:
            # FOREIGN KEYëŠ” DROP CONSTRAINT ì—†ì´ ê·¸ëƒ¥ ADD CONSTRAINTë§Œ ì‹œë„
            migration_sql.append(f"-- FOREIGN_KEY {name} differs or missing. Adding.\n{ddl}\n")
        elif obj_type != "TABLE" and are_different:
            # TABLE ì™¸ ë‹¤ë¥¸ íƒ€ì…ì´ ë‹¤ë¥´ê±°ë‚˜, TABLEì´ ALTER ë¶ˆê°€í•˜ì—¬ ì¬ ìƒì„± í•„ìš”í•œ ê²½ìš°
            action = "Recreating" if obj_type != "FUNCTION" else "Updating" # í•¨ìˆ˜ëŠ” Updateë¡œ í‘œì‹œ (DROP/CREATE ë™ì¼)
            migration_sql.append(f"-- {obj_type} {name} differs. {action}.\nDROP {obj_type.upper()} IF EXISTS public.{name} CASCADE;\n{ddl}\n")
        elif obj_type == "TABLE" and are_different and not alter_statements:
             # TABLEì´ ë‹¤ë¥´ì§€ë§Œ ALTER ë¬¸ì´ ìƒì„±ë˜ì§€ ì•Šì€ ê²½ìš° (ì¬ ìƒì„± í•„ìš”)
             migration_sql.append(f"-- TABLE {name} differs significantly. Recreating.\nDROP TABLE IF EXISTS public.{name} CASCADE;\n{ddl}\n")
        elif not are_different and not alter_statements: # í…Œì´ë¸” í¬í•¨ ëª¨ë“  íƒ€ì…ì´ ë™ì¼í•˜ê³  ALTER ë¬¸ë„ ì—†ëŠ” ê²½ìš°
            # ë™ì¼í•œ ê²½ìš°: ìŠ¤í‚µ ì²˜ë¦¬
            original_ddl = ""
            if obj_type == "TABLE":
                 original_ddl = generate_create_table_ddl(
                        name,
                        src_data[name],
                        composite_uniques=src_composite_uniques,
                        composite_primaries=src_composite_primaries
                        )
            elif obj_type == "TYPE":
                 original_ddl = src_enum_ddls.get(name, "") # ìŠ¤í‚µ ë¡œê·¸ìš© Enum DDL
            else: # View, Function, Index, Sequence ë“±
                 original_ddl = src_data.get(name, "") # src_dataê°€ DDL ë”•ì…”ë„ˆë¦¬ë¼ê³  ê°€ì •

            skipped_sql.append(f"-- {obj_type} {name} is up-to-date; skipping.\n")
            if original_ddl: # DDLì´ ìˆëŠ” ê²½ìš°ë§Œ ì£¼ì„ ì²˜ë¦¬í•˜ì—¬ ì¶”ê°€
                 commented_ddl = '\n'.join([f"-- {line}" for line in original_ddl.strip().splitlines()])
                 skipped_sql.append(commented_ddl + "\n")

    # íƒ€ê²Ÿì—ë§Œ ìˆëŠ” ê°ì²´ëŠ” í˜„ì¬ ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ (í•„ìš” ì‹œ ì¶”ê°€)

    return migration_sql, skipped_sql


# --- SQL ì •ê·œí™” í•¨ìˆ˜ ---
def normalize_sql(sql_text):
    """SQL ë¬¸ìì—´ì—ì„œ ì£¼ì„ ì œê±°, ì†Œë¬¸ì ë³€í™˜, ê³µë°± ì •ê·œí™” ìˆ˜í–‰ (ë‹¬ëŸ¬ ì¸ìš© ë¬¸ìì—´ ë³´í˜¸)"""
    if not sql_text:
        return ""

    # ë‹¬ëŸ¬ ì¸ìš© ë¬¸ìì—´ ì¶”ì¶œ ë° ì„ì‹œ ì¹˜í™˜
    dollar_quoted_strings = []
    def replace_dollar_quoted(match):
        dollar_quoted_strings.append(match.group(0))
        return f"__DOLLAR_QUOTED_STRING_{len(dollar_quoted_strings)-1}__"

    # ì •ê·œ í‘œí˜„ì‹ ìˆ˜ì •: ì‹œì‘ê³¼ ë íƒœê·¸ê°€ ë™ì¼í•´ì•¼ í•¨ ($tag$...$tag$)
    # íƒœê·¸ëŠ” ë¹„ì–´ìˆê±°ë‚˜, ë¬¸ìë¡œë§Œ êµ¬ì„±ë  ìˆ˜ ìˆìŒ
    sql_text_no_dollars = re.sub(r"(\$([a-zA-Z_]\w*)?\$).*?\1", replace_dollar_quoted, sql_text, flags=re.DOTALL)

    # -- ìŠ¤íƒ€ì¼ ì£¼ì„ ì œê±°
    processed_sql = re.sub(r'--.*$', '', sql_text_no_dollars, flags=re.MULTILINE)
    # /* */ ìŠ¤íƒ€ì¼ ì£¼ì„ ì œê±° (ê°„ë‹¨í•œ ê²½ìš°ë§Œ ì²˜ë¦¬, ì¤‘ì²© ë¶ˆê°€)
    # processed_sql = re.sub(r'/\*.*?\*/', '', processed_sql, flags=re.DOTALL) # í•„ìš” ì‹œ ì¶”ê°€

    # ì†Œë¬¸ìë¡œ ë³€í™˜ (ë‹¬ëŸ¬ ì¸ìš© ì œì™¸ ë¶€ë¶„ë§Œ)
    processed_sql = processed_sql.lower()
    # ê´„í˜¸, ì‰¼í‘œ, ì„¸ë¯¸ì½œë¡  ì£¼ë³€ ê³µë°± ì œê±°
    processed_sql = re.sub(r'\s*([(),;])\s*', r'\1', processed_sql)
    # ë“±í˜¸(=) ë“± ì—°ì‚°ì ì£¼ë³€ ê³µë°± ì œê±° (ë” ë§ì€ ì—°ì‚°ì ì¶”ê°€ ê°€ëŠ¥)
    processed_sql = re.sub(r'\s*([=<>!+-/*%])\s*', r'\1', processed_sql)
    # ì—¬ëŸ¬ ê³µë°± (ìŠ¤í˜ì´ìŠ¤, íƒ­, ê°œí–‰ í¬í•¨)ì„ ë‹¨ì¼ ìŠ¤í˜ì´ìŠ¤ë¡œ ë³€ê²½
    processed_sql = re.sub(r'\s+', ' ', processed_sql)
    # ì•ë’¤ ê³µë°± ì œê±°
    processed_sql = processed_sql.strip()

    # ì„ì‹œ ì¹˜í™˜ëœ ë‹¬ëŸ¬ ì¸ìš© ë¬¸ìì—´ ë³µì›
    for i, original_string in enumerate(dollar_quoted_strings):
        processed_sql = processed_sql.replace(f"__DOLLAR_QUOTED_STRING_{i}__", original_string)

    # ë§ˆì§€ë§‰ ì„¸ë¯¸ì½œë¡  ì œê±° (ì˜µì…˜)
    return processed_sql.rstrip(';')


# --- ê²€ì¦ ê²°ê³¼ ì¶œë ¥ í•¨ìˆ˜ ---
def print_verification_report(src_objs, tgt_objs, obj_type):
    """ì†ŒìŠ¤ì™€ íƒ€ê²Ÿ ê°ì²´ ëª©ë¡ì„ ë¹„êµí•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."""
    print(f"\nVerifying {obj_type}...")
    src_names = set(src_objs.keys())
    tgt_names = set(tgt_objs.keys())

    source_only = sorted(list(src_names - tgt_names))
    target_only = sorted(list(tgt_names - src_names))

    print(f"  Source Count: {len(src_names)}")
    print(f"  Target Count: {len(tgt_names)}")

    if source_only:
        print(f"  Objects only in Source ({len(source_only)}): {', '.join(source_only)}")
    else:
        print("  Objects only in Source (0): None")

    if target_only:
        print(f"  Objects only in Target ({len(target_only)}): {', '.join(target_only)}")
    else:
        print("  Objects only in Target (0): None")

    is_synced = not source_only and not target_only
    status = "Synced" if is_synced else "Differences found"
    print(f"  Status: {status}")
    return is_synced

def extract_foreign_keys(metadata, composite_fks):
    """
    { "table.col->ref_table.ref_col": DDL } í˜•íƒœë¡œ ë°˜í™˜
    ë³µí•© FKì™€ ë‹¨ì¼ FKë¥¼ ëª¨ë‘ ì§€ì›, CASCADE ì˜µì…˜ í¬í•¨
    ëª¨ë“  FKëŠ” composite_fksì—ì„œ ê°€ì ¸ì˜´ (pg_constraint ê¸°ë°˜)
    """
    
    def get_fk_action(action_code):
        """PostgreSQL FK action codeë¥¼ SQLë¡œ ë³€í™˜"""
        action_map = {
            'a': 'NO ACTION',
            'r': 'RESTRICT',
            'c': 'CASCADE',
            'n': 'SET NULL',
            'd': 'SET DEFAULT'
        }
        return action_map.get(action_code, 'NO ACTION')
    
    fk_map = {}
    
    # ëª¨ë“  FKë¥¼ composite_fksì—ì„œ ì²˜ë¦¬ (ë‹¨ì¼ ë° ë³µí•© FK ëª¨ë‘ í¬í•¨)
    for table_name, fk_list in composite_fks.items():
        for fk_info in fk_list:
            constraint_name = fk_info['constraint_name']
            columns = fk_info['columns']
            ref_table = fk_info['ref_table']
            ref_columns = fk_info['ref_columns']
            on_delete = fk_info.get('on_delete', 'a')
            on_update = fk_info.get('on_update', 'a')
            
            # í‚¤ ìƒì„±
            if len(columns) == 1:
                # ë‹¨ì¼ ì»¬ëŸ¼ FK: table.col->ref_table.ref_col
                constraint_key = f"{table_name}.{columns[0]}->{ref_table}.{ref_columns[0]}"
            else:
                # ë³µí•© FK: table.(col1,col2)->ref_table.(ref_col1,ref_col2)
                cols_str = ','.join(columns)
                ref_cols_str = ','.join(ref_columns)
                constraint_key = f"{table_name}.({cols_str})->{ref_table}.({ref_cols_str})"
            
            # DDL ìƒì„±
            quoted_cols = ', '.join(f'"{col}"' for col in columns)
            quoted_ref_cols = ', '.join(f'"{col}"' for col in ref_columns)
            
            ddl_parts = [
                f'ALTER TABLE public."{table_name}"',
                f'ADD CONSTRAINT "{constraint_name}"',
                f'FOREIGN KEY ({quoted_cols})',
                f'REFERENCES public."{ref_table}" ({quoted_ref_cols})'
            ]
            
            # CASCADE ì˜µì…˜ ì¶”ê°€ (NO ACTIONì´ ì•„ë‹Œ ê²½ìš°ë§Œ)
            on_delete_action = get_fk_action(on_delete)
            on_update_action = get_fk_action(on_update)
            
            if on_delete_action != 'NO ACTION':
                ddl_parts.append(f'ON DELETE {on_delete_action}')
            if on_update_action != 'NO ACTION':
                ddl_parts.append(f'ON UPDATE {on_update_action}')
            
            ddl = ' '.join(ddl_parts) + ';'
            fk_map[constraint_key] = ddl
    
    return fk_map

def add_not_valid_constraint(ddl):
    """Append NOT VALID to a FK constraint if not already present."""
    if "NOT VALID" in ddl.upper():
        return ddl
    stripped = ddl.rstrip()
    if stripped.endswith(";"):
        return stripped[:-1] + " NOT VALID;"
    return stripped + " NOT VALID"

def build_fk_validate_statements(migration_sql_blocks):
    """Build VALIDATE CONSTRAINT statements from FK migration blocks."""
    validate_sql = []
    pattern = re.compile(
        r'ALTER\\s+TABLE\\s+public\\."?([^"\\s]+)"?\\s+ADD\\s+CONSTRAINT\\s+"?([^"\\s]+)"?',
        re.IGNORECASE,
    )
    for block in migration_sql_blocks:
        sql_content = "\n".join(
            line for line in block.splitlines() if not line.strip().startswith("--")
        )
        match = pattern.search(sql_content)
        if not match:
            continue
        table_name, constraint_name = match.groups()
        validate_sql.append(
            f'ALTER TABLE public."{table_name}" VALIDATE CONSTRAINT "{constraint_name}";'
        )
    return validate_sql

def main():
    # --- ì»¤ë§¨ë“œë¼ì¸ ì¸ìˆ˜ íŒŒì‹± ---
    parser = argparse.ArgumentParser(description="Compare source and target PostgreSQL schemas and generate/apply migration SQL, or verify differences.")
    parser.add_argument('--config', default=DEFAULT_CONFIG_PATH,
                        help=f"Path to config YAML file (default: {DEFAULT_CONFIG_PATH}).")
    # Verification flag
    parser.add_argument('--verify', action='store_true',
                        help="Only verify schema differences (object names and counts) without generating/executing SQL.")
    # Commit flag (only relevant if --verify is not used)
    parser.add_argument('--commit', action=argparse.BooleanOptionalAction, default=True,
                        help="Execute the generated migration SQL on the target database. Use --no-commit to only generate files. Ignored if --verify is used.")
    # Experimental ALTER TABLE flag
    parser.add_argument('--use-alter', action='store_true', default=False,
                        help="EXPERIMENTAL: Use ALTER TABLE for column additions/deletions instead of DROP/CREATE. Use with caution.")
    parser.add_argument('--with-data', action='store_true',
                    help="Include data migration after schema changes")
    parser.add_argument('--skip-fk', action='store_true', default=False,
                        help="Skip foreign key migration.")
    parser.add_argument('--fk-not-valid', action='store_true', default=False,
                        help="Add foreign keys as NOT VALID and write a validate SQL file.")
    parser.add_argument('--install-extensions', action=argparse.BooleanOptionalAction, default=True,
                        help="Detect missing extensions and add CREATE EXTENSION statements (default: enabled).")
    args = parser.parse_args()
    # --- ì¸ìˆ˜ íŒŒì‹± ë ---
    if args.skip_fk and args.fk_not_valid:
        print("Error: --skip-fk and --fk-not-valid are mutually exclusive.")
        return

    # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„± (YYYYMMDDHHMMSS í˜•ì‹)
    timestamp = datetime.datetime.now().strftime("%Y%m%d%H%M%S")

    # history ë””ë ‰í† ë¦¬ í™•ì¸ ë° ìƒì„±
    history_dir = "history"
    os.makedirs(history_dir, exist_ok=True)

    config_path = args.config
    config = load_config(config_path)
    if not config:
        return

    # ì„¤ì • ìœ íš¨ì„± ê²€ì‚¬ ë° ì¶”ì¶œ
    if 'source' not in config or not isinstance(config['source'], dict):
        print(f"Error: 'source' configuration is missing or invalid in {config_path}.")
        return
    if 'targets' not in config or not isinstance(config['targets'], dict) or 'gcp_test' not in config['targets'] or not isinstance(config['targets']['gcp_test'], dict):
        print(f"Error: 'targets.gcp_test' configuration is missing or invalid in {config_path}.")
        return

    source_config = config['source']
    target_config = config['targets']['gcp_test']

    # psycopg2ì—ì„œ ì‚¬ìš©í•˜ëŠ” í‚¤ ì´ë¦„ìœ¼ë¡œ ì¡°ì • ('db' -> 'dbname', 'username' -> 'user')
    # source ì„¤ì • ì¡°ì •
    if 'db' in source_config:
        source_config['dbname'] = source_config.pop('db')
    if 'username' in source_config:
        source_config['user'] = source_config.pop('username')

    # target ì„¤ì • ì¡°ì •
    if 'db' in target_config:
        target_config['dbname'] = target_config.pop('db')
    if 'username' in target_config:
        target_config['user'] = target_config.pop('username')


    # ì—°ê²°
    try:
        print("Connecting to source database...")
        src_conn = get_connection(source_config)
        print("Connecting to target database (gcp_test)...")
        tgt_conn = get_connection(target_config)
    except psycopg2.Error as e:
        print(f"Database connection error: {e}")
        return
    except Exception as e:
        print(f"An unexpected error occurred during connection: {e}")
        return

    extension_migration_sql = []
    if args.verify:
        missing_extensions, _ = get_extension_diffs(src_conn, tgt_conn, allowlist=None)
        if missing_extensions:
            print(f"Missing extensions in target database: {', '.join(missing_extensions)}")
    else:
        allowed_missing, skipped_missing = get_extension_diffs(
            src_conn,
            tgt_conn,
            allowlist=AUTO_INSTALL_EXTENSIONS,
        )
        if skipped_missing:
            print("Warning: Missing extensions not in allowlist; install manually:")
            print(f"  {', '.join(skipped_missing)}")
        if allowed_missing:
            if args.install_extensions:
                print(f"Adding extension setup for: {', '.join(allowed_missing)}")
                extension_migration_sql = build_extension_sql(allowed_missing)
            else:
                print("Missing extensions detected (auto-install disabled):")
                print(f"  {', '.join(allowed_missing)}")


    # --- ë°ì´í„° ì¡°íšŒ ---
    print("Fetching Enum DDLs...")
    src_enum_ddls = fetch_enums(src_conn) # DDL ìƒì„± ë° ìŠ¤í‚µ ë¡œê·¸ìš©
    tgt_enum_ddls = fetch_enums(tgt_conn) # ìŠ¤í‚µ ë¡œê·¸ìš©

    print("Fetching Enum Values...")
    src_enum_values = fetch_enums_values(src_conn) # ë¹„êµìš©
    tgt_enum_values = fetch_enums_values(tgt_conn) # ë¹„êµìš©

    print("Fetching Table Metadata...")
    src_tables_meta, src_composite_uniques, src_composite_primaries, src_composite_fks = fetch_tables_metadata(src_conn)
    tgt_tables_meta, tgt_composite_uniques, tgt_composite_primaries, tgt_composite_fks = fetch_tables_metadata(tgt_conn)


    print("Fetching View DDLs...")
    src_views = fetch_views(src_conn) # ë¹„êµ ë° DDL ìƒì„±ìš©
    tgt_views = fetch_views(tgt_conn) # ë¹„êµìš©

    print("Fetching Function DDLs...")
    src_functions = fetch_functions(src_conn) # ë¹„êµ ë° DDL ìƒì„±ìš©
    tgt_functions = fetch_functions(tgt_conn) # ë¹„êµìš©

    print("Fetching Index DDLs...")
    src_indexes, src_pkey_indexes = fetch_indexes(src_conn) # ë¹„êµ ë° DDL ìƒì„±ìš© + ì •ë³´ìš©
    tgt_indexes, tgt_pkey_indexes = fetch_indexes(tgt_conn) # ë¹„êµìš© + ì •ë³´ìš©

    print("Fetching Sequence DDLs...")
    print("  Fetching from source database...")
    src_sequences = fetch_sequences(src_conn) # ë¹„êµ ë° DDL ìƒì„±ìš©
    print(f"  Source sequences count: {len(src_sequences)}")
    print("  Fetching from target database...")
    tgt_sequences = fetch_sequences(tgt_conn) # ë¹„êµìš©
    print(f"  Target sequences count: {len(tgt_sequences)}")
    
    # íƒ€ê²Ÿ ì‹œí€€ìŠ¤ê°€ ë¹„ì–´ìˆë‹¤ë©´ ì§ì ‘ í™•ì¸
    if len(tgt_sequences) == 0:
        print("  âš ï¸  No sequences found in target, checking manually...")
        with tgt_conn.cursor() as cur:
            # ë°©ë²• 1: pg_classë¡œ í™•ì¸
            cur.execute("""
            SELECT COUNT(*) 
            FROM pg_class c 
            JOIN pg_namespace n ON c.relnamespace = n.oid 
            WHERE n.nspname = 'public' AND c.relkind = 'S'
            """)
            count = cur.fetchone()[0]
            print(f"    pg_class sequence count: {count}")
            
            # ë°©ë²• 2: ì‹¤ì œ ì‹œí€€ìŠ¤ ëª©ë¡ í™•ì¸
            cur.execute("""
            SELECT c.relname
            FROM pg_class c 
            JOIN pg_namespace n ON c.relnamespace = n.oid 
            WHERE n.nspname = 'public' AND c.relkind = 'S'
            ORDER BY c.relname
            """)
            seq_names = [row[0] for row in cur.fetchall()]
            print(f"    Target sequence names: {seq_names[:5]}...")  # ì²˜ìŒ 5ê°œë§Œ ì¶œë ¥
            
            # ë°©ë²• 3: information_schemaë¡œë„ í™•ì¸
            cur.execute("""
            SELECT sequence_name 
            FROM information_schema.sequences 
            WHERE sequence_schema = 'public'
            ORDER BY sequence_name
            """)
            info_seq_names = [row[0] for row in cur.fetchall()]
            print(f"    information_schema sequence names: {info_seq_names[:5]}...")
            
            if count > 0 and len(tgt_sequences) == 0:
                print("  âŒ BUG: Sequences exist in database but fetch_sequences returned empty!")
                print("    This indicates a bug in fetch_sequences function.")

    # --- ê²€ì¦ ëª¨ë“œ ì²˜ë¦¬ ---
    if args.verify:
        print("\n--- Schema Verification Mode ---")
        all_synced = True
        # ê²€ì¦ ì‹œì—ëŠ” ì´ë¦„ ëª©ë¡ë§Œ ë¹„êµ
        all_synced &= print_verification_report(src_enum_ddls, tgt_enum_ddls, "Enums (Types)")
        all_synced &= print_verification_report(src_sequences, tgt_sequences, "Sequences")
        all_synced &= print_verification_report(src_tables_meta, tgt_tables_meta, "Tables")
        all_synced &= print_verification_report(src_views, tgt_views, "Views")
        all_synced &= print_verification_report(src_functions, tgt_functions, "Functions")
        # ë¹„êµ ëŒ€ìƒ ì¸ë±ìŠ¤ë§Œ ê²€ì¦ ë¦¬í¬íŠ¸ì— ì‚¬ìš©
        all_synced &= print_verification_report(src_indexes, tgt_indexes, "Indexes (excluding _pkey)")

        # íƒ€ê²Ÿì—ë§Œ ìˆëŠ” _pkey ì¸ë±ìŠ¤ ì •ë³´ ì¶œë ¥
        target_only_pkeys = sorted(list(set(tgt_pkey_indexes.keys()) - set(src_pkey_indexes.keys())))
        if target_only_pkeys:
            print(f"\n  Info: Found target-only primary key indexes (ignored in comparison): {', '.join(target_only_pkeys)}")

        print("\n--- Verification Summary ---")
        if all_synced:
            print("Source and target schemas are perfectly synchronized (based on object names).")
        else:
            print("Schema differences found. Please review the report above.")

        # ê²€ì¦ ëª¨ë“œì—ì„œëŠ” ì—°ê²°ë§Œ ë‹«ê³  ì¢…ë£Œ
        src_conn.close()
        if tgt_conn:
            tgt_conn.close()
        print("\nConnections closed.")
        return # ì—¬ê¸°ì„œ í•¨ìˆ˜ ì¢…ë£Œ
    # --- ê²€ì¦ ëª¨ë“œ ì²˜ë¦¬ ë ---


    # --- ë§ˆì´ê·¸ë ˆì´ì…˜/íŒŒì¼ ìƒì„± ëª¨ë“œ (ê¸°ì¡´ ë¡œì§) ---
    print("\n--- Migration Generation Mode ---")
    if args.use_alter:
        print("--- Using experimental ALTER TABLE mode ---")
    all_migration_sql = [] # ì‹¤ì œ ë§ˆì´ê·¸ë ˆì´ì…˜ SQL ì €ì¥
    all_skipped_sql = []   # ê±´ë„ˆë›´ SQL ì €ì¥

    if extension_migration_sql:
        all_migration_sql.extend(extension_migration_sql)
    fk_validate_sql = []

    # ìˆœì„œ: enum, table, sequence, view, function, index
    print("Comparing Enums (Values)...")
    # Enum ë¹„êµ ì‹œ ê°’ ëª©ë¡(values)ì„ ì‚¬ìš©í•˜ê³ , DDL ìƒì„±ì„ ìœ„í•´ src_enum_ddls ì „ë‹¬
    mig_sql, skip_sql = compare_and_generate_migration(src_enum_values, tgt_enum_values, "TYPE", src_enum_ddls=src_enum_ddls)
    all_migration_sql.extend(mig_sql)
    all_skipped_sql.extend(skip_sql)

    print("Comparing Tables (Metadata)...")
    # use_alter ì˜µì…˜ ì „ë‹¬
    mig_sql, skip_sql = compare_and_generate_migration(src_tables_meta, tgt_tables_meta, "TABLE", 
                                                       use_alter=args.use_alter, src_enum_ddls=src_enum_ddls,
                                                       src_composite_uniques = src_composite_uniques,tgt_composite_uniques= tgt_composite_uniques,
                                                       src_composite_primaries = src_composite_primaries, tgt_composite_primaries = tgt_composite_primaries  ) # src_enum_ddls ì „ë‹¬ ì¶”ê°€
    all_migration_sql.extend(mig_sql)
    all_skipped_sql.extend(skip_sql)

    # args.with_data ëª¨ë“œê°€ ì•„ë‹ ë•Œë§Œ ì‹œí€€ìŠ¤ ë§ˆì´ê·¸ë ˆì´ì…˜ SQL ìƒì„± (í…Œì´ë¸” ì´í›„ë¡œ ì´ë™)
    if not args.with_data:
        print("Comparing Sequences (DDL)...")
        print(f"  Source sequences: {list(src_sequences.keys())}")
        print(f"  Target sequences: {list(tgt_sequences.keys())}")
        
        # IDENTITY ì»¬ëŸ¼ì˜ ì‹œí€€ìŠ¤ëŠ” í…Œì´ë¸” ìƒì„± ì‹œ ìë™ìœ¼ë¡œ ìƒì„±ë˜ë¯€ë¡œ ë³„ë„ ë§ˆì´ê·¸ë ˆì´ì…˜ ë¶ˆí•„ìš”
        if src_sequences:
            print("  Note: Sequences will be handled after table creation (for IDENTITY columns)")
        else:
            print("  No sequences to migrate")
        
        # ì‹œí€€ìŠ¤ ë§ˆì´ê·¸ë ˆì´ì…˜ SQLì€ ìƒì„±í•˜ì§€ ì•ŠìŒ (í…Œì´ë¸” ìƒì„± í›„ ê°’ ë™ê¸°í™”ë¡œ ì²˜ë¦¬)
        # mig_sql, skip_sql = compare_and_generate_migration(src_sequences, tgt_sequences, "SEQUENCE")
        # all_migration_sql.extend(mig_sql)
        # all_skipped_sql.extend(skip_sql)

    if args.skip_fk:
        print("Skipping Foreign Keys (--skip-fk enabled)...")
    else:
        print("Comparing Foreign Keys...")  # ğŸ‘ˆ ì´ ë¶€ë¶„ ì¶”ê°€

        src_fk_map = extract_foreign_keys(src_tables_meta, src_composite_fks)
        tgt_fk_map = extract_foreign_keys(tgt_tables_meta, tgt_composite_fks)

        mig_sql, skip_sql = compare_and_generate_migration(
            src_fk_map,
            tgt_fk_map,
            "FOREIGN_KEY",
            fk_not_valid=args.fk_not_valid,
        )
        all_migration_sql.extend(mig_sql)
        all_skipped_sql.extend(skip_sql)
        if args.fk_not_valid:
            fk_validate_sql = build_fk_validate_statements(mig_sql)

    print("Comparing Views (DDL)...")
    mig_sql, skip_sql = compare_and_generate_migration(src_views, tgt_views, "VIEW")
    all_migration_sql.extend(mig_sql)
    all_skipped_sql.extend(skip_sql)

    print("Comparing Functions (DDL)...")
    mig_sql, skip_sql = compare_and_generate_migration(src_functions, tgt_functions, "FUNCTION")
    all_migration_sql.extend(mig_sql)
    all_skipped_sql.extend(skip_sql)

    print("Comparing Indexes (DDL, excluding _pkey)...")
    # ë¹„êµ ëŒ€ìƒ ì¸ë±ìŠ¤ë§Œ ë§ˆì´ê·¸ë ˆì´ì…˜ ìƒì„±ì— ì‚¬ìš©
    mig_sql, skip_sql = compare_and_generate_migration(src_indexes, tgt_indexes, "INDEX")
    all_migration_sql.extend(mig_sql)
    all_skipped_sql.extend(skip_sql)
    # --- ë¹„êµ ë° SQL ìƒì„± ë ---

    

    # íŒŒì¼ëª… ìƒì„± (target_nameì€ configì—ì„œ ê°€ì ¸ì˜¨ ì²«ë²ˆì§¸ íƒ€ê²Ÿ í‚¤ë¡œ ê°€ì •)
    # TODO: ì—¬ëŸ¬ íƒ€ê²Ÿì„ ì²˜ë¦¬í•´ì•¼ í•˜ëŠ” ê²½ìš° ë¡œì§ ìˆ˜ì • í•„ìš”
    target_name = list(config['targets'].keys())[0] if config.get('targets') else 'unknown_target'

    migration_filename = os.path.join(history_dir, f"migrate.{target_name}.{timestamp}.sql")
    skipped_filename = os.path.join(history_dir, f"skip.{target_name}.{timestamp}.sql")

    # ë§ˆì´ê·¸ë ˆì´ì…˜ SQL íŒŒì¼ ì €ì¥
    try:
        with open(migration_filename, "w", encoding="utf-8") as f:
            f.write("\n".join(all_migration_sql))
        print(f"Migration SQL written to {migration_filename}")
    except IOError as e:
        print(f"Error writing migration file {migration_filename}: {e}")


    # ê±´ë„ˆë›´ SQL íŒŒì¼ ì €ì¥
    try:
        with open(skipped_filename, "w", encoding="utf-8") as f:
            f.write("\n".join(all_skipped_sql))
        print(f"Skipped SQL written to {skipped_filename}")
    except IOError as e:
        print(f"Error writing skipped file {skipped_filename}: {e}")

    if args.fk_not_valid and fk_validate_sql:
        validate_filename = os.path.join(
            history_dir, f"validate_fks.{target_name}.{timestamp}.sql"
        )
        try:
            with open(validate_filename, "w", encoding="utf-8") as f:
                f.write("\n".join(fk_validate_sql))
            print(f"FK validate SQL written to {validate_filename}")
        except IOError as e:
            print(f"Error writing FK validate file {validate_filename}: {e}")

    if args.with_data:
        print("\n-- Running Data Migration --")
        
        # ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ì „ì— ê¸°ì¡´ íŠ¸ëœì­ì…˜ ì»¤ë°‹ ë° ì—°ê²° ë‹«ê¸° (lock ì™„ì „ í•´ì œ)
        # run_data_migration_parallelì´ ë‚´ë¶€ì—ì„œ ìƒˆ ì—°ê²°ì„ ìƒì„±í•¨
        print("  Closing existing connections to release all locks...")
        try:
            src_conn.commit()
            tgt_conn.commit()
        except:
            pass
        src_conn.close()
        tgt_conn.close()
        print("  Connections closed, locks released.")
        
        # ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ í›„ ì‹œí€€ìŠ¤ ê²€ì¦ì„ ìœ„í•´ ìƒˆ ì—°ê²° ìƒì„±
        src_conn = get_connection(source_config)
        tgt_conn = get_connection(target_config)
        
        try:
            run_data_migration_parallel(src_conn, src_tables_meta, src_composite_fks, config_path=config_path)
            print("\nData migration completed and committed.")

            # ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ì´í›„ ì‹œí€€ìŠ¤ ê²€ì¦ ë° ìˆ˜ì • ì‹¤í–‰
            print(f"\n--- Post-Data Migration Sequence Verification and Correction ---")
            
            # IDENTITY ì»¬ëŸ¼ì˜ ì‹œí€€ìŠ¤ ê°’ ê²€ì¦ ë° ìˆ˜ì •
            print(f"\n--- Verifying and Correcting IDENTITY Sequence Values ---")
            with src_conn.cursor() as src_cur, tgt_conn.cursor() as tgt_cur:
                for table_name, columns in src_tables_meta.items():
                    # IDENTITY ì»¬ëŸ¼ ì°¾ê¸°
                    identity_columns = [col for col in columns if col.get('identity', False)]
                    
                    for col in identity_columns:
                        col_name = col['name']
                        seq_name = f"{table_name}_{col_name}_seq"
                        
                        try:
                            # íƒ€ê²Ÿ ì‹œí€€ìŠ¤ì˜ last_value ì¡°íšŒ
                            tgt_cur.execute(f"SELECT last_value FROM public.{seq_name}")
                            tgt_last_value = tgt_cur.fetchone()[0]
                            
                            # íƒ€ê²Ÿ í…Œì´ë¸”ì˜ ìµœëŒ€ ID ê°’ ì¡°íšŒ (ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ í›„)
                            tgt_cur.execute(f"SELECT COALESCE(MAX({col_name}), 0) FROM public.{table_name}")
                            tgt_max_id = tgt_cur.fetchone()[0]
                            
                            print(f"  ğŸ“Š {table_name}.{col_name}:")
                            print(f"    Target sequence last_value: {tgt_last_value}")
                            print(f"    Target table max ID (after data migration): {tgt_max_id}")
                            
                            # ì‹œí€€ìŠ¤ ê°’ê³¼ í…Œì´ë¸” ìµœëŒ€ ID ë¹„êµ
                            if tgt_last_value == tgt_max_id:
                                print(f"  âœ… {seq_name}: sequence and table values match")
                            elif tgt_last_value > tgt_max_id:
                                print(f"  âš ï¸  {seq_name}: sequence value ({tgt_last_value}) > table max ID ({tgt_max_id})")
                                print(f"      Keeping sequence value (data may have been deleted)")
                            else:
                                print(f"  ğŸ”§ {seq_name}: sequence value ({tgt_last_value}) < table max ID ({tgt_max_id})")
                                print(f"      Updating sequence value to match table max ID")
                                
                                # ì‹œí€€ìŠ¤ ê°’ì„ í…Œì´ë¸” ìµœëŒ€ IDë¡œ ì—…ë°ì´íŠ¸
                                setval_sql = f"SELECT setval('public.{seq_name}', {tgt_max_id}, true)"
                                print(f"      Executing: {setval_sql}")
                                tgt_cur.execute(setval_sql)
                                
                                # ì—…ë°ì´íŠ¸ í›„ ê°’ í™•ì¸
                                tgt_cur.execute(f"SELECT last_value FROM public.{seq_name}")
                                new_last_value = tgt_cur.fetchone()[0]
                                print(f"      After setval: last_value={new_last_value}")
                                print(f"  âœ… {seq_name}: updated from {tgt_last_value} to {tgt_max_id}")
                                
                        except Exception as e:
                            print(f"  âŒ {seq_name}: failed to verify/correct - {e}")
                            import traceback
                            traceback.print_exc()
            
            # ëª…ì‹œì  ì‹œí€€ìŠ¤ ê°’ ê²€ì¦ ë° ìˆ˜ì • (ì†ŒìŠ¤ì— ëª…ì‹œì  ì‹œí€€ìŠ¤ê°€ ìˆëŠ” ê²½ìš°)
            common_sequences = set(src_sequences.keys()) & set(tgt_sequences.keys())
            if common_sequences:
                print(f"\n--- Verifying and Correcting Explicit Sequence Values ---")
                print(f"Common sequences: {list(common_sequences)}")
                
                with src_conn.cursor() as src_cur, tgt_conn.cursor() as tgt_cur:
                    for seq_name in common_sequences:
                        try:
                            # ì†ŒìŠ¤ ì‹œí€€ìŠ¤ì˜ í˜„ì¬ ê°’ ì¡°íšŒ
                            src_cur.execute(f"SELECT last_value, is_called FROM public.{seq_name}")
                            src_last_value, src_is_called = src_cur.fetchone()
                            
                            # íƒ€ê²Ÿ ì‹œí€€ìŠ¤ì˜ í˜„ì¬ ê°’ ì¡°íšŒ
                            tgt_cur.execute(f"SELECT last_value, is_called FROM public.{seq_name}")
                            tgt_last_value, tgt_is_called = tgt_cur.fetchone()
                            
                            print(f"  ğŸ“Š {seq_name}:")
                            print(f"    Source: last_value={src_last_value}, is_called={src_is_called}")
                            print(f"    Target: last_value={tgt_last_value}, is_called={tgt_is_called}")
                            
                            # ê°’ì´ ë‹¤ë¥¸ ê²½ìš°ì—ë§Œ ì—…ë°ì´íŠ¸
                            if src_last_value != tgt_last_value:
                                # ì‹œí€€ìŠ¤ ê°’ì„ ì†ŒìŠ¤ì™€ ë™ì¼í•˜ê²Œ ì„¤ì •
                                setval_sql = f"SELECT setval('public.{seq_name}', {src_last_value}, {src_is_called})"
                                print(f"    Executing: {setval_sql}")
                                tgt_cur.execute(setval_sql)
                                
                                # ì—…ë°ì´íŠ¸ í›„ ê°’ í™•ì¸
                                tgt_cur.execute(f"SELECT last_value, is_called FROM public.{seq_name}")
                                new_tgt_last_value, new_tgt_is_called = tgt_cur.fetchone()
                                print(f"    After setval: last_value={new_tgt_last_value}, is_called={new_tgt_is_called}")
                                
                                print(f"  âœ… {seq_name}: {tgt_last_value} â†’ {src_last_value}")
                            else:
                                print(f"  â­ï¸  {seq_name}: already synced ({src_last_value})")
                                
                        except Exception as e:
                            print(f"  âŒ {seq_name}: failed to verify/correct - {e}")
                            import traceback
                            traceback.print_exc()
            
            tgt_conn.commit()
            print("\nSequence verification and correction completed and committed.")
            
            # ìµœì¢… ì‹œí€€ìŠ¤ ê°’ ê²€ì¦
            print(f"\n--- Final Sequence Value Verification ---")
            verify_sequence_values(tgt_conn, src_tables_meta)

        except Exception as e:
            print(f"Error during data migration: {e}")
            tgt_conn.rollback()
            print("Transaction rolled back.")
        finally:
            src_conn.close()
            tgt_conn.close()
            print("Connections closed.")
        return
    # --- ë§ˆì´ê·¸ë ˆì´ì…˜ SQL ì‹¤í–‰ (commit ì˜µì…˜ì´ Trueì¼ ê²½ìš°) ---
    elif args.commit:
        if not all_migration_sql:
            print("No migration SQL to execute.")
        else:
            print(f"\nExecuting migration SQL on target database ({target_name})...")
            execution_successful = True
            executed_count = 0
            failed_count = 0
            
            try:
                with tgt_conn.cursor() as cur:
                    total_blocks = len(all_migration_sql)
                    
                    # ê° SQL ë¸”ë¡ ì²˜ë¦¬ (ê°ê° ë…ë¦½ì ìœ¼ë¡œ ì¦‰ì‹œ ì»¤ë°‹)
                    for i, sql_block in enumerate(all_migration_sql):
                        # ë¸”ë¡ ë‚´ ì£¼ì„ ì œì™¸ ë° ì‹¤ì œ ì‹¤í–‰í•  SQL ì¶”ì¶œ
                        sql_content = "\n".join(line for line in sql_block.strip().splitlines() if not line.strip().startswith('--'))
                        if not sql_content.strip():
                            continue # ì‹¤í–‰í•  ë‚´ìš© ì—†ìœ¼ë©´ ë‹¤ìŒ ë¸”ë¡ìœ¼ë¡œ

                        print(f"--- Executing Block {i+1}/{total_blocks} ---")
                        try:
                            # sql_content ì „ì²´ë¥¼ ì‹¤í–‰
                            print(f"  SQL: {sql_content[:100]}{'...' if len(sql_content) > 100 else ''}")
                            cur.execute(sql_content)
                            
                            # âœ… ê° ë¸”ë¡ë§ˆë‹¤ ì¦‰ì‹œ ì»¤ë°‹ (lock ë¹ ë¥´ê²Œ í•´ì œ)
                            tgt_conn.commit()
                            executed_count += 1
                            print(f"  âœ… Block {i+1} committed")
                            
                        except psycopg2.Error as e:
                            failed_count += 1
                            print(f"  âŒ Block {i+1} failed:")
                            print(f"     Error: {e}")
                            print("  Rolling back this block...")
                            tgt_conn.rollback()
                            
                            # DDL ì‹¤íŒ¨ëŠ” ì‹¬ê°í•˜ë¯€ë¡œ ì „ì²´ ì¤‘ë‹¨
                            execution_successful = False
                            break
                            
                        except Exception as e:
                            failed_count += 1
                            print(f"  âŒ Block {i+1} unexpected error:")
                            print(f"     Error: {e}")
                            print("  Rolling back this block...")
                            tgt_conn.rollback()
                            
                            # ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬ë„ ì „ì²´ ì¤‘ë‹¨
                            execution_successful = False
                            break

                # ì‹¤í–‰ ê²°ê³¼ ì¶œë ¥ ë° í›„ì† ì‘ì—…
                if execution_successful:
                    print(f"\nâœ… Migration SQL executed successfully!")
                    print(f"   Executed: {executed_count}/{total_blocks} blocks")
                    
                    print("\nâœ… Schema migration completed!")
                    print("ğŸ’¡ Next step: Run with --with-data to migrate data and initialize sequences.")
                    
                    src_conn.close()
                    tgt_conn.close()
                    print("Connections closed.")
                else:
                    print(f"\nâŒ Migration SQL execution failed.")
                    print(f"   Executed: {executed_count}/{total_blocks} blocks")
                    print(f"   Failed: {failed_count} blocks")
                    src_conn.close()
                    tgt_conn.close()
                    print("Connections closed.")
                    
            except Exception as e:
                print(f"\nAn unexpected error occurred during SQL execution: {e}")
                print("Rolling back transaction...")
                tgt_conn.rollback()
                print("Transaction rolled back.")
                src_conn.close()
                tgt_conn.close()
                print("Connections closed.")

    # --- SQL ì‹¤í–‰ ë ---


if __name__ == '__main__':
    main()
