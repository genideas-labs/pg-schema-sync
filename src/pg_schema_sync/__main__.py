#!/usr/bin/env python3
import psycopg2
from psycopg2 import sql # SQL ì‹ë³„ì ì•ˆì „ ì²˜ë¦¬ìš©
import yaml # YAML ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import datetime # íƒ€ì„ìŠ¤íƒ¬í”„ìš©
import os # ë””ë ‰í† ë¦¬ ìƒì„±ìš©
import argparse # ì»¤ë§¨ë“œë¼ì¸ ì¸ìˆ˜ ì²˜ë¦¬ìš©
import re # SQL ì •ê·œí™”ìš©
from collections import defaultdict
from dataMig import run_data_migration_parallel, compare_row_counts
# --- ì œì™¸í•  ê°ì²´ ëª©ë¡ ---
# Liquibase ë“± ë§ˆì´ê·¸ë ˆì´ì…˜ ë„êµ¬ ê´€ë ¨ í…Œì´ë¸” ë˜ëŠ” ê¸°íƒ€ ì œì™¸ ëŒ€ìƒ
EXCLUDE_TABLES = ['databasechangelog', 'databasechangeloglock']
# ê´€ë ¨ ì¸ë±ìŠ¤ ë˜ëŠ” ê¸°íƒ€ ì œì™¸ ëŒ€ìƒ
EXCLUDE_INDEXES = ['databasechangeloglock_pkey'] # í•„ìš”ì‹œ íƒ€ê²Ÿ ì „ìš© ì¸ë±ìŠ¤ ì¶”ê°€

# --- DB ì—°ê²° í•¨ìˆ˜ ---
def get_connection(config):
    conn = psycopg2.connect(**config)
    return conn

# --- Enum DDL ì¡°íšŒ ---
def fetch_enums(conn):
    cur = conn.cursor()
    query = """
    SELECT t.typname,
           'CREATE TYPE public.' || t.typname || ' AS ENUM (' ||
           string_agg(quote_literal(e.enumlabel), ', ' ORDER BY e.enumsortorder) ||
           ');' as ddl
    FROM pg_type t
    JOIN pg_enum e ON t.oid = e.enumtypid
    JOIN pg_namespace n ON t.typnamespace = n.oid
    WHERE n.nspname = 'public'
    GROUP BY t.typname;
    """
    cur.execute(query)
    enums = {typname: ddl for typname, ddl in cur.fetchall()}
    cur.close()
    return enums

# --- Enum Values ì¡°íšŒ ---
def fetch_enums_values(conn):
    """Enum íƒ€ì…ë³„ ê°’ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤."""
    cur = conn.cursor()
    # ë¨¼ì € public ìŠ¤í‚¤ë§ˆì˜ ëª¨ë“  enum íƒ€ì… ì´ë¦„ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    enum_types_query = """
    SELECT t.typname
    FROM pg_type t
    JOIN pg_namespace n ON t.typnamespace = n.oid
    WHERE n.nspname = 'public' AND t.typtype = 'e';
    """
    cur.execute(enum_types_query)
    enum_names = [row[0] for row in cur.fetchall()]
    
    enums_values = {}
    for enum_name in enum_names:
        # ê° enum íƒ€ì…ì˜ ê°’ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤.
        # psycopg2.sql ì‚¬ìš©í•˜ì—¬ ì•ˆì „í•˜ê²Œ ì‹ë³„ì ì²˜ë¦¬
        query_template = sql.SQL("SELECT enum_range(NULL::{})").format(
            sql.Identifier('public', enum_name)
        )
        try:
            cur.execute(query_template)
            # ê²°ê³¼ëŠ” íŠœí”Œ í˜•íƒœì˜ ë¦¬ìŠ¤íŠ¸ [(value1, value2, ...)] ì´ë¯€ë¡œ ì²«ë²ˆì§¸ ìš”ì†Œ ì‚¬ìš©
            values = cur.fetchone()[0] if cur.rowcount > 0 else []
            enums_values[enum_name] = sorted(values) # ì¼ê´€ëœ ë¹„êµë¥¼ ìœ„í•´ ì •ë ¬
        except psycopg2.Error as e:
            print(f"Warning: Could not fetch values for enum {enum_name}. Error: {e}")
            enums_values[enum_name] = [] # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬
            conn.rollback() # ì˜¤ë¥˜ ë°œìƒ ì‹œ íŠ¸ëœì­ì…˜ ë¡¤ë°±

    cur.close()
    return enums_values

# --- Table Metadata (ì»¬ëŸ¼ ì •ë³´) ì¡°íšŒ ---
# --- Table Metadata (ì»¬ëŸ¼ ì •ë³´) ì¡°íšŒ ---
def fetch_tables_metadata(conn):
    cur = conn.cursor()

    # 1. í…Œì´ë¸” ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    cur.execute("""
    SELECT table_name
    FROM information_schema.tables
    WHERE table_schema = 'public' AND table_type = 'BASE TABLE'
    """)
    table_names = [row[0] for row in cur.fetchall()]

    # 2. ì œì•½ì¡°ê±´ ì •ë³´: FK / UNIQUE / PRIMARY
    cur.execute("""
    SELECT
      tc.constraint_name,
      tc.constraint_type,
      tc.table_name,
      kcu.column_name,
      ccu.table_name AS foreign_table,
      ccu.column_name AS foreign_column
    FROM information_schema.table_constraints AS tc
    LEFT JOIN information_schema.key_column_usage AS kcu
      ON tc.constraint_name = kcu.constraint_name AND tc.table_schema = kcu.table_schema
    LEFT JOIN information_schema.constraint_column_usage AS ccu
      ON tc.constraint_name = ccu.constraint_name AND tc.table_schema = ccu.table_schema
    WHERE tc.table_schema = 'public';
    """)

    fk_lookup = {}
    unique_col_flags = {}
    primary_col_flags = {}
    composite_uniques_temp = defaultdict(list)
    composite_primaries_temp = defaultdict(list)

    for constraint_name, constraint_type, table, column, ref_table, ref_col in cur.fetchall():
        if constraint_type == 'FOREIGN KEY' and ref_table and ref_col:
            fk_lookup[(table, column)] = {'table': ref_table, 'column': ref_col}
        elif constraint_type == 'UNIQUE':
            if column:
                composite_uniques_temp[(table, constraint_name)].append(column)
        elif constraint_type == 'PRIMARY KEY':
            if column:
                composite_primaries_temp[(table, constraint_name)].append(column)

    for (table, constraint), cols in composite_uniques_temp.items():
        if len(cols) == 1:
            unique_col_flags[(table, cols[0])] = True
        elif len(cols) > 1:
            pass  # ë³µí•© í‚¤ëŠ” ë‚˜ì¤‘ì— ì²˜ë¦¬

    for (table, constraint), cols in composite_primaries_temp.items():
        if len(cols) == 1:
            primary_col_flags[(table, cols[0])] = True
        elif len(cols) > 1:
            pass  # ë³µí•© í‚¤ëŠ” ë‚˜ì¤‘ì— ì²˜ë¦¬
    
    # ìµœì¢… composite êµ¬ì¡° ìƒì„±
    final_composite_uniques = defaultdict(list)
    for (table, constraint_name), cols in composite_uniques_temp.items():
        if len(cols) > 1:
            # ì¤‘ë³µ ì œê±°í•˜ë©´ì„œ ìˆœì„œ ìœ ì§€
            seen = set()
            deduped = []
            for c in cols:
                if c not in seen:
                    seen.add(c)
                    deduped.append(c)
            final_composite_uniques[table].append((constraint_name, deduped))

    final_composite_primaries = {}
    for (table, constraint_name), cols in composite_primaries_temp.items():
        if len(cols) > 1:
            # ì¤‘ë³µ ì œê±°
            seen = set()
            deduped = []
            for c in cols:
                if c not in seen:
                    seen.add(c)
                    deduped.append(c)
            final_composite_primaries[table] = deduped

    # 3. ì»¬ëŸ¼ ì •ë³´ ìˆ˜ì§‘
    tables_metadata = {}
    for table_name in table_names:
        cur.execute("""
        SELECT column_name, data_type, is_nullable, udt_name, column_default, is_identity
        FROM information_schema.columns
        WHERE table_schema = 'public' AND table_name = %s
        ORDER BY ordinal_position;
        """, (table_name,))

        columns = []
        for col_name, data_type, is_nullable, udt_name, col_default, is_identity in cur.fetchall():
            col_type = data_type
            if data_type == 'ARRAY':
                base_type = udt_name.lstrip('_')
                col_type = base_type + '[]'

            col_data = {
                'name': col_name,
                'type': col_type,
                'nullable': is_nullable == 'YES',
                'default': col_default,
                'identity': is_identity == 'YES'
            }
            if (table_name, col_name) in fk_lookup:
                col_data['foreign_key'] = fk_lookup[(table_name, col_name)]
            if (table_name, col_name) in unique_col_flags:
                col_data['unique'] = True
            if (table_name, col_name) in primary_col_flags:
                col_data['primary_key'] = True

            columns.append(col_data)

        tables_metadata[table_name] = columns

    cur.close()
    return tables_metadata, final_composite_uniques, final_composite_primaries

# def fetch_tables_metadata(conn):
#     """í…Œì´ë¸”ë³„ ì»¬ëŸ¼ ë©”íƒ€ë°ì´í„°(ì´ë¦„, íƒ€ì…, Nullì—¬ë¶€, ê¸°ë³¸ê°’, identity, FK, UNIQUE)ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
#     cur = conn.cursor()
#     params = []
#     query_str = """
#     SELECT table_name
#     FROM information_schema.tables
#     WHERE table_schema = 'public' AND table_type='BASE TABLE'
#     """
#     if EXCLUDE_TABLES:
#         query_str += " AND table_name NOT IN %s"
#         params.append(tuple(EXCLUDE_TABLES))

#     cur.execute(query_str, params if params else None)
#     tables_metadata = {}
#     table_names = [row[0] for row in cur.fetchall()]

#     fk_lookup = {}
#     unique_lookup = set()
#     pk_lookup = set()

#     # ì „ì²´ FK ì •ë³´ ë¯¸ë¦¬ ì¡°íšŒ
#     cur.execute("""
#     SELECT
#     tc.constraint_type,
#         tc.table_name,
#         kcu.column_name,
#         ccu.table_name AS foreign_table_name,
#         ccu.column_name AS foreign_column_name
#     FROM information_schema.table_constraints AS tc
#     LEFT JOIN information_schema.key_column_usage AS kcu
#         ON tc.constraint_name = kcu.constraint_name AND tc.table_schema = kcu.table_schema
#     LEFT JOIN information_schema.constraint_column_usage AS ccu
#         ON tc.constraint_name = ccu.constraint_name AND tc.table_schema = ccu.table_schema
#     WHERE tc.table_schema = 'public';
#     """)


#     for constraint_type, table_name, column_name, ref_table, ref_column in cur.fetchall():
#         if not column_name:
#             continue  # ë³µí•© í‚¤ì˜ ì¼ë¶€ê°€ ì•„ë‹ ìˆ˜ ìˆìŒ
#         if constraint_type == 'FOREIGN KEY' and ref_table and ref_column:
#             fk_lookup[(table_name, column_name)] = {"table": ref_table, "column": ref_column}
#         elif constraint_type == 'UNIQUE':
#             unique_lookup.add((table_name, column_name))
#         elif constraint_type == 'PRIMARY KEY':
#             pk_lookup.add((table_name, column_name))


#     # í…Œì´ë¸”ë³„ ì»¬ëŸ¼ ì¡°íšŒ
#     for table_name in table_names:
#         col_query = f"""
#         SELECT column_name,
#                data_type,
#                is_nullable,
#                udt_name,
#                column_default,
#                is_identity
#         FROM information_schema.columns
#         WHERE table_schema = 'public' AND table_name = %s
#         ORDER BY ordinal_position;
#         """
#         cur.execute(col_query, (table_name,))
#         columns = []
#         for col_name, data_type, is_nullable, udt_name, col_default, is_identity in cur.fetchall():
#             col_type = data_type
#             if data_type == 'ARRAY':
#                 base_type = udt_name.lstrip('_')
#                 col_type = base_type + '[]' if base_type else 'text[]'  # fallback

#             col_data = {
#                 'name': col_name,
#                 'type': col_type,
#                 'nullable': is_nullable == 'YES',
#                 'default': col_default,
#                 'identity': is_identity == 'YES'
#             }
#             if (table_name, col_name) in fk_lookup:
#                 col_data["foreign_key"] = fk_lookup[(table_name, col_name)]
#             if (table_name, col_name) in unique_lookup:
#                 col_data["unique"] = True
#             if (table_name, col_name) in pk_lookup:
#                 col_data["primary_key"] = True  # âœ… ì—¬ê¸°ì— ì¶”ê°€
#             columns.append(col_data)
#         tables_metadata[table_name] = columns

#     cur.close()
#     return tables_metadata



# --- Table DDL ìƒì„± í•¨ìˆ˜ (ë©”íƒ€ë°ì´í„° ê¸°ë°˜ - í•„ìš” ì‹œ ì‚¬ìš©) ---
def generate_create_table_ddl(table_name, columns, 
                              composite_uniques=None, 
                              composite_primaries=None):
    """ì»¬ëŸ¼ ë©”íƒ€ë°ì´í„°ì™€ ë³µí•© ì œì•½ ì¡°ê±´ìœ¼ë¡œ CREATE TABLE DDL ìƒì„±"""
    composite_uniques = composite_uniques or {}
    composite_primaries = composite_primaries or {}

    col_defs = []
    table_constraints = []
    enum_ddls = []

    for col in columns:
        col_type = col['type']
        quoted_col_name = f'"{col["name"]}"'

        # ì‚¬ìš©ì ì •ì˜ enum íƒ€ì… ì²˜ë¦¬
        if isinstance(col_type, str) and col_type.upper() == 'USER-DEFINED':
            if table_name in ("menu_item_opts_set_schema", "menu_item_opts_schema") and col['name'] == "type":
                col_type = "public.option_type"
                enum_ddls.append(
                    """DO $$
BEGIN
  IF NOT EXISTS (SELECT 1 FROM pg_type WHERE typname = 'option_type') THEN
    CREATE TYPE public.option_type AS ENUM ('additional', 'substitution');
  END IF;
END$$;"""
                )
            elif table_name == "menu" and col['name'] == "onboarding_status":
                col_type = "public.p2_onboarding_status"
                enum_ddls.append(
                    """DO $$
BEGIN
  IF NOT EXISTS (SELECT 1 FROM pg_type WHERE typname = 'p2_onboarding_status') THEN
    CREATE TYPE public.p2_onboarding_status AS ENUM ('NOT_STARTED', 'STEP1', 'STEP2', 'STEP3', 'COMPLETED');
  END IF;
END$$;"""
                )
            elif table_name in {"order_menu_items", "order_payments", "orders"} and col['name'] == "status":
                col_type = "public.order_status"
                enum_ddls.append(
                    """DO $$
BEGIN
  IF NOT EXISTS (SELECT 1 FROM pg_type WHERE typname = 'order_status') THEN
    CREATE TYPE public.order_status AS ENUM ('new', 'accepted', 'canceled', 'banned', 'cooking', 'pickup', 'prepayment', 'done');
  END IF;
END$$;"""
                )

        # âœ… inline ì»¬ëŸ¼ ì •ì˜ ì²˜ë¦¬
        is_identity = col.get("identity", False)
        is_primary = col.get("primary_key", False)
        is_unique = col.get("unique", False)
        is_nullable = col.get("nullable", True)
        default_val = col.get("default")

        if is_identity and is_primary:
            col_def = f'{quoted_col_name} BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY'
        else:
            col_def = f"{quoted_col_name} {col_type}"
            if is_identity:
                col_def += " GENERATED BY DEFAULT AS IDENTITY"
            if default_val is not None:
                col_def += f" DEFAULT {default_val}"
            if not is_nullable:
                col_def += " NOT NULL"
            if is_unique:
                col_def += " UNIQUE"
            if is_primary:
                col_def += " PRIMARY KEY"

        col_defs.append(col_def)
    print("composite_uniques",composite_uniques)
    # âœ… ë³µí•© UNIQUE ì œì•½ì¡°ê±´
    if table_name in composite_uniques:
        for constraint_name, cols in composite_uniques[table_name]:
            quoted_cols = ", ".join(f'"{c}"' for c in cols)
            table_constraints.append(
                f'CONSTRAINT "{constraint_name}" UNIQUE ({quoted_cols})'
            )

    # âœ… ë³µí•© PRIMARY KEY ì œì•½ì¡°ê±´
    if table_name in composite_primaries:
        cols = composite_primaries[table_name]
        quoted_cols = ", ".join(f'"{col}"' for col in cols)
        constraint_name = f"{table_name}_pkey"
        table_constraints.append(f'CONSTRAINT {constraint_name} PRIMARY KEY ({quoted_cols})')
    print("table_constraints",table_constraints)
    # ì „ì²´ CREATE TABLE DDL
    all_defs = col_defs + table_constraints
    table_ddl = f'CREATE TABLE public."{table_name}" (\n    ' + ",\n    ".join(all_defs) + "\n);"

    return "\n\n".join(enum_ddls + [table_ddl])


# def generate_foreign_key_ddls(tables_metadata):
#     """ëª¨ë“  foreign keyë¥¼ ALTER TABLE DDLë¡œ ìƒì„±"""
#     fk_ddls = []
#     for table_name, columns in tables_metadata.items():
#         for col in columns:
#             if "foreign_key" in col:
#                 fk = col["foreign_key"]
#                 constraint_name = f"{table_name}_{col['name']}_fkey"
#                 ddl = (
#                     f'ALTER TABLE public."{table_name}" '
#                     f'ADD CONSTRAINT "{constraint_name}" '
#                     f'FOREIGN KEY ("{col["name"]}") '
#                     f'REFERENCES public."{fk["table"]}" ("{fk["column"]}");'
#                 )
#                 fk_ddls.append(ddl)
#     return fk_ddls

# --- View DDL ì¡°íšŒ ---
def fetch_views(conn):
    """ë·° DDLì„ information_schema.views.view_definitionì„ ì‚¬ìš©í•˜ì—¬ ì¡°íšŒí•©ë‹ˆë‹¤."""
    cur = conn.cursor()
    query = """
    SELECT table_name,
           view_definition
    FROM information_schema.views
    WHERE table_schema = 'public';
    """
    cur.execute(query)
    views = {}
    for view_name, view_def in cur.fetchall():
        # view_definitionì€ SELECT ë¬¸ë§Œ í¬í•¨í•˜ë¯€ë¡œ CREATE OR REPLACE VIEW ì¶”ê°€
        # view_definition ëì— ì„¸ë¯¸ì½œë¡ ì´ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì œê±° í›„ ì¶”ê°€
        ddl = f"CREATE OR REPLACE VIEW public.{view_name} AS\n{view_def.rstrip(';')};"
        views[view_name] = ddl
    # ì¤‘ë³µ ì½”ë“œ ì œê±°: ìœ„ì—ì„œ ì´ë¯¸ views ë”•ì…”ë„ˆë¦¬ì— í• ë‹¹í•¨
    cur.close()
    return views

# --- Function DDL ì¡°íšŒ ---
def fetch_functions(conn):
    cur = conn.cursor()
    query = """
    SELECT p.proname,
           pg_get_functiondef(p.oid) as ddl
    FROM pg_proc p
    JOIN pg_namespace n ON p.pronamespace = n.oid
    JOIN pg_language l ON p.prolang = l.oid  -- Join with pg_language
    WHERE n.nspname = 'public'
      AND p.prokind = 'f'  -- Filter for regular functions
      AND l.lanname != 'c'; -- Filter out C language functions
    """

    cur.execute(query)
    functions = {proname: ddl for proname, ddl in cur.fetchall()}
    cur.close()
    return functions

# --- Index DDL ì¡°íšŒ (ê¸°ë³¸ í‚¤ ì¸ë±ìŠ¤ ë¶„ë¦¬) ---
def fetch_indexes(conn):
    """ì¸ë±ìŠ¤ DDLì„ ì¡°íšŒí•˜ë˜, UNIQUE/PRIMARY KEY ì œì•½ì¡°ê±´ìœ¼ë¡œ ìƒì„±ëœ ì¸ë±ìŠ¤ëŠ” ì œì™¸í•©ë‹ˆë‹¤."""
    cur = conn.cursor()

    # 1. constraintì—ì„œ ìƒì„±ëœ ì¸ë±ìŠ¤ ì´ë¦„ë“¤ ìˆ˜ì§‘
    cur.execute("""
    SELECT conname
    FROM pg_constraint
    WHERE contype IN ('u', 'p')  -- UNIQUE or PRIMARY KEY
      AND connamespace = 'public'::regnamespace;
    """)
    constraint_index_names = {row[0] for row in cur.fetchall()}

    # 2. pg_indexesì—ì„œ ì¼ë°˜ ì¸ë±ìŠ¤ ì¡°íšŒ
    cur.execute("""
    SELECT indexname,
           indexdef
    FROM pg_indexes
    WHERE schemaname = 'public';
    """)

    indexes = {}
    pkey_indexes = {}

    for indexname, ddl in cur.fetchall():
        if indexname in constraint_index_names:
            # âœ… UNIQUE/PK constraintì—ì„œ ìœ ë˜í•œ ì¸ë±ìŠ¤ëŠ” ë¬´ì‹œ
            continue
        if indexname.endswith('_pkey'):
            pkey_indexes[indexname] = ddl
        else:
            indexes[indexname] = ddl

    cur.close()
    return indexes, pkey_indexes


# --- Sequence DDL ì¡°íšŒ ---
def fetch_sequences(conn):
    """ì‹œí€€ìŠ¤ DDLì„ ì¡°íšŒí•©ë‹ˆë‹¤."""
    cur = conn.cursor()
    
    # ì‹œí€€ìŠ¤ ëª©ë¡ ì¡°íšŒ
    query = """
    SELECT 
        c.relname as sequence_name
    FROM pg_class c 
    JOIN pg_namespace n ON c.relnamespace = n.oid 
    WHERE n.nspname = 'public' AND c.relkind = 'S'
    ORDER BY c.relname;
    """
    cur.execute(query)
    rows = cur.fetchall()
    
    sequences = {}
    
    for row in rows:
        seq_name = row[0]
        
        # ì‹œí€€ìŠ¤ì˜ í˜„ì¬ ê°’ í™•ì¸
        try:
            cur.execute(f"SELECT last_value, is_called FROM public.{seq_name}")
            current_last_value, current_is_called = cur.fetchone()
        except Exception as e:
            print(f"Warning: Could not fetch current value for sequence {seq_name}: {e}")
            current_last_value, current_is_called = None, False
        
        # ê¸°ë³¸ CREATE SEQUENCE DDL ìƒì„±
        ddl_parts = [f"CREATE SEQUENCE public.{seq_name}"]
        
        # í˜„ì¬ ê°’ ì„¤ì • (ì‹œí€€ìŠ¤ê°€ ì´ë¯¸ ì‚¬ìš©ëœ ê²½ìš°)
        if current_is_called and current_last_value is not None:
            ddl_parts.append(f"RESTART WITH {current_last_value}")
        
        ddl = " ".join(ddl_parts) + ";"
        sequences[seq_name] = ddl
    
    cur.close()
    return sequences

def sync_sequence_values(src_conn, tgt_conn, sequence_names):
    """ì‹œí€€ìŠ¤ì˜ í˜„ì¬ ê°’ì„ ì†ŒìŠ¤ì—ì„œ íƒ€ê²Ÿìœ¼ë¡œ ë™ê¸°í™”í•©ë‹ˆë‹¤."""
    print("\n--- Syncing Sequence Values ---")
    
    with src_conn.cursor() as src_cur, tgt_conn.cursor() as tgt_cur:
        for seq_name in sequence_names:
            try:
                # ì†ŒìŠ¤ ì‹œí€€ìŠ¤ì˜ í˜„ì¬ ê°’ ì¡°íšŒ
                src_cur.execute(f"SELECT last_value, is_called FROM public.{seq_name}")
                src_last_value, src_is_called = src_cur.fetchone()
                
                # íƒ€ê²Ÿ ì‹œí€€ìŠ¤ì˜ í˜„ì¬ ê°’ ì¡°íšŒ
                tgt_cur.execute(f"SELECT last_value, is_called FROM public.{seq_name}")
                tgt_last_value, tgt_is_called = tgt_cur.fetchone()
                
                print(f"  ğŸ“Š {seq_name}:")
                print(f"    Source: last_value={src_last_value}, is_called={src_is_called}")
                print(f"    Target: last_value={tgt_last_value}, is_called={tgt_is_called}")
                
                # ê°’ì´ ë‹¤ë¥¸ ê²½ìš°ì—ë§Œ ì—…ë°ì´íŠ¸
                if src_last_value != tgt_last_value:
                    # ì‹œí€€ìŠ¤ ê°’ì„ ì†ŒìŠ¤ì™€ ë™ì¼í•˜ê²Œ ì„¤ì •
                    setval_sql = f"SELECT setval('public.{seq_name}', {src_last_value}, {src_is_called})"
                    print(f"    Executing: {setval_sql}")
                    tgt_cur.execute(setval_sql)
                    
                    # ì—…ë°ì´íŠ¸ í›„ ê°’ í™•ì¸
                    tgt_cur.execute(f"SELECT last_value, is_called FROM public.{seq_name}")
                    new_tgt_last_value, new_tgt_is_called = tgt_cur.fetchone()
                    print(f"    After setval: last_value={new_tgt_last_value}, is_called={new_tgt_is_called}")
                    
                    print(f"  âœ… {seq_name}: {tgt_last_value} â†’ {src_last_value}")
                else:
                    print(f"  â­ï¸  {seq_name}: already synced ({src_last_value})")
                    
            except Exception as e:
                print(f"  âŒ {seq_name}: failed to sync - {e}")
                import traceback
                traceback.print_exc()

# --- ì•ˆì „í•œ íƒ€ì… ë³€ê²½ íŒë‹¨ í•¨ìˆ˜ ---
def is_safe_type_change(old_type, new_type):
    """ì•”ì‹œì  ë³€í™˜ì´ ê°€ëŠ¥í•˜ê³  ì•ˆì „í•œ íƒ€ì… ë³€ê²½ì¸ì§€ íŒë‹¨í•©ë‹ˆë‹¤."""
    old_type_norm = normalize_sql(old_type)
    new_type_norm = normalize_sql(new_type)

    # varchar ê¸¸ì´ ì¦ê°€ ë˜ëŠ” textë¡œ ë³€ê²½
    if old_type_norm.startswith('character varying') and (new_type_norm.startswith('character varying') or new_type_norm == 'text'):
        try:
            old_len_match = re.search(r'\((\d+)\)', old_type_norm)
            new_len_match = re.search(r'\((\d+)\)', new_type_norm)
            old_len = int(old_len_match.group(1)) if old_len_match else float('inf')
            new_len = int(new_len_match.group(1)) if new_len_match else float('inf')
            # ê¸¸ì´ê°€ ê°™ê±°ë‚˜ ì¦ê°€í•˜ëŠ” ê²½ìš° ë˜ëŠ” textë¡œ ë³€ê²½í•˜ëŠ” ê²½ìš° ì•ˆì „
            return new_len >= old_len or new_type_norm == 'text'
        except:
            return False # ê¸¸ì´ íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì•ˆì „í•˜ì§€ ì•ŠìŒìœ¼ë¡œ ê°„ì£¼
    # ìˆ«ì íƒ€ì… í™•ì¥ (smallint -> int -> bigint)
    elif old_type_norm == 'smallint' and new_type_norm in ['integer', 'bigint']:
        return True
    elif old_type_norm == 'integer' and new_type_norm == 'bigint':
        return True
    # ìˆ«ì -> ë¬¸ìì—´ (ì¼ë°˜ì ìœ¼ë¡œ ì•ˆì „)
    elif old_type_norm in ['smallint', 'integer', 'bigint', 'numeric', 'real', 'double precision'] and \
         (new_type_norm.startswith('character varying') or new_type_norm == 'text'):
         return True
    # TODO: ë‹¤ë¥¸ ì•ˆì „í•œ ë³€í™˜ ì¶”ê°€ ê°€ëŠ¥ (ì˜ˆ: timestamp -> timestamptz)

    return False # ê·¸ ì™¸ëŠ” ì•ˆì „í•˜ì§€ ì•ŠìŒìœ¼ë¡œ ê°„ì£¼

# --- ë¹„êµ í›„ migration SQL ìƒì„± (íƒ€ì…ë³„ ë¡œì§ ë¶„ê¸°, Enum DDL ì°¸ì¡° ì¶”ê°€, ALTER TABLE ì§€ì› ì¶”ê°€) ---
def compare_and_generate_migration(src_data, tgt_data, obj_type, src_enum_ddls=None, use_alter=False,
                                 src_composite_uniques=None, tgt_composite_uniques=None,
                                 src_composite_primaries=None, tgt_composite_primaries=None):
    """
    ì†ŒìŠ¤ì™€ íƒ€ê²Ÿ ë°ì´í„°ë¥¼ ë¹„êµí•˜ì—¬ ë§ˆì´ê·¸ë ˆì´ì…˜ SQLê³¼ ê±´ë„ˆë›´ SQLì„ ìƒì„±í•©ë‹ˆë‹¤.
    obj_typeì— ë”°ë¼ ë¹„êµ ë°©ì‹ì„ ë‹¤ë¥´ê²Œ ì ìš©í•©ë‹ˆë‹¤.
    use_alter=Trueì¼ ê²½ìš°, í…Œì´ë¸” ì»¬ëŸ¼ ì¶”ê°€/ì‚­ì œì— ëŒ€í•´ ALTER TABLE ì‚¬ìš© ì‹œë„.
    Enum íƒ€ì…ì˜ DDL ìƒì„±ì„ ìœ„í•´ src_enum_ddls ë”•ì…”ë„ˆë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.
    """
    migration_sql = []
    skipped_sql = []
    alter_statements = [] # í•¨ìˆ˜ ì‹œì‘ ì‹œ ì´ˆê¸°í™”
    src_keys = set(src_data.keys())
    tgt_keys = set(tgt_data.keys())

    # ì†ŒìŠ¤ì—ë§Œ ìˆëŠ” ê°ì²´ ì²˜ë¦¬
    for name in src_keys - tgt_keys:
        if obj_type == "TABLE":
            ddl = generate_create_table_ddl(
                        name,
                        src_data[name],
                        composite_uniques=src_composite_uniques,
                        composite_primaries=src_composite_primaries
                        )

        elif obj_type == "TYPE": # ì†ŒìŠ¤ì—ë§Œ ìˆëŠ” Enum ì²˜ë¦¬
            ddl = src_enum_ddls.get(name, f"-- ERROR: DDL not found for Enum {name}")
        elif obj_type == "SEQUENCE": # ì†ŒìŠ¤ì—ë§Œ ìˆëŠ” Sequence ì²˜ë¦¬
            ddl = src_data.get(name, f"-- ERROR: DDL not found for Sequence {name}")
        elif obj_type == "INDEX":
            raw_ddl = src_data.get(name, f"-- ERROR: DDL not found for Index {name}")
            ddl = f"""
                    DO $$
                    BEGIN
                        IF NOT EXISTS (
                            SELECT 1 FROM pg_indexes WHERE schemaname = 'public' AND indexname = '{name}'
                        ) THEN
                            {raw_ddl};
                        END IF;
                    END$$;
                    """.strip()    
        else: # View, Function, Index ë“±
            ddl = src_data.get(name, f"-- ERROR: DDL not found for {obj_type} {name}")
        migration_sql.append(f"-- CREATE {obj_type} {name}\n{ddl}\n")

    # ì–‘ìª½ì— ëª¨ë‘ ìˆëŠ” ê°ì²´ ë¹„êµ ì²˜ë¦¬
    for name in src_keys.intersection(tgt_keys):
        are_different = False
        ddl = "" # ë³€ê²½ ì‹œ ì‚¬ìš©í•  DDL (ì£¼ë¡œ ì†ŒìŠ¤ ê¸°ì¤€)

        if obj_type == "TABLE":
            src_cols_map = {col['name']: col for col in src_data[name]}
            tgt_cols_map = {col['name']: col for col in tgt_data[name]}
            src_col_names = set(src_cols_map.keys())
            tgt_col_names = set(tgt_cols_map.keys())

            cols_to_add = src_col_names - tgt_col_names
            cols_to_drop = tgt_col_names - src_col_names
            cols_to_compare = src_col_names.intersection(tgt_col_names)

            # alter_statements = [] # ì—¬ê¸°ì„œ ì´ˆê¸°í™” ì œê±°
            needs_recreate = False # ALTERë¡œ ì²˜ë¦¬ ë¶ˆê°€ëŠ¥í•œ ë³€ê²½ì´ ìˆëŠ”ì§€ ì—¬ë¶€

            # ê³µí†µ ì»¬ëŸ¼ì´ í•˜ë‚˜ë„ ì—†ê³ , ì¶”ê°€/ì‚­ì œí•  ì»¬ëŸ¼ì´ ìˆë‹¤ë©´ ì¬ ìƒì„± í•„ìš” (ë²„ê·¸ ìˆ˜ì •)
            if not cols_to_compare and (cols_to_add or cols_to_drop):
                 needs_recreate = True

            # ì»¬ëŸ¼ ì •ì˜ ë¹„êµ (íƒ€ì…, Null ì—¬ë¶€ ë“±) - needs_recreateê°€ ì•„ì§ Falseì¼ ë•Œë§Œ ìˆ˜í–‰
            if not needs_recreate:
                for col_name in cols_to_compare:
                    src_col = src_cols_map[col_name]
                    tgt_col = tgt_cols_map[col_name]
                    src_type_norm = normalize_sql(src_col['type'])
                    tgt_type_norm = normalize_sql(tgt_col['type'])

                    # 1. íƒ€ì… ë³€ê²½ í™•ì¸
                    if src_type_norm != tgt_type_norm:
                        if use_alter and is_safe_type_change(tgt_type_norm, src_type_norm):
                            # ì•ˆì „í•œ íƒ€ì… ë³€ê²½ì´ë©´ ALTER TYPE ì¶”ê°€
                            quoted_col_name = f'"{col_name}"' # ë”°ì˜´í‘œ ì¶”ê°€
                            alter_statements.append(f"ALTER TABLE public.{name} ALTER COLUMN {quoted_col_name} TYPE {src_col['type']};")
                        else:
                            # ì•ˆì „í•˜ì§€ ì•Šì€ íƒ€ì… ë³€ê²½ì´ë©´ ì¬ ìƒì„± í•„ìš”
                            needs_recreate = True
                            break

                    # 2. Null í—ˆìš© ì—¬ë¶€ ë³€ê²½ í™•ì¸ (íƒ€ì…ì´ ë™ì¼í•  ë•Œë§Œ ê³ ë ¤)
                    elif src_col['nullable'] != tgt_col['nullable']:
                        if use_alter:
                            if src_col['nullable'] is False: # NOT NULLë¡œ ë³€ê²½
                                alter_statements.append(f"-- WARNING: Setting NOT NULL on column {col_name} may fail if existing data contains NULLs.")
                                quoted_col_name = f'"{col_name}"' # ë”°ì˜´í‘œ ì¶”ê°€
                                alter_statements.append(f"ALTER TABLE public.{name} ALTER COLUMN {quoted_col_name} SET NOT NULL;")
                            else: # NULL í—ˆìš©ìœ¼ë¡œ ë³€ê²½
                                quoted_col_name = f'"{col_name}"' # ë”°ì˜´í‘œ ì¶”ê°€
                                alter_statements.append(f"ALTER TABLE public.{name} ALTER COLUMN {quoted_col_name} DROP NOT NULL;")
                        else:
                             # use_alter=False ì´ë©´ ì¬ ìƒì„± í•„ìš”
                             needs_recreate = True
                             break

            # ALTER ë¬¸ ìƒì„± (ì»¬ëŸ¼ ì¶”ê°€/ì‚­ì œ) - needs_recreateê°€ Falseì´ê³  use_alter=Trueì¼ ë•Œë§Œ
            if not needs_recreate and use_alter:
                if cols_to_add:
                    for col_name in cols_to_add:
                        col = src_cols_map[col_name]
                        col_def = f"{col['name']} {col['type']}"
                        if col['default'] is not None:
                            col_def += f" DEFAULT {col['default']}"
                        if not col['nullable']:
                            col_def += " NOT NULL"
                        # sql.Identifier ì‚¬ìš© ìœ„í•´ conn ê°ì²´ í•„ìš” -> ì„ì‹œ ì²˜ë¦¬ (f-string ì˜¤ë¥˜ ìˆ˜ì •)
                        default_clause = f" DEFAULT {col['default']}" if col.get('default') is not None else ""
                        not_null_clause = " NOT NULL" if not col.get('nullable', True) else ""
                        # ì»¬ëŸ¼ ì´ë¦„ì— ë”°ì˜´í‘œ ì¶”ê°€ (psycopg2.sql.Identifier ëŒ€ì‹  ì„ì‹œ ì‚¬ìš©)
                        quoted_col_name = f'"{col_name}"'
                        alter_statements.append(f"ALTER TABLE public.{name} ADD COLUMN {quoted_col_name} {col['type']}{default_clause}{not_null_clause};")
                if cols_to_drop:
                    for col_name in cols_to_drop:
                        # ì»¬ëŸ¼ ì‚­ì œëŠ” ìœ„í—˜í•˜ë¯€ë¡œ ì£¼ì„ ì¶”ê°€
                        alter_statements.append(f"-- WARNING: Dropping column {col_name} may cause data loss.")
                        # ì»¬ëŸ¼ ì´ë¦„ì— ë”°ì˜´í‘œ ì¶”ê°€ (psycopg2.sql.Identifier ëŒ€ì‹  ì„ì‹œ ì‚¬ìš©)
                        quoted_col_name = f'"{col_name}"'
                        alter_statements.append(f"ALTER TABLE public.{name} DROP COLUMN {quoted_col_name};")

                if alter_statements: # ALTER ë¬¸ì´ ìƒì„±ëœ ê²½ìš° (ì¶”ê°€/ì‚­ì œ/ë³€ê²½ í¬í•¨)
                    migration_sql.append(f"-- ALTER TABLE {name} for column changes\n" + "\n".join(alter_statements) + "\n")
                    are_different = True # ë§ˆì´ê·¸ë ˆì´ì…˜ SQLì´ ìƒì„±ë˜ì—ˆìœ¼ë¯€ë¡œ differentë¡œ ì²˜ë¦¬
                else:
                    # ALTER ë¬¸ ì—†ê³ , needs_recreateë„ Falseì´ë©´ ë³€ê²½ ì—†ìŒ
                    are_different = False

            # ì¬ ìƒì„± í•„ìš” ì—¬ë¶€ ìµœì¢… ê²°ì •
            # needs_recreateê°€ Trueì´ë©´ ë¬´ì¡°ê±´ ì¬ ìƒì„±
            if needs_recreate:
                are_different = True
                ddl = generate_create_table_ddl(
                    name,
                    src_data[name],
                    composite_uniques=src_composite_uniques,
                        composite_primaries=src_composite_primaries
                )

                alter_statements = [] # ALTER ë¬¸ì€ ë¬´ì‹œ
            # use_alter=False ì´ê³  ì»¬ëŸ¼ êµ¬ì„±ì´ ë‹¤ë¥´ë©´ ì¬ ìƒì„±
            elif not use_alter and (len(src_cols_map) != len(tgt_cols_map) or \
                                    any(sc['name'] != tc['name'] or \
                                        normalize_sql(sc['type']) != normalize_sql(tc['type']) or \
                                        sc['nullable'] != tc['nullable']
                                        for sc, tc in zip(src_data[name], tgt_data[name]))):
                 are_different = True
                 ddl = generate_create_table_ddl(
                        name,
                        src_data[name],
                        composite_uniques=src_composite_uniques,
                        composite_primaries=src_composite_primaries
                        )

                 alter_statements = [] # ALTER ë¬¸ì€ ë¬´ì‹œ
            elif not alter_statements:
                 # ì¬ ìƒì„± í•„ìš” ì—†ê³ , ALTER ë¬¸ë„ ì—†ìœ¼ë©´ ë³€ê²½ ì—†ìŒ
                 are_different = False
        elif obj_type == "INDEX":
            if name not in tgt_data:
                ddl = src_data[name]
                ddl = f"""
                        DO $$
                        BEGIN
                            IF NOT EXISTS (
                                SELECT 1 FROM pg_indexes WHERE schemaname = 'public' AND indexname = '{name}'
                            ) THEN
                                {ddl};
                            END IF;
                        END$$;
                        """.strip()
                migration_sql.append(f"-- INDEX {name} differs or missing. Adding.\n{ddl}\n")
                continue
            else:
                if normalize_sql(src_data[name]) != normalize_sql(tgt_data[name]):
                    ddl = src_data[name]
                    ddl = f"""
                            DO $$
                            BEGIN
                                IF NOT EXISTS (
                                    SELECT 1 FROM pg_indexes WHERE schemaname = 'public' AND indexname = '{name}'
                                ) THEN
                                    {ddl};
                                END IF;
                            END$$;
                            """.strip()
                    migration_sql.append(f"-- INDEX {name} differs. Replacing.\n{ddl}\n")
                else:
                    commented = '\n'.join([f"-- {line}" for line in src_data[name].strip().splitlines()])
                    skipped_sql.append(f"-- INDEX {name} is up-to-date; skipping.\n{commented}\n")
                continue
        elif obj_type == "TYPE": # Enum íƒ€ì… ê°€ì •
            src_values = src_data[name]
            tgt_values = tgt_data[name]
            if src_values != tgt_values:
                are_different = True
                # Enum DDLì€ src_enum_ddls ì—ì„œ ê°€ì ¸ì˜´
                ddl = src_enum_ddls.get(name, f"-- ERROR: DDL not found for Enum {name}")
        elif obj_type == "FUNCTION":
            # í•¨ìˆ˜ëŠ” ì›ë³¸ DDLë¡œ ë¹„êµ (ì •ê·œí™” ì‹œ ë‹¬ëŸ¬ ì¸ìš© ë¬¸ì œ ë°œìƒ ê°€ëŠ¥ì„±)
            if src_data[name] != tgt_data[name]:
                are_different = True
                ddl = src_data[name]
        elif obj_type == "FOREIGN_KEY":
            if name not in tgt_data:
                are_different = True
                ddl = src_data[name]
            else:
                src_ddl = normalize_sql(src_data[name])
                tgt_ddl = normalize_sql(tgt_data[name])
                if src_ddl != tgt_ddl:
                    are_different = True
                    ddl = src_data[name]

            if are_different:
                # âœ… DROP ì—†ì´ ì¶”ê°€ë§Œ ì‹œë„
                migration_sql.append(f"-- FOREIGN_KEY {name} differs or missing. Adding.\n{ddl}\n")
            else:
                # ìŠ¤í‚µ ì²˜ë¦¬
                commented = '\n'.join([f"-- {line}" for line in src_data[name].strip().splitlines()])
                skipped_sql.append(f"-- FOREIGN_KEY {name} is up-to-date; skipping.\n{commented}\n")
            
            continue  # ğŸ‘ˆ ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ ì´í›„ ê³µí†µ ì²˜ë¦¬ ë¸”ë¡ ê±´ë„ˆëœ€
        elif obj_type == "SEQUENCE": # ì–‘ìª½ì— ìˆëŠ” Sequence ì²˜ë¦¬
            # ì‹œí€€ìŠ¤ê°€ í…Œì´ë¸”ì—ì„œ ì‚¬ìš© ì¤‘ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ DROP ëŒ€ì‹  ALTER ì‚¬ìš©
            src_ddl_norm = normalize_sql(src_data[name])
            tgt_ddl_norm = normalize_sql(tgt_data[name])
            if src_ddl_norm != tgt_ddl_norm:
                # RESTART WITH ê°’ë§Œ ì¶”ì¶œí•˜ì—¬ ALTER SEQUENCE ì‚¬ìš©
                restart_match = re.search(r'RESTART WITH (\d+)', src_data[name])
                if restart_match:
                    restart_value = restart_match.group(1)
                    ddl = f"ALTER SEQUENCE public.{name} RESTART WITH {restart_value};"
                    migration_sql.append(f"-- ALTER SEQUENCE {name} to sync current value\n{ddl}\n")
                else:
                    # RESTART WITHê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ CREATE SEQUENCE ì‚¬ìš©
                    ddl = src_data[name]
                    migration_sql.append(f"-- SEQUENCE {name} differs. Recreating.\nDROP SEQUENCE IF EXISTS public.{name} CASCADE;\n{ddl}\n")
            else:
                # ë™ì¼í•œ ê²½ìš° ìŠ¤í‚µ
                commented = '\n'.join([f"-- {line}" for line in src_data[name].strip().splitlines()])
                skipped_sql.append(f"-- SEQUENCE {name} is up-to-date; skipping.\n{commented}\n")
            continue  # ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ ì´í›„ ê³µí†µ ì²˜ë¦¬ ë¸”ë¡ ê±´ë„ˆëœ€
        else:
            # ë‚˜ë¨¸ì§€ íƒ€ì… (View, Index, Sequence): ì •ê·œí™”ëœ DDL ë¹„êµ
            src_ddl_norm = normalize_sql(src_data[name])
            tgt_ddl_norm = normalize_sql(tgt_data[name])
            if src_ddl_norm != tgt_ddl_norm:
                are_different = True
                ddl = src_data[name] # ë³€ê²½ ì‹œ ì†ŒìŠ¤ DDL ì‚¬ìš©

        # ë¹„êµ ê²°ê³¼ì— ë”°ë¼ SQL ìƒì„± (TABLE íƒ€ì…ì€ ìœ„ì—ì„œ ì²˜ë¦¬ë¨)
        if obj_type == "FOREIGN_KEY" and are_different:
            # FOREIGN KEYëŠ” DROP CONSTRAINT ì—†ì´ ê·¸ëƒ¥ ADD CONSTRAINTë§Œ ì‹œë„
            migration_sql.append(f"-- FOREIGN_KEY {name} differs or missing. Adding.\n{ddl}\n")
        elif obj_type != "TABLE" and are_different:
            # TABLE ì™¸ ë‹¤ë¥¸ íƒ€ì…ì´ ë‹¤ë¥´ê±°ë‚˜, TABLEì´ ALTER ë¶ˆê°€í•˜ì—¬ ì¬ ìƒì„± í•„ìš”í•œ ê²½ìš°
            action = "Recreating" if obj_type != "FUNCTION" else "Updating" # í•¨ìˆ˜ëŠ” Updateë¡œ í‘œì‹œ (DROP/CREATE ë™ì¼)
            migration_sql.append(f"-- {obj_type} {name} differs. {action}.\nDROP {obj_type.upper()} IF EXISTS public.{name} CASCADE;\n{ddl}\n")
        elif obj_type == "TABLE" and are_different and not alter_statements:
             # TABLEì´ ë‹¤ë¥´ì§€ë§Œ ALTER ë¬¸ì´ ìƒì„±ë˜ì§€ ì•Šì€ ê²½ìš° (ì¬ ìƒì„± í•„ìš”)
             migration_sql.append(f"-- TABLE {name} differs significantly. Recreating.\nDROP TABLE IF EXISTS public.{name} CASCADE;\n{ddl}\n")
        elif not are_different and not alter_statements: # í…Œì´ë¸” í¬í•¨ ëª¨ë“  íƒ€ì…ì´ ë™ì¼í•˜ê³  ALTER ë¬¸ë„ ì—†ëŠ” ê²½ìš°
            # ë™ì¼í•œ ê²½ìš°: ìŠ¤í‚µ ì²˜ë¦¬
            original_ddl = ""
            if obj_type == "TABLE":
                 original_ddl = generate_create_table_ddl(
                        name,
                        src_data[name],
                        composite_uniques=src_composite_uniques,
                        composite_primaries=src_composite_primaries
                        )
            elif obj_type == "TYPE":
                 original_ddl = src_enum_ddls.get(name, "") # ìŠ¤í‚µ ë¡œê·¸ìš© Enum DDL
            else: # View, Function, Index, Sequence ë“±
                 original_ddl = src_data.get(name, "") # src_dataê°€ DDL ë”•ì…”ë„ˆë¦¬ë¼ê³  ê°€ì •

            skipped_sql.append(f"-- {obj_type} {name} is up-to-date; skipping.\n")
            if original_ddl: # DDLì´ ìˆëŠ” ê²½ìš°ë§Œ ì£¼ì„ ì²˜ë¦¬í•˜ì—¬ ì¶”ê°€
                 commented_ddl = '\n'.join([f"-- {line}" for line in original_ddl.strip().splitlines()])
                 skipped_sql.append(commented_ddl + "\n")

    # íƒ€ê²Ÿì—ë§Œ ìˆëŠ” ê°ì²´ëŠ” í˜„ì¬ ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ (í•„ìš” ì‹œ ì¶”ê°€)

    return migration_sql, skipped_sql


# --- SQL ì •ê·œí™” í•¨ìˆ˜ ---
def normalize_sql(sql_text):
    """SQL ë¬¸ìì—´ì—ì„œ ì£¼ì„ ì œê±°, ì†Œë¬¸ì ë³€í™˜, ê³µë°± ì •ê·œí™” ìˆ˜í–‰ (ë‹¬ëŸ¬ ì¸ìš© ë¬¸ìì—´ ë³´í˜¸)"""
    if not sql_text:
        return ""

    # ë‹¬ëŸ¬ ì¸ìš© ë¬¸ìì—´ ì¶”ì¶œ ë° ì„ì‹œ ì¹˜í™˜
    dollar_quoted_strings = []
    def replace_dollar_quoted(match):
        dollar_quoted_strings.append(match.group(0))
        return f"__DOLLAR_QUOTED_STRING_{len(dollar_quoted_strings)-1}__"

    # ì •ê·œ í‘œí˜„ì‹ ìˆ˜ì •: ì‹œì‘ê³¼ ë íƒœê·¸ê°€ ë™ì¼í•´ì•¼ í•¨ ($tag$...$tag$)
    # íƒœê·¸ëŠ” ë¹„ì–´ìˆê±°ë‚˜, ë¬¸ìë¡œë§Œ êµ¬ì„±ë  ìˆ˜ ìˆìŒ
    sql_text_no_dollars = re.sub(r"(\$([a-zA-Z_]\w*)?\$).*?\1", replace_dollar_quoted, sql_text, flags=re.DOTALL)

    # -- ìŠ¤íƒ€ì¼ ì£¼ì„ ì œê±°
    processed_sql = re.sub(r'--.*$', '', sql_text_no_dollars, flags=re.MULTILINE)
    # /* */ ìŠ¤íƒ€ì¼ ì£¼ì„ ì œê±° (ê°„ë‹¨í•œ ê²½ìš°ë§Œ ì²˜ë¦¬, ì¤‘ì²© ë¶ˆê°€)
    # processed_sql = re.sub(r'/\*.*?\*/', '', processed_sql, flags=re.DOTALL) # í•„ìš” ì‹œ ì¶”ê°€

    # ì†Œë¬¸ìë¡œ ë³€í™˜ (ë‹¬ëŸ¬ ì¸ìš© ì œì™¸ ë¶€ë¶„ë§Œ)
    processed_sql = processed_sql.lower()
    # ê´„í˜¸, ì‰¼í‘œ, ì„¸ë¯¸ì½œë¡  ì£¼ë³€ ê³µë°± ì œê±°
    processed_sql = re.sub(r'\s*([(),;])\s*', r'\1', processed_sql)
    # ë“±í˜¸(=) ë“± ì—°ì‚°ì ì£¼ë³€ ê³µë°± ì œê±° (ë” ë§ì€ ì—°ì‚°ì ì¶”ê°€ ê°€ëŠ¥)
    processed_sql = re.sub(r'\s*([=<>!+-/*%])\s*', r'\1', processed_sql)
    # ì—¬ëŸ¬ ê³µë°± (ìŠ¤í˜ì´ìŠ¤, íƒ­, ê°œí–‰ í¬í•¨)ì„ ë‹¨ì¼ ìŠ¤í˜ì´ìŠ¤ë¡œ ë³€ê²½
    processed_sql = re.sub(r'\s+', ' ', processed_sql)
    # ì•ë’¤ ê³µë°± ì œê±°
    processed_sql = processed_sql.strip()

    # ì„ì‹œ ì¹˜í™˜ëœ ë‹¬ëŸ¬ ì¸ìš© ë¬¸ìì—´ ë³µì›
    for i, original_string in enumerate(dollar_quoted_strings):
        processed_sql = processed_sql.replace(f"__DOLLAR_QUOTED_STRING_{i}__", original_string)

    # ë§ˆì§€ë§‰ ì„¸ë¯¸ì½œë¡  ì œê±° (ì˜µì…˜)
    return processed_sql.rstrip(';')


# --- ê²€ì¦ ê²°ê³¼ ì¶œë ¥ í•¨ìˆ˜ ---
def print_verification_report(src_objs, tgt_objs, obj_type):
    """ì†ŒìŠ¤ì™€ íƒ€ê²Ÿ ê°ì²´ ëª©ë¡ì„ ë¹„êµí•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."""
    print(f"\nVerifying {obj_type}...")
    src_names = set(src_objs.keys())
    tgt_names = set(tgt_objs.keys())

    source_only = sorted(list(src_names - tgt_names))
    target_only = sorted(list(tgt_names - src_names))

    print(f"  Source Count: {len(src_names)}")
    print(f"  Target Count: {len(tgt_names)}")

    if source_only:
        print(f"  Objects only in Source ({len(source_only)}): {', '.join(source_only)}")
    else:
        print("  Objects only in Source (0): None")

    if target_only:
        print(f"  Objects only in Target ({len(target_only)}): {', '.join(target_only)}")
    else:
        print("  Objects only in Target (0): None")

    is_synced = not source_only and not target_only
    status = "Synced" if is_synced else "Differences found"
    print(f"  Status: {status}")
    return is_synced

def extract_foreign_keys(metadata):
    """
    { "table.col->ref_table.ref_col": DDL } í˜•íƒœë¡œ ë°˜í™˜
    """
    fk_map = {}
    for table_name, columns in metadata.items():
        for col in columns:
            fk = col.get("foreign_key")
            if fk:
                constraint_key = f"{table_name}.{col['name']}->{fk['table']}.{fk['column']}"
                constraint_name = f"{table_name}_{col['name']}_fkey"
                ddl = (
                    f'ALTER TABLE public."{table_name}" '
                    f'ADD CONSTRAINT "{constraint_name}" '
                    f'FOREIGN KEY ("{col["name"]}") '
                    f'REFERENCES public."{fk["table"]}" ("{fk["column"]}");'
                )
                fk_map[constraint_key] = ddl
    return fk_map

def main():
    # --- ì»¤ë§¨ë“œë¼ì¸ ì¸ìˆ˜ íŒŒì‹± ---
    parser = argparse.ArgumentParser(description="Compare source and target PostgreSQL schemas and generate/apply migration SQL, or verify differences.")
    # Verification flag
    parser.add_argument('--verify', action='store_true',
                        help="Only verify schema differences (object names and counts) without generating/executing SQL.")
    # Commit flag (only relevant if --verify is not used)
    parser.add_argument('--commit', action=argparse.BooleanOptionalAction, default=True,
                        help="Execute the generated migration SQL on the target database. Use --no-commit to only generate files. Ignored if --verify is used.")
    # Experimental ALTER TABLE flag
    parser.add_argument('--use-alter', action='store_true', default=False,
                        help="EXPERIMENTAL: Use ALTER TABLE for column additions/deletions instead of DROP/CREATE. Use with caution.")
    parser.add_argument('--with-data', action='store_true',
                    help="Include data migration after schema changes")
    args = parser.parse_args()
    # --- ì¸ìˆ˜ íŒŒì‹± ë ---

    # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„± (YYYYMMDDHHMMSS í˜•ì‹)
    timestamp = datetime.datetime.now().strftime("%Y%m%d%H%M%S")

    # history ë””ë ‰í† ë¦¬ í™•ì¸ ë° ìƒì„±
    history_dir = "history"
    os.makedirs(history_dir, exist_ok=True)

    # config.yaml íŒŒì¼ ë¡œë“œ
    try:
        with open("config.yaml", 'r', encoding='utf-8') as stream:
            config = yaml.safe_load(stream)
            if not config:
                print("Error: config.yaml is empty or invalid.")
                return
    except FileNotFoundError:
        print("Error: config.yaml not found.")
        return
    except yaml.YAMLError as exc:
        print(f"Error parsing config.yaml: {exc}")
        return
    except Exception as e:
        print(f"An unexpected error occurred while reading config.yaml: {e}")
        return

    # ì„¤ì • ìœ íš¨ì„± ê²€ì‚¬ ë° ì¶”ì¶œ
    if 'source' not in config or not isinstance(config['source'], dict):
        print("Error: 'source' configuration is missing or invalid in config.yaml.")
        return
    if 'targets' not in config or not isinstance(config['targets'], dict) or 'gcp_test' not in config['targets'] or not isinstance(config['targets']['gcp_test'], dict):
        print("Error: 'targets.gcp_test' configuration is missing or invalid in config.yaml.")
        return

    source_config = config['source']
    target_config = config['targets']['gcp_test']

    # psycopg2ì—ì„œ ì‚¬ìš©í•˜ëŠ” í‚¤ ì´ë¦„ìœ¼ë¡œ ì¡°ì • ('db' -> 'dbname', 'username' -> 'user')
    # source ì„¤ì • ì¡°ì •
    if 'db' in source_config:
        source_config['dbname'] = source_config.pop('db')
    if 'username' in source_config:
        source_config['user'] = source_config.pop('username')

    # target ì„¤ì • ì¡°ì •
    if 'db' in target_config:
        target_config['dbname'] = target_config.pop('db')
    if 'username' in target_config:
        target_config['user'] = target_config.pop('username')


    # ì—°ê²°
    try:
        print("Connecting to source database...")
        src_conn = get_connection(source_config)
        print("Connecting to target database (gcp_test)...")
        tgt_conn = get_connection(target_config)
    except psycopg2.Error as e:
        print(f"Database connection error: {e}")
        return
    except Exception as e:
        print(f"An unexpected error occurred during connection: {e}")
        return


    # --- ë°ì´í„° ì¡°íšŒ ---
    print("Fetching Enum DDLs...")
    src_enum_ddls = fetch_enums(src_conn) # DDL ìƒì„± ë° ìŠ¤í‚µ ë¡œê·¸ìš©
    tgt_enum_ddls = fetch_enums(tgt_conn) # ìŠ¤í‚µ ë¡œê·¸ìš©

    print("Fetching Enum Values...")
    src_enum_values = fetch_enums_values(src_conn) # ë¹„êµìš©
    tgt_enum_values = fetch_enums_values(tgt_conn) # ë¹„êµìš©

    print("Fetching Table Metadata...")
    src_tables_meta, src_composite_uniques, src_composite_primaries = fetch_tables_metadata(src_conn)
    tgt_tables_meta, tgt_composite_uniques, tgt_composite_primaries = fetch_tables_metadata(tgt_conn)


    print("Fetching View DDLs...")
    src_views = fetch_views(src_conn) # ë¹„êµ ë° DDL ìƒì„±ìš©
    tgt_views = fetch_views(tgt_conn) # ë¹„êµìš©

    print("Fetching Function DDLs...")
    src_functions = fetch_functions(src_conn) # ë¹„êµ ë° DDL ìƒì„±ìš©
    tgt_functions = fetch_functions(tgt_conn) # ë¹„êµìš©

    print("Fetching Index DDLs...")
    src_indexes, src_pkey_indexes = fetch_indexes(src_conn) # ë¹„êµ ë° DDL ìƒì„±ìš© + ì •ë³´ìš©
    tgt_indexes, tgt_pkey_indexes = fetch_indexes(tgt_conn) # ë¹„êµìš© + ì •ë³´ìš©

    print("Fetching Sequence DDLs...")
    print("  Fetching from source database...")
    src_sequences = fetch_sequences(src_conn) # ë¹„êµ ë° DDL ìƒì„±ìš©
    print("  Fetching from target database...")
    tgt_sequences = fetch_sequences(tgt_conn) # ë¹„êµìš©
    print(f"  Source sequences count: {len(src_sequences)}")
    print(f"  Target sequences count: {len(tgt_sequences)}")
    # --- ë°ì´í„° ì¡°íšŒ ë ---


    # --- ê²€ì¦ ëª¨ë“œ ì²˜ë¦¬ ---
    if args.verify:
        print("\n--- Schema Verification Mode ---")
        all_synced = True
        # ê²€ì¦ ì‹œì—ëŠ” ì´ë¦„ ëª©ë¡ë§Œ ë¹„êµ
        all_synced &= print_verification_report(src_enum_ddls, tgt_enum_ddls, "Enums (Types)")
        all_synced &= print_verification_report(src_sequences, tgt_sequences, "Sequences")
        all_synced &= print_verification_report(src_tables_meta, tgt_tables_meta, "Tables")
        all_synced &= print_verification_report(src_views, tgt_views, "Views")
        all_synced &= print_verification_report(src_functions, tgt_functions, "Functions")
        # ë¹„êµ ëŒ€ìƒ ì¸ë±ìŠ¤ë§Œ ê²€ì¦ ë¦¬í¬íŠ¸ì— ì‚¬ìš©
        all_synced &= print_verification_report(src_indexes, tgt_indexes, "Indexes (excluding _pkey)")

        # íƒ€ê²Ÿì—ë§Œ ìˆëŠ” _pkey ì¸ë±ìŠ¤ ì •ë³´ ì¶œë ¥
        target_only_pkeys = sorted(list(set(tgt_pkey_indexes.keys()) - set(src_pkey_indexes.keys())))
        if target_only_pkeys:
            print(f"\n  Info: Found target-only primary key indexes (ignored in comparison): {', '.join(target_only_pkeys)}")

        print("\n--- Verification Summary ---")
        if all_synced:
            print("Source and target schemas are perfectly synchronized (based on object names).")
        else:
            print("Schema differences found. Please review the report above.")

        # ê²€ì¦ ëª¨ë“œì—ì„œëŠ” ì—°ê²°ë§Œ ë‹«ê³  ì¢…ë£Œ
        src_conn.close()
        if tgt_conn:
            tgt_conn.close()
        print("\nConnections closed.")
        return # ì—¬ê¸°ì„œ í•¨ìˆ˜ ì¢…ë£Œ
    # --- ê²€ì¦ ëª¨ë“œ ì²˜ë¦¬ ë ---


    # --- ë§ˆì´ê·¸ë ˆì´ì…˜/íŒŒì¼ ìƒì„± ëª¨ë“œ (ê¸°ì¡´ ë¡œì§) ---
    print("\n--- Migration Generation Mode ---")
    if args.use_alter:
        print("--- Using experimental ALTER TABLE mode ---")
    all_migration_sql = [] # ì‹¤ì œ ë§ˆì´ê·¸ë ˆì´ì…˜ SQL ì €ì¥
    all_skipped_sql = []   # ê±´ë„ˆë›´ SQL ì €ì¥

    # ìˆœì„œ: enum, sequence, table, view, function, index
    print("Comparing Enums (Values)...")
    # Enum ë¹„êµ ì‹œ ê°’ ëª©ë¡(values)ì„ ì‚¬ìš©í•˜ê³ , DDL ìƒì„±ì„ ìœ„í•´ src_enum_ddls ì „ë‹¬
    mig_sql, skip_sql = compare_and_generate_migration(src_enum_values, tgt_enum_values, "TYPE", src_enum_ddls=src_enum_ddls)
    all_migration_sql.extend(mig_sql)
    all_skipped_sql.extend(skip_sql)

    print("Comparing Sequences (DDL)...")
    mig_sql, skip_sql = compare_and_generate_migration(src_sequences, tgt_sequences, "SEQUENCE")
    all_migration_sql.extend(mig_sql)
    all_skipped_sql.extend(skip_sql)

    print("Comparing Tables (Metadata)...")
    # use_alter ì˜µì…˜ ì „ë‹¬
    mig_sql, skip_sql = compare_and_generate_migration(src_tables_meta, tgt_tables_meta, "TABLE", 
                                                       use_alter=args.use_alter, src_enum_ddls=src_enum_ddls,
                                                       src_composite_uniques = src_composite_uniques,tgt_composite_uniques= tgt_composite_uniques,
                                                       src_composite_primaries = src_composite_primaries, tgt_composite_primaries = tgt_composite_primaries  ) # src_enum_ddls ì „ë‹¬ ì¶”ê°€
    all_migration_sql.extend(mig_sql)
    all_skipped_sql.extend(skip_sql)
    # í…Œì´ë¸” ë©”íƒ€ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ FK DDL ìƒì„±



    print("Comparing Foreign Keys...")  # ğŸ‘ˆ ì´ ë¶€ë¶„ ì¶”ê°€

    src_fk_map = extract_foreign_keys(src_tables_meta)
    tgt_fk_map = extract_foreign_keys(tgt_tables_meta)

    mig_sql, skip_sql = compare_and_generate_migration(src_fk_map, tgt_fk_map, "FOREIGN_KEY")
    all_migration_sql.extend(mig_sql)
    all_skipped_sql.extend(skip_sql)

    print("Comparing Views (DDL)...")
    mig_sql, skip_sql = compare_and_generate_migration(src_views, tgt_views, "VIEW")
    all_migration_sql.extend(mig_sql)
    all_skipped_sql.extend(skip_sql)

    print("Comparing Functions (DDL)...")
    mig_sql, skip_sql = compare_and_generate_migration(src_functions, tgt_functions, "FUNCTION")
    all_migration_sql.extend(mig_sql)
    all_skipped_sql.extend(skip_sql)

    print("Comparing Indexes (DDL, excluding _pkey)...")
    # ë¹„êµ ëŒ€ìƒ ì¸ë±ìŠ¤ë§Œ ë§ˆì´ê·¸ë ˆì´ì…˜ ìƒì„±ì— ì‚¬ìš©
    mig_sql, skip_sql = compare_and_generate_migration(src_indexes, tgt_indexes, "INDEX")
    all_migration_sql.extend(mig_sql)
    all_skipped_sql.extend(skip_sql)
    # --- ë¹„êµ ë° SQL ìƒì„± ë ---

    

    # íŒŒì¼ëª… ìƒì„± (target_nameì€ configì—ì„œ ê°€ì ¸ì˜¨ ì²«ë²ˆì§¸ íƒ€ê²Ÿ í‚¤ë¡œ ê°€ì •)
    # TODO: ì—¬ëŸ¬ íƒ€ê²Ÿì„ ì²˜ë¦¬í•´ì•¼ í•˜ëŠ” ê²½ìš° ë¡œì§ ìˆ˜ì • í•„ìš”
    target_name = list(config['targets'].keys())[0] if config.get('targets') else 'unknown_target'

    migration_filename = os.path.join(history_dir, f"migrate.{target_name}.{timestamp}.sql")
    skipped_filename = os.path.join(history_dir, f"skip.{target_name}.{timestamp}.sql")

    # ë§ˆì´ê·¸ë ˆì´ì…˜ SQL íŒŒì¼ ì €ì¥
    try:
        with open(migration_filename, "w", encoding="utf-8") as f:
            f.write("\n".join(all_migration_sql))
        print(f"Migration SQL written to {migration_filename}")
    except IOError as e:
        print(f"Error writing migration file {migration_filename}: {e}")


    # ê±´ë„ˆë›´ SQL íŒŒì¼ ì €ì¥
    try:
        with open(skipped_filename, "w", encoding="utf-8") as f:
            f.write("\n".join(all_skipped_sql))
        print(f"Skipped SQL written to {skipped_filename}")
    except IOError as e:
        print(f"Error writing skipped file {skipped_filename}: {e}")

    if args.with_data:
        print("\n-- Running only Data Migration --")
        try:
            run_data_migration_parallel(src_conn ,src_tables_meta)
            print("\nData migration completed and committed.")

            # ì‹œí€€ìŠ¤ ê°’ ë™ê¸°í™” ì‹¤í–‰ (ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ í›„)
            common_sequences = set(src_sequences.keys()) & set(tgt_sequences.keys())
            if common_sequences:
                sync_sequence_values(src_conn, tgt_conn, common_sequences)
                tgt_conn.commit()
                print("Sequence values synchronized and committed.")

            # ê²€ì¦: ëª¨ë“  í…Œì´ë¸”ì˜ row count ë¹„êµ
            table_list = list(src_tables_meta.keys())
            diffs = compare_row_counts(src_conn, tgt_conn, table_list)
            if diffs:
                print("\nâ— Row count mismatches detected:")
                for tbl, (src_cnt, tgt_cnt) in diffs.items():
                    print(f"  - {tbl}: source={src_cnt}, target={tgt_cnt}")
            else:
                print("\nâœ… All row counts match between source and target.")

        except Exception as e:
            print(f"Error during data migration: {e}")
            print("Transaction rolled back.")
        finally:
            src_conn.close()
            print("Connections closed.")
        return  
    # --- ë§ˆì´ê·¸ë ˆì´ì…˜ SQL ì‹¤í–‰ (commit ì˜µì…˜ì´ Trueì¼ ê²½ìš°) ---
    elif args.commit:
        if not all_migration_sql:
            print("No migration SQL to execute.")
        else:
            print(f"\nExecuting migration SQL on target database ({target_name})...")
            execution_successful = True
            try:
                with tgt_conn.cursor() as cur:
                    # ê° SQL ë¸”ë¡ ì²˜ë¦¬
                    for i, sql_block in enumerate(all_migration_sql):
                        # ë¸”ë¡ ë‚´ ì£¼ì„ ì œì™¸ ë° ì‹¤ì œ ì‹¤í–‰í•  SQL ì¶”ì¶œ
                        sql_content = "\n".join(line for line in sql_block.strip().splitlines() if not line.strip().startswith('--'))
                        if not sql_content.strip():
                            continue # ì‹¤í–‰í•  ë‚´ìš© ì—†ìœ¼ë©´ ë‹¤ìŒ ë¸”ë¡ìœ¼ë¡œ

                        # SQL ë¸”ë¡ ì „ì²´ë¥¼ ë‹¨ì¼ ë¬¸ì¥ìœ¼ë¡œ ì‹¤í–‰
                        # (compare_and_generate_migrationì—ì„œ ì´ë¯¸ ì™„ì „í•œ DDL ë‹¨ìœ„ë¡œ ìƒì„±ë¨)
                        if not sql_content.strip(): # ì¶”ê°€ëœ ë¹ˆ ë¸”ë¡ ìŠ¤í‚µ
                            continue

                        print(f"--- Executing Block {i+1} (1 statement) ---")
                        try:
                            # sql_content ì „ì²´ë¥¼ ì‹¤í–‰
                            print(f"  Executing statement 1: {sql_content[:100]}{'...' if len(sql_content) > 100 else ''}")
                            cur.execute(sql_content)
                        except psycopg2.Error as e:
                            print(f"\nError executing block {i+1}:")
                            print(f"  Block Content: {sql_content}")
                            print(f"  Error: {e}")
                            print("Rolling back transaction...")
                            tgt_conn.rollback()
                            print("Transaction rolled back.")
                            execution_successful = False
                            break # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì „ì²´ ì‹¤í–‰ ì¤‘ë‹¨
                        except Exception as e:
                            print(f"\nAn unexpected error occurred during block {i+1} execution:")
                            print(f"  Block Content: {sql_content}")
                            print(f"  Error: {e}")
                            print("Rolling back transaction...")
                            tgt_conn.rollback()
                            print("Transaction rolled back.")
                            execution_successful = False
                            break # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì „ì²´ ì‹¤í–‰ ì¤‘ë‹¨

                # ëª¨ë“  ë¸”ë¡ ë° ë¬¸ì¥ì´ ì„±ê³µì ìœ¼ë¡œ ì‹¤í–‰ëœ ê²½ìš°ì—ë§Œ ì»¤ë°‹
                if execution_successful:
                    tgt_conn.commit()
                    print("\nMigration SQL executed successfully and committed.")
                    
                    # ì‹œí€€ìŠ¤ ê°’ ë™ê¸°í™” ì‹¤í–‰
                    common_sequences = set(src_sequences.keys()) & set(tgt_sequences.keys())
                    print(f"\n--- Sequence Sync Debug ---")
                    print(f"Source sequences: {list(src_sequences.keys())}")
                    print(f"Target sequences: {list(tgt_sequences.keys())}")
                    print(f"Common sequences: {list(common_sequences)}")
                    
                    if common_sequences:
                        print(f"Calling sync_sequence_values with {len(common_sequences)} sequences...")
                        sync_sequence_values(src_conn, tgt_conn, common_sequences)
                        tgt_conn.commit()
                        print("Sequence values synchronized and committed.")
                    else:
                        print("No common sequences found, skipping sequence sync.")
                    
                    src_conn.close()
                    tgt_conn.close()
                    print("Connections closed.")
                else:
                    print("Migration SQL execution failed, skipping sequence sync.")
            except Exception as e: # ì»¤ì„œ ìƒì„± ë“± ì™¸ë¶€ try ë¸”ë¡ì˜ ì˜ˆì™¸ ì²˜ë¦¬
                print(f"\nAn unexpected error occurred during SQL execution setup: {e}")
                print("Rolling back transaction...")
                tgt_conn.rollback()
                print("Transaction rolled back.")
            except Exception as e:
                print(f"\nAn unexpected error occurred during SQL execution: {e}")
                print("Rolling back transaction...")
                tgt_conn.rollback()
                print("Transaction rolled back.")

    # --- SQL ì‹¤í–‰ ë ---


if __name__ == '__main__':
    main()
