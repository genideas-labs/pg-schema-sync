#!/usr/bin/env python3
import json
from collections import defaultdict, deque, OrderedDict
import yaml
import psycopg2
from psycopg2 import sql
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

def get_connection(config):
    conn = psycopg2.connect(**config)
    return conn
SKIP_TABLES = {'slow_request_logs', 'member_action_log'}

def migrate_single_table_with_conn(src_conn, tgt_conn, table_name, table_meta):
    """ì—°ê²°ì„ ì¬ì‚¬ìš©í•˜ì—¬ ë‹¨ì¼ í…Œì´ë¸” ë°ì´í„°ë¥¼ ë§ˆì´ê·¸ë ˆì´ì…˜í•©ë‹ˆë‹¤."""
    try:
        with src_conn.cursor() as src_cur, tgt_conn.cursor() as tgt_cur:
            src_cur.execute(f'SELECT * FROM public."{table_name}"')
            rows = src_cur.fetchall()

            if not rows:
                print(f"  â­ï¸  {table_name}: No data, skipped", flush=True)
                return True, None

            column_names = [desc[0] for desc in src_cur.description]
            quoted_column_names = [f'"{col}"' for col in column_names]
            values_placeholders = ", ".join(["%s"] * len(column_names))

            conflict_clause = "ON CONFLICT (id) DO NOTHING"

            column_type_map = {col['name']: col['type'] for col in table_meta}

            insert_sql = f'''
                INSERT INTO public."{table_name}" ({", ".join(quoted_column_names)})
                VALUES ({values_placeholders})
                {conflict_clause}
            '''

            serialized_rows = [
                tuple(
                    serialize_value(val, column_type_map.get(col_name))
                    for val, col_name in zip(row, column_names)
                )
                for row in rows
            ]
            
            tgt_cur.executemany(insert_sql, serialized_rows)
            tgt_conn.commit()
            print(f"  âœ… {table_name}: Inserted {len(rows)} rows", flush=True)
        return True, None

    except Exception as e:
        # ë¡¤ë°±í•˜ê³  ì—ëŸ¬ ë¦¬í¬íŠ¸
        tgt_conn.rollback()
        print(f"  âŒ {table_name}: {type(e).__name__}: {str(e)}", flush=True)
        return False, str(e)

def serialize_value(val, pg_type=None):
    if isinstance(val, list):
        if pg_type and (pg_type.endswith('[]') or pg_type.startswith('_')):
            if not val:
                return '{}'
            escaped_items = []
            for v in val:
                if isinstance(v, str):
                    # ë¬¸ìì—´ ì›ì†Œì¼ ê²½ìš° ì´ìŠ¤ì¼€ì´í”„
                    escaped_items.append(f'"{v.replace(chr(34), r"\\\"")}"')
                elif isinstance(v, dict):
                    # dict â†’ JSON ë¬¸ìì—´ â†’ ë‹¤ì‹œ ì´ìŠ¤ì¼€ì´í”„
                    json_str = json.dumps(v).replace('"', r'\"')
                    escaped_items.append(f'"{json_str}"')
                else:
                    escaped_items.append(str(v))
            return '{' + ','.join(escaped_items) + '}'
        else:
            return json.dumps(val)
    elif isinstance(val, (dict, set)):
        return json.dumps(val)
    return val

def get_all_foreign_keys(conn):
    """íƒ€ê²Ÿ DBì˜ ëª¨ë“  FK ì œì•½ì¡°ê±´ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    with conn.cursor() as cur:
        cur.execute("""
        SELECT 
            conrelid::regclass AS table_name,
            conname AS constraint_name,
            pg_get_constraintdef(pc.oid) AS constraint_def
        FROM pg_constraint pc
        JOIN pg_namespace n ON n.oid = pc.connamespace
        WHERE pc.contype = 'f' AND n.nspname = 'public'
        ORDER BY table_name, conname;
        """)
        return cur.fetchall()

def drop_all_foreign_keys(conn):
    """ëª¨ë“  FK ì œì•½ì¡°ê±´ì„ ë°°ì¹˜ë¡œ DROPí•©ë‹ˆë‹¤ (ë¹ ë¥¸ ì²˜ë¦¬)."""
    print("\nğŸ”“ Dropping all FK constraints (batch mode)...", flush=True)
    fks = get_all_foreign_keys(conn)
    
    if not fks:
        print("  No FK constraints found.")
        return []
    
    print(f"  Found {len(fks)} FK constraints to drop.", flush=True)
    
    # ë°°ì¹˜ í¬ê¸° (ì ì ˆí•œ í¬ê¸°ë¡œ ë¹ ë¥´ê²Œ ì²˜ë¦¬í•˜ë©´ì„œë„ ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„ ê°€ëŠ¥)
    BATCH_SIZE = 20
    dropped_count = 0
    failed_count = 0
    
    with conn.cursor() as cur:
        # lock timeout ì„¤ì • - ì™¸ë¶€ ì¶©ëŒì€ ì´ë¯¸ í•´ê²°ë˜ì—ˆìœ¼ë¯€ë¡œ ì ë‹¹íˆ ì„¤ì •
        cur.execute("SET lock_timeout = '10s';")
        print(f"  â±ï¸  Lock timeout set to 10 seconds", flush=True)
        
        for i in range(0, len(fks), BATCH_SIZE):
            batch = fks[i:i+BATCH_SIZE]
            batch_num = i // BATCH_SIZE + 1
            total_batches = (len(fks) + BATCH_SIZE - 1) // BATCH_SIZE
            
            try:
                # ë°°ì¹˜ ì „ì²´ ì‹¤í–‰
                for table_name, constraint_name, _ in batch:
                    drop_sql = f'ALTER TABLE {table_name} DROP CONSTRAINT IF EXISTS "{constraint_name}";'
                    cur.execute(drop_sql)
                    dropped_count += 1
                
                # ë°°ì¹˜ë§ˆë‹¤ ì»¤ë°‹
                conn.commit()
                print(f"  âœ… Batch {batch_num}/{total_batches}: Dropped {len(batch)} FKs ({dropped_count}/{len(fks)} total)", flush=True)
                
            except Exception as e:
                conn.rollback()
                print(f"  âš ï¸  Batch {batch_num} failed, retrying one by one...", flush=True)
                
                # ì‹¤íŒ¨í•œ ë°°ì¹˜ëŠ” í•˜ë‚˜ì”© ì¬ì‹œë„
                for table_name, constraint_name, _ in batch:
                    try:
                        drop_sql = f'ALTER TABLE {table_name} DROP CONSTRAINT IF EXISTS "{constraint_name}";'
                        cur.execute(drop_sql)
                        conn.commit()
                        dropped_count += 1
                    except Exception as e2:
                        conn.rollback()
                        failed_count += 1
                        if 'lock timeout' in str(e2).lower():
                            print(f"    â­ï¸  Skipped (busy): {table_name}.{constraint_name}", flush=True)
                        else:
                            print(f"    âœ— Failed: {table_name}.{constraint_name}: {e2}", flush=True)
    
    print(f"\nâœ… Dropped {dropped_count}/{len(fks)} FK constraints (Failed: {failed_count}).\n", flush=True)
    return fks

def recreate_foreign_keys_not_valid(conn, fks):
    """FK ì œì•½ì¡°ê±´ì„ ë°°ì¹˜ë¡œ NOT VALIDë¡œ ì¬ìƒì„±í•©ë‹ˆë‹¤ (ë¹ ë¥¸ ì²˜ë¦¬)."""
    print("\nğŸ”— Recreating FK constraints (NOT VALID, batch mode)...", flush=True)
    
    if not fks:
        print("  No FK constraints to recreate.")
        return
    
    # ë°°ì¹˜ í¬ê¸° (ì ì ˆí•œ í¬ê¸°ë¡œ ë¹ ë¥´ê²Œ ì²˜ë¦¬í•˜ë©´ì„œë„ ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„ ê°€ëŠ¥)
    BATCH_SIZE = 20
    added_count = 0
    failed_count = 0
    
    with conn.cursor() as cur:
        # lock timeout ì„¤ì • - ì™¸ë¶€ ì¶©ëŒì€ ì´ë¯¸ í•´ê²°ë˜ì—ˆìœ¼ë¯€ë¡œ ì ë‹¹íˆ ì„¤ì •
        cur.execute("SET lock_timeout = '10s';")
        print(f"  â±ï¸  Lock timeout set to 10 seconds", flush=True)
        
        for i in range(0, len(fks), BATCH_SIZE):
            batch = fks[i:i+BATCH_SIZE]
            batch_num = i // BATCH_SIZE + 1
            total_batches = (len(fks) + BATCH_SIZE - 1) // BATCH_SIZE
            
            try:
                # ë°°ì¹˜ ì „ì²´ ì‹¤í–‰
                for table_name, constraint_name, constraint_def in batch:
                    add_sql = f'ALTER TABLE {table_name} ADD CONSTRAINT "{constraint_name}" {constraint_def} NOT VALID;'
                    cur.execute(add_sql)
                    added_count += 1
                
                # ë°°ì¹˜ë§ˆë‹¤ ì»¤ë°‹
                conn.commit()
                print(f"  âœ… Batch {batch_num}/{total_batches}: Added {len(batch)} FKs ({added_count}/{len(fks)} total)", flush=True)
                
            except Exception as e:
                conn.rollback()
                print(f"  âš ï¸  Batch {batch_num} failed, retrying one by one...", flush=True)
                
                # ì‹¤íŒ¨í•œ ë°°ì¹˜ëŠ” í•˜ë‚˜ì”© ì¬ì‹œë„
                for table_name, constraint_name, constraint_def in batch:
                    try:
                        add_sql = f'ALTER TABLE {table_name} ADD CONSTRAINT "{constraint_name}" {constraint_def} NOT VALID;'
                        cur.execute(add_sql)
                        conn.commit()
                        added_count += 1
                    except Exception as e2:
                        conn.rollback()
                        failed_count += 1
                        if 'lock timeout' in str(e2).lower():
                            print(f"    â­ï¸  Skipped (busy): {table_name}.{constraint_name}", flush=True)
                        else:
                            print(f"    âœ— Failed: {table_name}.{constraint_name}: {e2}", flush=True)
    
    print(f"\nâœ… Recreated {added_count}/{len(fks)} FK constraints (Failed: {failed_count}).\n", flush=True)

def generate_validate_script(fks, output_file='validate_fks.sql'):
    """FK VALIDATE ìŠ¤í¬ë¦½íŠ¸ë¥¼ íŒŒì¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤ (ë‚˜ì¤‘ì— íŠ¸ë˜í”½ ì—†ëŠ” ì‹œê°„ëŒ€ì— ì‹¤í–‰)."""
    print(f"\nğŸ“ Generating VALIDATE script: {output_file}", flush=True)
    
    if not fks:
        print("  No FK constraints to validate.")
        return
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("-- FK VALIDATE Script\n")
        f.write("-- ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” íŠ¸ë˜í”½ì´ ì ì€ ì‹œê°„ëŒ€ì— ì‹¤í–‰í•˜ì„¸ìš”.\n")
        f.write("-- VALIDATEëŠ” ì „ì²´ í…Œì´ë¸”ì„ ìŠ¤ìº”í•˜ë¯€ë¡œ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n")
        f.write(f"-- Total: {len(fks)} FK constraints\n\n")
        f.write("-- Progress tracking:\n")
        f.write("-- \\timing on\n\n")
        
        for idx, (table_name, constraint_name, _) in enumerate(fks, 1):
            f.write(f"-- [{idx}/{len(fks)}] Validating {table_name}.{constraint_name}\n")
            f.write(f"ALTER TABLE {table_name} VALIDATE CONSTRAINT \"{constraint_name}\";\n")
            if idx % 10 == 0:
                f.write(f"-- Progress: {idx}/{len(fks)} completed\n")
            f.write("\n")
        
        f.write("-- All FK constraints validated!\n")
    
    print(f"âœ… VALIDATE script generated: {output_file}", flush=True)
    print(f"   Run this script later with: psql -f {output_file}\n", flush=True)

def run_data_migration_parallel(src_conn, src_tables_meta, src_composite_fks=None, max_total_attempts=10):
    # FK ì˜ì¡´ì„± ì •ë ¬ì´ í•„ìš” ì—†ìŒ - FKë¥¼ ë¯¸ë¦¬ DROPí•˜ë¯€ë¡œ
    print("\n--- Starting Parallel Data Migration ---")
    print(f"Total tables to migrate: {len(src_tables_meta)}")
    
    remaining_tables = [
        (tbl, meta)
        for tbl, meta in src_tables_meta.items()
        if tbl not in SKIP_TABLES
    ]

    table_errors = defaultdict(str)
    try:
        with open("config.yaml", 'r', encoding='utf-8') as stream:
            config = yaml.safe_load(stream)
            if not config:
                print("Error: config.yaml is empty or invalid.")
                return
    except FileNotFoundError:
        print("Error: config.yaml not found.")
        return
    except yaml.YAMLError as exc:
        print(f"Error parsing config.yaml: {exc}")
        return
    except Exception as e:
        print(f"An unexpected error occurred while reading config.yaml: {e}")
        return
    target_config = config['targets']['gcp_test']
    source_config = config['source']
    
    # ì—°ê²° í’€ ìƒì„± (ë³‘ë ¬ ì²˜ë¦¬ìš©)
    MAX_WORKERS = 5
    connection_pool = []
    
    print(f"\nğŸ”Œ Creating connection pool ({MAX_WORKERS} workers)...", flush=True)
    for i in range(MAX_WORKERS):
        src_conn = get_connection(source_config)
        tgt_conn = get_connection(target_config)
        connection_pool.append((src_conn, tgt_conn))
    print(f"  Connection pool ready: {len(connection_pool)} worker connections", flush=True)
    
    # ì—°ê²° í• ë‹¹ì„ ìœ„í•œ lock
    pool_lock = threading.Lock()
    available_connections = list(range(MAX_WORKERS))
    
    def get_conn_from_pool():
        """ì—°ê²° í’€ì—ì„œ ì—°ê²° ìŒ ê°€ì ¸ì˜¤ê¸°"""
        with pool_lock:
            if available_connections:
                idx = available_connections.pop(0)
                return idx, connection_pool[idx]
            return None, (None, None)
    
    def return_conn_to_pool(idx):
        """ì—°ê²° í’€ì— ë°˜í™˜"""
        with pool_lock:
            available_connections.append(idx)
    
    def migrate_table_worker(table_name, table_meta):
        """Worker í•¨ìˆ˜: ì—°ê²° í’€ì—ì„œ ì—°ê²° ê°€ì ¸ì™€ì„œ í…Œì´ë¸” ë§ˆì´ê·¸ë ˆì´ì…˜"""
        conn_idx, (src_conn, tgt_conn) = get_conn_from_pool()
        try:
            return migrate_single_table_with_conn(src_conn, tgt_conn, table_name, table_meta)
        finally:
            return_conn_to_pool(conn_idx)
    
    try:
        # 1. íƒ€ê²Ÿ DBì—ì„œ ëª¨ë“  FK ì €ì¥ í›„ DROP (ì²« ë²ˆì§¸ ì—°ê²° ì‚¬ìš©)
        dropped_fks = drop_all_foreign_keys(connection_pool[0][1])
        
        # 2. ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ (ë³‘ë ¬ ì²˜ë¦¬, ì—°ê²° í’€ ì¬ì‚¬ìš©)
        print(f"\nğŸ“Š Migrating {len(remaining_tables)} tables in parallel ({MAX_WORKERS} workers)...", flush=True)
        
        for attempt in range(1, max_total_attempts + 1):
            if not remaining_tables:
                break
            
            print(f"\n=== Migration Attempt {attempt}/{max_total_attempts} ===", flush=True)
            next_round = []
            completed = 0
            
            with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                future_to_table = {
                    executor.submit(migrate_table_worker, table_name, table_meta): table_name
                    for table_name, table_meta in remaining_tables
                }
                
                # as_completedë¡œ ì™„ë£Œë˜ëŠ” ëŒ€ë¡œ ì²˜ë¦¬ (ìˆœì„œ ë¬´ê´€)
                for future in as_completed(future_to_table):
                    table_name = future_to_table[future]
                    try:
                        success, error_msg = future.result()
                        completed += 1
                        
                        if not success:
                            table_meta = src_tables_meta[table_name]
                            next_round.append((table_name, table_meta))
                            table_errors[table_name] = error_msg or f"Failed on attempt {attempt}"
                        
                        # ì§„í–‰ìƒí™© (ë§¤ 10ê°œë§ˆë‹¤)
                        if completed % 10 == 0:
                            print(f"  Progress: {completed}/{len(remaining_tables)} tables", flush=True)
                    except Exception as exc:
                        table_meta = src_tables_meta[table_name]
                        next_round.append((table_name, table_meta))
                        table_errors[table_name] = str(exc)
                        completed += 1
            
            print(f"  Completed: {completed}/{len(remaining_tables)} tables", flush=True)
            remaining_tables = next_round
        
        # 3. FK ì¬ìƒì„± (NOT VALID) (ì²« ë²ˆì§¸ ì—°ê²° ì‚¬ìš©)
        recreate_foreign_keys_not_valid(connection_pool[0][1], dropped_fks)
        
    finally:
        # ì—°ê²° í’€ ëª¨ë‘ ë‹«ê¸°
        print("\nğŸ”Œ Closing connection pool...", flush=True)
        for src_conn, tgt_conn in connection_pool:
            try:
                src_conn.close()
                tgt_conn.close()
            except:
                pass
        print("  Connection pool closed.", flush=True)
    
    # 4. VALIDATE ìŠ¤í¬ë¦½íŠ¸ ìƒì„± (ë‚˜ì¤‘ì— ìˆ˜ë™ ì‹¤í–‰)
    generate_validate_script(dropped_fks, output_file='validate_fks.sql')
    
    if remaining_tables:
        print("\n--- Data Migration Completed with Failures ---")
        for table_name, _ in remaining_tables:
            print(f"  âŒ {table_name}: {table_errors[table_name]}")
    else:
        print("\nâœ… All tables migrated successfully.")
        print("âœ… ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ")
    
    

from collections import defaultdict, deque, OrderedDict

def sort_tables_by_fk_dependency(tables_metadata, composite_fks=None):
    graph = defaultdict(set)  # {A: {B}} â†’ AëŠ” Bì— ì¢…ì†ë¨ (ì¦‰, B â†’ A)
    in_degree = defaultdict(int)
    fk_count = 0

    # 1. ë‹¨ì¼ ì»¬ëŸ¼ FK ì²˜ë¦¬
    for table, columns in tables_metadata.items():
        in_degree.setdefault(table, 0)
        for col in columns:
            fk = col.get("foreign_key")
            if fk:
                ref_table = fk["table"]
                graph[ref_table].add(table)
                in_degree[table] += 1
                fk_count += 1
    
    # 2. ë³µí•© FK ì²˜ë¦¬ (ìƒˆë¡œ ì¶”ê°€)
    composite_fk_count = 0
    if composite_fks:
        for table, fk_list in composite_fks.items():
            in_degree.setdefault(table, 0)
            for fk_info in fk_list:
                ref_table = fk_info['ref_table']
                # ì¤‘ë³µ ì¹´ìš´íŠ¸ ë°©ì§€: ì´ë¯¸ ë‹¨ì¼ FKë¡œ ì¶”ê°€ëœ ê²½ìš° ì œì™¸
                if table not in graph[ref_table]:
                    graph[ref_table].add(table)
                    in_degree[table] += 1
                    composite_fk_count += 1
    
    print(f"\nğŸ”— FK Dependencies detected:")
    print(f"  - Single column FKs: {fk_count}")
    print(f"  - Composite FKs: {composite_fk_count}")
    print(f"  - Total FK relationships: {fk_count + composite_fk_count}")

    # ìœ„ìƒ ì •ë ¬ (Topological Sort)
    # ì˜ì¡´ì„±ì´ ì—†ëŠ” í…Œì´ë¸”ë“¤(ë¶€ëª¨ í…Œì´ë¸”)ë¶€í„° ì‹œì‘
    independent_tables = sorted([t for t in tables_metadata.keys() if in_degree[t] == 0])
    queue = deque(independent_tables)
    sorted_tables = []

    print(f"  - Independent tables (no FK dependencies): {len(independent_tables)}")

    while queue:
        current = queue.popleft()
        sorted_tables.append(current)
        
        # í˜„ì¬ í…Œì´ë¸”ì— ì˜ì¡´í•˜ëŠ” í…Œì´ë¸”ë“¤ì˜ in_degree ê°ì†Œ
        # ì¦‰, í˜„ì¬ í…Œì´ë¸”(ë¶€ëª¨)ì„ ì°¸ì¡°í•˜ëŠ” ìì‹ í…Œì´ë¸”ë“¤ í™•ì¸
        for dependent in sorted(graph[current]):  # ì•ŒíŒŒë²³ ìˆœ ì •ë ¬ë¡œ ì¼ê´€ì„± ìœ ì§€
            in_degree[dependent] -= 1
            if in_degree[dependent] == 0:
                # ëª¨ë“  ì˜ì¡´ì„±ì´ í•´ê²°ë˜ë©´ íì— ì¶”ê°€
                queue.append(dependent)

    # ìˆœí™˜ ì°¸ì¡° ê°ì§€
    if len(sorted_tables) < len(tables_metadata):
        print("\nâš ï¸ Warning: Cyclic dependency detected among tables!")
        remaining = set(tables_metadata) - set(sorted_tables)
        print(f"  - Tables with circular dependencies: {sorted(remaining)}")
        # ìˆœí™˜ ì°¸ì¡°ê°€ ìˆëŠ” í…Œì´ë¸”ë“¤ì€ ì•ŒíŒŒë²³ ìˆœìœ¼ë¡œ ì¶”ê°€
        sorted_tables.extend(sorted(remaining))

    # âœ… OrderedDictìœ¼ë¡œ ì •ë ¬ëœ ê²°ê³¼ ë°˜í™˜
    return OrderedDict((table, tables_metadata[table]) for table in sorted_tables)

def batch_insert(tgt_conn, tgt_cur, insert_sql, serialized_rows, table_name, batch_size=1000):
    total = len(serialized_rows)
    for i in range(0, total, batch_size):
        batch = serialized_rows[i:i + batch_size]
        try:
            tgt_cur.executemany(insert_sql, batch)
            tgt_conn.commit()
            print(f"    âœ… Batch {i // batch_size + 1}: Inserted {len(batch)} rows into {table_name}")
        except Exception as e:
            tgt_conn.rollback()
            print(f"    âŒ Batch {i // batch_size + 1}: Failed to insert {len(batch)} rows into {table_name}")
            print(f"       Error: {e}")

def compare_row_counts(src_conn, tgt_conn, table_names):
    """
    src_conn, tgt_conn: psycopg2 ì»¤ë„¥ì…˜
    table_names: ë¹„êµí•  í…Œì´ë¸”ëª… ë¦¬ìŠ¤íŠ¸
    ë°˜í™˜ê°’: {table: (src_count, tgt_count)} í˜•íƒœë¡œ, ì°¨ì´ê°€ ìˆëŠ” í…Œì´ë¸”ë§Œ ë‹´ì•„ì„œ ë¦¬í„´
    """
    diffs = {}
    with src_conn.cursor() as src_cur, tgt_conn.cursor() as tgt_cur:
        for tbl in table_names:
            src_cur.execute(sql.SQL('SELECT COUNT(*) FROM public.{}').format(sql.Identifier(tbl)))
            src_count = src_cur.fetchone()[0]
            tgt_cur.execute(sql.SQL('SELECT COUNT(*) FROM public.{}').format(sql.Identifier(tbl)))
            tgt_count = tgt_cur.fetchone()[0]
            if src_count != tgt_count:
                diffs[tbl] = (src_count, tgt_count)
    return diffs